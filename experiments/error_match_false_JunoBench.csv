nb_name,eid,ename,evalue,traceback,notebook_path,timestamp,error_type,error_message,clean_error_name,clean_error_message,error_match,fixed_or_not
NBspecific_1,dac09c73-577f-3d5d-834b-45942405eabb,nameerror,name 'tf_idf' is not defined,"['---------------------------------------------------------------------------', 'NameError                                 Traceback (most recent call last)', '/tmp/ipykernel_1018/1279619201.py in <module>\n----> 1 tf_idf.toarray()\n', ""NameError: name 'tf_idf' is not defined""]",/junobench_env/NBspecific_1/NBspecific_1_extension.ipynb,2026-01-14T16:37:00.489958,CellExecutionError,"An error occurred while executing the following cell:
------------------

import nltk 
nltk.download('punkt') # For word_tokenize and sent_tokenize 
nltk.download('stopwords') # For stopwords 
nltk.download('wordnet') # For WordNetLemmatizer 
nltk.download('omw-1.4') # For WordNetLemmatizer (often needed for WordNet)

lemmatizer = WordNetLemmatizer()

def lemma_words(text):
    return "" "".join([lemmatizer.lemmatize(word) for word in text])
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m<ipython-input-27-d6ce47d93672>[0m in [0;36m<cell line: 7>[0;34m()[0m
[1;32m      5[0m [0mnltk[0m[0;34m.[0m[0mdownload[0m[0;34m([0m[0;34m'omw-1.4'[0m[0;34m)[0m [0;31m# For WordNetLemmatizer (often needed for WordNet)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      6[0m [0;34m[0m[0m
[0;32m----> 7[0;31m [0mlemmatizer[0m [0;34m=[0m [0mWordNetLemmatizer[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      8[0m [0;34m[0m[0m
[1;32m      9[0m [0;32mdef[0m [0mlemma_words[0m[0;34m([0m[0mtext[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;31mNameError[0m: name 'WordNetLemmatizer' is not defined
NameError: name 'WordNetLemmatizer' is not defined
",NameError,name 'WordNetLemmatizer' is not defined,FALSE,"fixed the error, could not resolve another import issue"
NBspecific_2,86d3fc43-5a6a-3e80-94ef-e82ea833017b,nameerror,name 'X_train' is not defined,"['---------------------------------------------------------------------------', 'NameError                                 Traceback (most recent call last)', ""<ipython-input-3-6b970bf11e02> in <module>\n      3 classifier = XGBClassifier(tree_method='gp_hist', gpu_id=0)\n      4 \n----> 5 classifier.fit(X_train, y_train)\n      6 y_hat = classifier.predict(X_test)\n      7 accuracy_score(y_test, y_hat)\n"", ""NameError: name 'X_train' is not defined""]",/junobench_env/NBspecific_2/NBspecific_2_extension.ipynb,2026-01-14T18:12:08.112654,CellExecutionError,"An error occurred while executing the following cell:
------------------
from xgboost import XGBClassifier 
classifier = XGBClassifier(tree_method='gpu_hist', gpu_id=0, random_state=1) # Added random_state 
# Ensure X_train, y_train, X_test, y_test are defined by running the train_test_split cell first 
classifier.fit(X_train, y_train) 
y_hat = classifier.predict(X_test) 
accuracy_score(y_test, y_hat)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mXGBoostError[0m                              Traceback (most recent call last)
[0;32m<ipython-input-21-64031c81fcdf>[0m in [0;36m<cell line: 4>[0;34m()[0m
[1;32m      2[0m [0mclassifier[0m [0;34m=[0m [0mXGBClassifier[0m[0;34m([0m[0mtree_method[0m[0;34m=[0m[0;34m'gpu_hist'[0m[0;34m,[0m [0mgpu_id[0m[0;34m=[0m[0;36m0[0m[0;34m,[0m [0mrandom_state[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m [0;31m# Added random_state[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m [0;31m# Ensure X_train, y_train, X_test, y_test are defined by running the train_test_split cell first[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 4[0;31m [0mclassifier[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0mX_train[0m[0;34m,[0m [0my_train[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      5[0m [0my_hat[0m [0;34m=[0m [0mclassifier[0m[0;34m.[0m[0mpredict[0m[0;34m([0m[0mX_test[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      6[0m [0maccuracy_score[0m[0;34m([0m[0my_test[0m[0;34m,[0m [0my_hat[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py[0m in [0;36minner_f[0;34m(*args, **kwargs)[0m
[1;32m    728[0m             [0;32mfor[0m [0mk[0m[0;34m,[0m [0marg[0m [0;32min[0m [0mzip[0m[0;34m([0m[0msig[0m[0;34m.[0m[0mparameters[0m[0;34m,[0m [0margs[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    729[0m                 [0mkwargs[0m[0;34m[[0m[0mk[0m[0;34m][0m [0;34m=[0m [0marg[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 730[0;31m             [0;32mreturn[0m [0mfunc[0m[0;34m([0m[0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    731[0m [0;34m[0m[0m
[1;32m    732[0m         [0;32mreturn[0m [0minner_f[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py[0m in [0;36mfit[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)[0m
[1;32m   1517[0m             )
[1;32m   1518[0m [0;34m[0m[0m
[0;32m-> 1519[0;31m             self._Booster = train(
[0m[1;32m   1520[0m                 [0mparams[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1521[0m                 [0mtrain_dmatrix[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py[0m in [0;36minner_f[0;34m(*args, **kwargs)[0m
[1;32m    728[0m             [0;32mfor[0m [0mk[0m[0;34m,[0m [0marg[0m [0;32min[0m [0mzip[0m[0;34m([0m[0msig[0m[0;34m.[0m[0mparameters[0m[0;34m,[0m [0margs[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    729[0m                 [0mkwargs[0m[0;34m[[0m[0mk[0m[0;34m][0m [0;34m=[0m [0marg[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 730[0;31m             [0;32mreturn[0m [0mfunc[0m[0;34m([0m[0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    731[0m [0;34m[0m[0m
[1;32m    732[0m         [0;32mreturn[0m [0minner_f[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py[0m in [0;36mtrain[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)[0m
[1;32m    179[0m         [0;32mif[0m [0mcb_container[0m[0;34m.[0m[0mbefore_iteration[0m[0;34m([0m[0mbst[0m[0;34m,[0m [0mi[0m[0;34m,[0m [0mdtrain[0m[0;34m,[0m [0mevals[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    180[0m             [0;32mbreak[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 181[0;31m         [0mbst[0m[0;34m.[0m[0mupdate[0m[0;34m([0m[0mdtrain[0m[0;34m,[0m [0mi[0m[0;34m,[0m [0mobj[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    182[0m         [0;32mif[0m [0mcb_container[0m[0;34m.[0m[0mafter_iteration[0m[0;34m([0m[0mbst[0m[0;34m,[0m [0mi[0m[0;34m,[0m [0mdtrain[0m[0;34m,[0m [0mevals[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    183[0m             [0;32mbreak[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py[0m in [0;36mupdate[0;34m(self, dtrain, iteration, fobj)[0m
[1;32m   2048[0m [0;34m[0m[0m
[1;32m   2049[0m         [0;32mif[0m [0mfobj[0m [0;32mis[0m [0;32mNone[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 2050[0;31m             _check_call(
[0m[1;32m   2051[0m                 _LIB.XGBoosterUpdateOneIter(
[1;32m   2052[0m                     [0mself[0m[0;34m.[0m[0mhandle[0m[0;34m,[0m [0mctypes[0m[0;34m.[0m[0mc_int[0m[0;34m([0m[0miteration[0m[0;34m)[0m[0;34m,[0m [0mdtrain[0m[0;34m.[0m[0mhandle[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py[0m in [0;36m_check_call[0;34m(ret)[0m
[1;32m    280[0m     """"""
[1;32m    281[0m     [0;32mif[0m [0mret[0m [0;34m!=[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 282[0;31m         [0;32mraise[0m [0mXGBoostError[0m[0;34m([0m[0mpy_str[0m[0;34m([0m[0m_LIB[0m[0;34m.[0m[0mXGBGetLastError[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    283[0m [0;34m[0m[0m
[1;32m    284[0m [0;34m[0m[0m

[0;31mXGBoostError[0m: [18:12:04] /workspace/src/tree/updater_gpu_hist.cu:781: Exception in gpu_hist: [18:12:04] /workspace/src/tree/updater_gpu_hist.cu:787: Check failed: ctx_->gpu_id >= 0 (-1 vs. 0) : Must have at least one device
Stack trace:
  [bt] (0) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7fe117f49f2a]
  [bt] (1) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb3e95a) [0x7fe117f6095a]
  [bt] (2) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb483cd) [0x7fe117f6a3cd]
  [bt] (3) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7fe117882c79]
  [bt] (4) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x461d09) [0x7fe117883d09]
  [bt] (5) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7fe1178e74f7]
  [bt] (6) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7fe117583ef0]
  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7fe2c0791e2e]
  [bt] (8) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7fe2c078e493]



Stack trace:
  [bt] (0) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7fe117f49f2a]
  [bt] (1) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb485c9) [0x7fe117f6a5c9]
  [bt] (2) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7fe117882c79]
  [bt] (3) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x461d09) [0x7fe117883d09]
  [bt] (4) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7fe1178e74f7]
  [bt] (5) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7fe117583ef0]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7fe2c0791e2e]
  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7fe2c078e493]
  [bt] (8) /usr/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0xa3e9) [0x7fe2bfedd3e9]


XGBoostError: [18:12:04] /workspace/src/tree/updater_gpu_hist.cu:781: Exception in gpu_hist: [18:12:04] /workspace/src/tree/updater_gpu_hist.cu:787: Check failed: ctx_->gpu_id >= 0 (-1 vs. 0) : Must have at least one device
Stack trace:
  [bt] (0) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7fe117f49f2a]
  [bt] (1) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb3e95a) [0x7fe117f6095a]
  [bt] (2) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb483cd) [0x7fe117f6a3cd]
  [bt] (3) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7fe117882c79]
  [bt] (4) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x461d09) [0x7fe117883d09]
  [bt] (5) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7fe1178e74f7]
  [bt] (6) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7fe117583ef0]
  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7fe2c0791e2e]
  [bt] (8) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7fe2c078e493]



Stack trace:
  [bt] (0) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7fe117f49f2a]
  [bt] (1) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb485c9) [0x7fe117f6a5c9]
  [bt] (2) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7fe117882c79]
  [bt] (3) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x461d09) [0x7fe117883d09]
  [bt] (4) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7fe1178e74f7]
  [bt] (5) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7fe117583ef0]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7fe2c0791e2e]
  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7fe2c078e493]
  [bt] (8) /usr/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0xa3e9) [0x7fe2bfedd3e9]


",XGBoostError,[18:12:04] /workspace/src/tree/updater_gpu_hist.cu:781: Exception in gpu_hist: [18:12:04] /workspace/src/tree/updater_gpu_hist.cu:787: Check failed: ctx_->gpu_id >= 0 (-1 vs. 0) : Must have at least one device,FALSE,No fix
NBspecific_4,f69e2754-c8c7-3e98-9f18-494fc9674d4d,nameerror,name 'sns' is not defined,"['---------------------------------------------------------------------------', 'NameError                                 Traceback (most recent call last)', ""/tmp/ipykernel_27/106132795.py in <module>\n----> 1 sns.set_context('talk', font_scale=.9)\n      2 # Exemple des types d'analyse qui peuvent √™tre effectu√©s\n      3 \n      4 # Count plot nous aide √† visualiser le nombre d'√©l√©ments par cat√©gorie\n      5 sns.countplot(data=dataset, x='Sex', hue='Risk')\n"", ""NameError: name 'sns' is not defined""]",/junobench_env/NBspecific_4/NBspecific_4_extension.ipynb,2026-01-14T18:15:42.544468,CellExecutionError,"An error occurred while executing the following cell:
------------------
# Importation des biblioth√®ques que nous utiliserons dans cette partie de la classe
from sklearn.model_selection import train_test_split, KFold, cross_val_score # pour s√©parer les donn√©es
from sklearn.metrics import accuracy_score, plot_confusion_matrix, classification_report, f1_score, precision_score, recall_score #Pour √©valuer notre mod√®le

from sklearn.model_selection import GridSearchCV

# Mod√®les d'algorithmes √† comparer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
# TODO: Ajouter ici tout nouveau mod√®le que vous souhaitez essayer (ANN, etc.)

# Cr√©ation des variables X et y
dataset_ready_x = dataset_ready.drop(['Risk_bad', 'Risk_good', 'Age', 'Sex_male'], axis='columns')
X = dataset_ready_x.values
feature_names = dataset_ready_x.columns

y = dataset_ready['Risk_bad'].values

# S√©paration de X et y en version d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mImportError[0m                               Traceback (most recent call last)
[0;32m<ipython-input-21-0d52f8edbe50>[0m in [0;36m<cell line: 3>[0;34m()[0m
[1;32m      1[0m [0;31m# Importation des biblioth√®ques que nous utiliserons dans cette partie de la classe[0m[0;34m[0m[0;34m[0m[0m
[1;32m      2[0m [0;32mfrom[0m [0msklearn[0m[0;34m.[0m[0mmodel_selection[0m [0;32mimport[0m [0mtrain_test_split[0m[0;34m,[0m [0mKFold[0m[0;34m,[0m [0mcross_val_score[0m [0;31m# pour s√©parer les donn√©es[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 3[0;31m [0;32mfrom[0m [0msklearn[0m[0;34m.[0m[0mmetrics[0m [0;32mimport[0m [0maccuracy_score[0m[0;34m,[0m [0mplot_confusion_matrix[0m[0;34m,[0m [0mclassification_report[0m[0;34m,[0m [0mf1_score[0m[0;34m,[0m [0mprecision_score[0m[0;34m,[0m [0mrecall_score[0m [0;31m#Pour √©valuer notre mod√®le[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m [0;32mfrom[0m [0msklearn[0m[0;34m.[0m[0mmodel_selection[0m [0;32mimport[0m [0mGridSearchCV[0m[0;34m[0m[0;34m[0m[0m

[0;31mImportError[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/usr/local/lib/python3.10/dist-packages/sklearn/metrics/__init__.py)
ImportError: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/usr/local/lib/python3.10/dist-packages/sklearn/metrics/__init__.py)
",ImportError,cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/usr/local/lib/python3.10/dist-packages/sklearn/metrics/__init__.py),FALSE,"fixed the error, could not resolve environment imports"
NBspecific_6,a4bec0b5-5bfe-35ed-8f24-eebeb07b1e4b,nameerror,name 'history' is not defined,"---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-17-91c7d6fec2eb> in <cell line: 1>()
----> 1 plt.plot(history.history['accuracy'],label='Train Accuracy')
      2 plt.plot(history.history['val_accuracy'],label='Validation Accuracy')
      3 plt.title('Accuracy Per epoch')
      4 plt.ylabel('Accuracy')
      5 plt.xlabel('Epoch')

NameError: name 'history' is not defined",/junobench_env/NBspecific_6/NBspecific_6_extension.ipynb,2026-01-14T16:39:15.013038,CellExecutionError,"An error occurred while executing the following cell:
------------------
history = model.fit(TrainBatch
          ,validation_data=ValidationBatch,
          epochs=2, verbose=1, callbacks=[model_checkpoint_callback])
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-17-8dea61041ba9>[0m in [0;36m<cell line: 1>[0;34m()[0m
[0;32m----> 1[0;31m history = model.fit(TrainBatch
[0m[1;32m      2[0m           [0;34m,[0m[0mvalidation_data[0m[0;34m=[0m[0mValidationBatch[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m           epochs=2, verbose=1, callbacks=[model_checkpoint_callback])

[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py[0m in [0;36merror_handler[0;34m(*args, **kwargs)[0m
[1;32m    120[0m             [0;31m# To get the full stack trace, call:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    121[0m             [0;31m# `keras.config.disable_traceback_filtering()`[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 122[0;31m             [0;32mraise[0m [0me[0m[0;34m.[0m[0mwith_traceback[0m[0;34m([0m[0mfiltered_tb[0m[0;34m)[0m [0;32mfrom[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    123[0m         [0;32mfinally[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    124[0m             [0;32mdel[0m [0mfiltered_tb[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py[0m in [0;36m_standardize_batch[0;34m(self, batch)[0m
[1;32m    199[0m             [0mbatch[0m [0;34m=[0m [0mtuple[0m[0;34m([0m[0mbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    200[0m         [0;32mif[0m [0;32mnot[0m [0misinstance[0m[0;34m([0m[0mbatch[0m[0;34m,[0m [0mtuple[0m[0;34m)[0m [0;32mor[0m [0mlen[0m[0;34m([0m[0mbatch[0m[0;34m)[0m [0;32mnot[0m [0;32min[0m [0;34m{[0m[0;36m1[0m[0;34m,[0m [0;36m2[0m[0;34m,[0m [0;36m3[0m[0;34m}[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 201[0;31m             raise ValueError(
[0m[1;32m    202[0m                 [0;34m""PyDataset.__getitem__() must return a tuple or a dict. ""[0m[0;34m[0m[0;34m[0m[0m
[1;32m    203[0m                 [0;34m""If a tuple, it must be ordered either ""[0m[0;34m[0m[0;34m[0m[0m

[0;31mValueError[0m: PyDataset.__getitem__() must return a tuple or a dict. If a tuple, it must be ordered either (input,) or (inputs, targets) or (inputs, targets, sample_weights). Received: None... of type <class 'NoneType'>
ValueError: PyDataset.__getitem__() must return a tuple or a dict. If a tuple, it must be ordered either (input,) or (inputs, targets) or (inputs, targets, sample_weights). Received: None... of type <class 'NoneType'>
",ValueError,"PyDataset.__getitem__() must return a tuple or a dict. If a tuple, it must be ordered either (input,) or (inputs, targets) or (inputs, targets, sample_weights). Received: None... of type <class 'NoneType'>",FALSE,fixed the error
NBspecific_10,6b1b4ed3-15c6-3c41-8afc-b6920d23d305,indexerror,"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices","['---------------------------------------------------------------------------', 'IndexError                                Traceback (most recent call last)', ""/tmp/ipykernel_23/1766986922.py in <module>\n      2 from statsmodels.tsa.stattools import adfuller\n      3 \n----> 4 check_stationarity(train['ActivePower'])\n"", 'IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices']",/junobench_env/NBspecific_10/NBspecific_10_extension.ipynb,2026-01-14T17:43:14.340190,CellExecutionError,"An error occurred while executing the following cell:
------------------
from keras.preprocessing.sequence import TimeseriesGenerator
import tensorflow as tf

train=TimeseriesGenerator(train,train,length=720,sampling_rate=1,batch_size=32)
test=TimeseriesGenerator(test,test,length=720,sampling_rate=1,batch_size=32)

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mImportError[0m                               Traceback (most recent call last)
[0;32m<ipython-input-125-1ce8a1846ada>[0m in [0;36m<cell line: 1>[0;34m()[0m
[0;32m----> 1[0;31m [0;32mfrom[0m [0mkeras[0m[0;34m.[0m[0mpreprocessing[0m[0;34m.[0m[0msequence[0m [0;32mimport[0m [0mTimeseriesGenerator[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0;32mimport[0m [0mtensorflow[0m [0;32mas[0m [0mtf[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m [0;34m[0m[0m
[1;32m      4[0m [0mtrain[0m[0;34m=[0m[0mTimeseriesGenerator[0m[0;34m([0m[0mtrain[0m[0;34m,[0m[0mtrain[0m[0;34m,[0m[0mlength[0m[0;34m=[0m[0;36m720[0m[0;34m,[0m[0msampling_rate[0m[0;34m=[0m[0;36m1[0m[0;34m,[0m[0mbatch_size[0m[0;34m=[0m[0;36m32[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0mtest[0m[0;34m=[0m[0mTimeseriesGenerator[0m[0;34m([0m[0mtest[0m[0;34m,[0m[0mtest[0m[0;34m,[0m[0mlength[0m[0;34m=[0m[0;36m720[0m[0;34m,[0m[0msampling_rate[0m[0;34m=[0m[0;36m1[0m[0;34m,[0m[0mbatch_size[0m[0;34m=[0m[0;36m32[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mImportError[0m: cannot import name 'TimeseriesGenerator' from 'keras.preprocessing.sequence' (/usr/local/lib/python3.10/dist-packages/keras/api/preprocessing/sequence/__init__.py)
ImportError: cannot import name 'TimeseriesGenerator' from 'keras.preprocessing.sequence' (/usr/local/lib/python3.10/dist-packages/keras/api/preprocessing/sequence/__init__.py)
",ImportError,cannot import name 'TimeseriesGenerator' from 'keras.preprocessing.sequence' (/usr/local/lib/python3.10/dist-packages/keras/api/preprocessing/sequence/__init__.py),FALSE,no fix
NBspecific_11,3ed26d39-0497-32e2-9470-074abb0d90af,typeerror,"Invalid shape (3, 224, 224) for image data","['---------------------------------------------------------------------------', 'TypeError                                 Traceback (most recent call last)', 'Cell In[36], line 1\n----> 1 plt.imshow(adv)\n', 'File /opt/conda/lib/python3.10/site-packages/matplotlib/pyplot.py:2695, in imshow(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\n   2690     return gca().minorticks_off()\n   2693 # Autogenerated by boilerplate.py.  Do not edit as changes will be lost.\n   2694 @_copy_docstring_and_deprecators(Axes.minorticks_on)\n-> 2695 def minorticks_on():\n   2696     return gca().minorticks_on()\n   2699 # Autogenerated by boilerplate.py.  Do not edit as changes will be lost.\n', 'File /opt/conda/lib/python3.10/site-packages/matplotlib/__init__.py:1442, in inner(ax, data, *args, **kwargs)\n   1440 inner.__doc__ = _add_data_doc(inner.__doc__, replace_names)\n   1441 inner.__signature__ = new_sig\n-> 1442 return inner\n', 'File /opt/conda/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5665, in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\n   5628 @_preprocess_data()\n   5629 @docstring.dedent_interpd\n   5630 def pcolor(self, *args, shading=None, alpha=None, norm=None, cmap=None,\n   5631            vmin=None, vmax=None, **kwargs):\n   5632     r""""""\n   5633     Create a pseudocolor plot with a non-regular rectangular grid.\n   5634 \n   5635     Call signature::\n   5636 \n   5637         pcolor([X, Y,] C, **kwargs)\n   5638 \n   5639     *X* and *Y* can be used to specify the corners of the quadrilaterals.\n   5640 \n   5641     .. hint::\n   5642 \n   5643         ``pcolor()`` can be very slow for large arrays. In most\n   5644         cases you should use the similar but much faster\n   5645         `~.Axes.pcolormesh` instead. See\n   5646         :ref:`Differences between pcolor() and pcolormesh()\n   5647         <differences-pcolor-pcolormesh>` for a discussion of the\n   5648         differences.\n   5649 \n   5650     Parameters\n   5651     ----------\n   5652     C : 2D array-like\n   5653         The color-mapped values.\n   5654 \n   5655     X, Y : array-like, optional\n   5656         The coordinates of the corners of quadrilaterals of a pcolormesh::\n   5657 \n   5658             (X[i+1, j], Y[i+1, j])       (X[i+1, j+1], Y[i+1, j+1])\n   5659                                   +-----+\n   5660                                   |     |\n   5661                                   +-----+\n   5662                 (X[i, j], Y[i, j])       (X[i, j+1], Y[i, j+1])\n   5663 \n   5664         Note that the column index corresponds to the x-coordinate, and\n-> 5665         the row index corresponds to y. For details, see the\n   5666         :ref:`Notes <axes-pcolormesh-grid-orientation>` section below.\n   5667 \n   5668         If ``shading=\'flat\'`` the dimensions of *X* and *Y* should be one\n   5669         greater than those of *C*, and the quadrilateral is colored due\n   5670         to the value at ``C[i, j]``.  If *X*, *Y* and *C* have equal\n   5671         dimensions, a warning will be raised and the last row and column\n   5672         of *C* will be ignored.\n   5673 \n   5674         If ``shading=\'nearest\'``, the dimensions of *X* and *Y* should be\n   5675         the same as those of *C* (if not, a ValueError will be raised). The\n   5676         color ``C[i, j]`` will be centered on ``(X[i, j], Y[i, j])``.\n   5677 \n   5678         If *X* and/or *Y* are 1-D arrays or column vectors they will be\n   5679         expanded as needed into the appropriate 2D arrays, making a\n   5680         rectangular grid.\n   5681 \n   5682     shading : {\'flat\', \'nearest\', \'auto\'}, default: :rc:`pcolor.shading`\n   5683         The fill style for the quadrilateral. Possible values:\n   5684 \n   5685         - \'flat\': A solid color is used for each quad. The color of the\n   5686           quad (i, j), (i+1, j), (i, j+1), (i+1, j+1) is given by\n   5687           ``C[i, j]``. The dimensions of *X* and *Y* should be\n   5688           one greater than those of *C*; if they are the same as *C*,\n   5689           then a deprecation warning is raised, and the last row\n   5690           and column of *C* are dropped.\n   5691         - \'nearest\': Each grid point will have a color centered on it,\n   5692           extending halfway between the adjacent grid centers.  The\n   5693           dimensions of *X* and *Y* must be the same as *C*.\n   5694         - \'auto\': Choose \'flat\' if dimensions of *X* and *Y* are one\n   5695           larger than *C*.  Choose \'nearest\' if dimensions are the same.\n   5696 \n   5697         See :doc:`/gallery/images_contours_and_fields/pcolormesh_grids`\n   5698         for more description.\n   5699 \n   5700     cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n   5701         A Colormap instance or registered colormap name. The colormap\n   5702         maps the *C* values to colors.\n   5703 \n   5704     norm : `~matplotlib.colors.Normalize`, optional\n   5705         The Normalize instance scales the data values to the canonical\n   5706         colormap range [0, 1] for mapping to colors. By default, the data\n   5707         range is mapped to the colorbar range using linear scaling.\n   5708 \n   5709     vmin, vmax : float, default: None\n   5710         The colorbar range. If *None*, suitable min/max values are\n   5711         automatically chosen by the `.Normalize` instance (defaults to\n   5712         the respective min/max values of *C* in case of the default linear\n   5713         scaling).\n   5714         It is an error to use *vmin*/*vmax* when *norm* is given.\n   5715 \n   5716     edgecolors : {\'none\', None, \'face\', color, color sequence}, optional\n   5717         The color of the edges. Defaults to \'none\'. Possible values:\n   5718 \n   5719         - \'none\' or \'\': No edge.\n   5720         - *None*: :rc:`patch.edgecolor` will be used. Note that currently\n   5721           :rc:`patch.force_edgecolor` has to be True for this to work.\n   5722         - \'face\': Use the adjacent face color.\n   5723         - A color or sequence of colors will set the edge color.\n   5724 \n   5725         The singular form *edgecolor* works as an alias.\n   5726 \n   5727     alpha : float, default: None\n   5728         The alpha blending value of the face color, between 0 (transparent)\n   5729         and 1 (opaque). Note: The edgecolor is currently not affected by\n   5730         this.\n   5731 \n   5732     snap : bool, default: False\n   5733         Whether to snap the mesh to pixel boundaries.\n   5734 \n   5735     Returns\n   5736     -------\n   5737     `matplotlib.collections.Collection`\n   5738 \n   5739     Other Parameters\n   5740     ----------------\n   5741     antialiaseds : bool, default: False\n   5742         The default *antialiaseds* is False if the default\n   5743         *edgecolors*\\ =""none"" is used.  This eliminates artificial lines\n   5744         at patch boundaries, and works regardless of the value of alpha.\n   5745         If *edgecolors* is not ""none"", then the default *antialiaseds*\n   5746         is taken from :rc:`patch.antialiased`.\n   5747         Stroking the edges may be preferred if *alpha* is 1, but will\n   5748         cause artifacts otherwise.\n   5749 \n   5750     data : indexable object, optional\n   5751         DATA_PARAMETER_PLACEHOLDER\n   5752 \n   5753     **kwargs\n   5754         Additionally, the following arguments are allowed. They are passed\n   5755         along to the `~matplotlib.collections.PolyCollection` constructor:\n   5756 \n   5757     %(PolyCollection:kwdoc)s\n   5758 \n   5759     See Also\n   5760     --------\n   5761     pcolormesh : for an explanation of the differences between\n   5762         pcolor and pcolormesh.\n   5763     imshow : If *X* and *Y* are each equidistant, `~.Axes.imshow` can be a\n   5764         faster alternative.\n   5765 \n   5766     Notes\n   5767     -----\n   5768     **Masked arrays**\n   5769 \n   5770     *X*, *Y* and *C* may be masked arrays. If either ``C[i, j]``, or one\n   5771     of the vertices surrounding ``C[i, j]`` (*X* or *Y* at\n   5772     ``[i, j], [i+1, j], [i, j+1], [i+1, j+1]``) is masked, nothing is\n   5773     plotted.\n   5774 \n   5775     .. _axes-pcolor-grid-orientation:\n   5776 \n   5777     **Grid orientation**\n   5778 \n   5779     The grid orientation follows the standard matrix convention: An array\n   5780     *C* with shape (nrows, ncolumns) is plotted with the column number as\n   5781     *X* and the row number as *Y*.\n   5782     """"""\n   5784     if shading is None:\n   5785         shading = rcParams[\'pcolor.shading\']\n', 'File /opt/conda/lib/python3.10/site-packages/matplotlib/image.py:710, in set_data(self, A)\n    704 if (self._A.dtype != np.uint8 and\n    705         not np.can_cast(self._A.dtype, float, ""same_kind"")):\n    706     raise TypeError(""Image data of dtype {} cannot be converted to ""\n    707                     ""float"".format(self._A.dtype))\n    709 if self._A.ndim == 3 and self._A.shape[-1] == 1:\n--> 710     # If just one dimension assume scalar and apply colormap\n    711     self._A = self._A[:, :, 0]\n    713 if not (self._A.ndim == 2\n    714         or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n', 'TypeError: Invalid shape (3, 224, 224) for image data']",/junobench_env/NBspecific_11/NBspecific_11_extension.ipynb,2026-01-14T17:48:36.862302,CellExecutionError,"An error occurred while executing the following cell:
------------------
import advbox
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m<ipython-input-21-f50ab8c978b3>[0m in [0;36m<cell line: 1>[0;34m()[0m
[0;32m----> 1[0;31m [0;32mimport[0m [0madvbox[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;31mModuleNotFoundError[0m: No module named 'advbox'
ModuleNotFoundError: No module named 'advbox'
",ModuleNotFoundError,No module named 'advbox',FALSE,fixed the eror
NBspecific_13,ea3fa3e6-48ec-3cb2-aae2-f614cc9087ef,nameerror,name 'X' is not defined,"['---------------------------------------------------------------------------', 'NameError                                 Traceback (most recent call last)', 'Cell In[1], line 3\n      1 from sklearn.tree import DecisionTreeClassifier\n      2 model = DecisionTreeClassifier()\n----> 3 model .fit(X,y)\n      4 y_hat = model.predict(X)\n      5 print ( accuracy_score(y_hat,y))\n', ""NameError: name 'X' is not defined""]",/junobench_env/NBspecific_13/NBspecific_13_extension.ipynb,2026-01-14T18:30:14.944529,CellExecutionError,"An error occurred while executing the following cell:
------------------
y= df['output']
x = df.drop(['output'],axis = 1)
X_train , X_test, y_train , y_test = train_test_split(X,y)
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier
model.fit(X_train , y_train)
y_hat= model.predict (X_test)
print (accuracy_score (y_hat, y_test))
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mTypeError[0m                                 Traceback (most recent call last)
[0;32m<ipython-input-33-89f36b1f32fa>[0m in [0;36m<cell line: 7>[0;34m()[0m
[1;32m      5[0m [0;34m[0m[0m
[1;32m      6[0m [0mmodel[0m [0;34m=[0m [0mDecisionTreeClassifier[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 7[0;31m [0mmodel[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0mX_train[0m [0;34m,[0m [0my_train[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      8[0m [0my_hat[0m[0;34m=[0m [0mmodel[0m[0;34m.[0m[0mpredict[0m [0;34m([0m[0mX_test[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      9[0m [0mprint[0m [0;34m([0m[0maccuracy_score[0m [0;34m([0m[0my_hat[0m[0;34m,[0m [0my_test[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mTypeError[0m: DecisionTreeClassifier.fit() missing 1 required positional argument: 'y'
TypeError: DecisionTreeClassifier.fit() missing 1 required positional argument: 'y'
",TypeError,DecisionTreeClassifier.fit() missing 1 required positional argument: 'y',FALSE,fixed the error
NBspecific_14,9dae5005-d2d2-3e01-adc5-699951ac5338,nameerror,name 'train_data' is not defined,"['---------------------------------------------------------------------------', 'NameError                                 Traceback (most recent call last)', 'Cell In[1], line 3\n      1 from sklearn.ensemble import RandomForestClassifier\n----> 3 y = train_data[""Survived""]\n      5 features = [""Pclass"", ""Sex"", ""SibSp"", ""Parch"", ""Fare""]\n      6 X = pd.get_dummies(train_data[features])\n', ""NameError: name 'train_data' is not defined""]",/junobench_env/NBspecific_14/NBspecific_14_extension.ipynb,2026-01-14T18:30:00.675141,CellExecutionError,"An error occurred while executing the following cell:
------------------
from sklearn.ensemble import RandomForestClassifier

y = train_data[""Survived""]

features = [""Pclass"", ""Sex"", ""SibSp"", ""Parch"", ""Fare""]
X = pd.get_dummies(train_data[features])
X_test = pd.get_dummies(test_data[features])

model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)
model.fit(X, y)
predictions = model.predict(X_test)

output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})
output.to_csv('submission.csv', index=False)
print(""Your submission was successfully saved!"")
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-6-35e2f8bf0a65>[0m in [0;36m<cell line: 11>[0;34m()[0m
[1;32m      9[0m [0mmodel[0m [0;34m=[0m [0mRandomForestClassifier[0m[0;34m([0m[0mn_estimators[0m[0;34m=[0m[0;36m100[0m[0;34m,[0m [0mmax_depth[0m[0;34m=[0m[0;36m5[0m[0;34m,[0m [0mrandom_state[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     10[0m [0mmodel[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0mX[0m[0;34m,[0m [0my[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 11[0;31m [0mpredictions[0m [0;34m=[0m [0mmodel[0m[0;34m.[0m[0mpredict[0m[0;34m([0m[0mX_test[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     12[0m [0;34m[0m[0m
[1;32m     13[0m [0moutput[0m [0;34m=[0m [0mpd[0m[0;34m.[0m[0mDataFrame[0m[0;34m([0m[0;34m{[0m[0;34m'PassengerId'[0m[0;34m:[0m [0mtest_data[0m[0;34m.[0m[0mPassengerId[0m[0;34m,[0m [0;34m'Survived'[0m[0;34m:[0m [0mpredictions[0m[0;34m}[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py[0m in [0;36mpredict[0;34m(self, X)[0m
[1;32m    818[0m             [0mThe[0m [0mpredicted[0m [0mclasses[0m[0;34m.[0m[0;34m[0m[0;34m[0m[0m
[1;32m    819[0m         """"""
[0;32m--> 820[0;31m         [0mproba[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mpredict_proba[0m[0;34m([0m[0mX[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    821[0m [0;34m[0m[0m
[1;32m    822[0m         [0;32mif[0m [0mself[0m[0;34m.[0m[0mn_outputs_[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py[0m in [0;36mpredict_proba[0;34m(self, X)[0m
[1;32m    860[0m         [0mcheck_is_fitted[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    861[0m         [0;31m# Check data[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 862[0;31m         [0mX[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_validate_X_predict[0m[0;34m([0m[0mX[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    863[0m [0;34m[0m[0m
[1;32m    864[0m         [0;31m# Assign chunk of trees to jobs[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py[0m in [0;36m_validate_X_predict[0;34m(self, X)[0m
[1;32m    600[0m         Validate X whenever one tries to predict, apply, predict_proba.""""""
[1;32m    601[0m         [0mcheck_is_fitted[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 602[0;31m         [0mX[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_validate_data[0m[0;34m([0m[0mX[0m[0;34m,[0m [0mdtype[0m[0;34m=[0m[0mDTYPE[0m[0;34m,[0m [0maccept_sparse[0m[0;34m=[0m[0;34m""csr""[0m[0;34m,[0m [0mreset[0m[0;34m=[0m[0;32mFalse[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    603[0m         [0;32mif[0m [0missparse[0m[0;34m([0m[0mX[0m[0;34m)[0m [0;32mand[0m [0;34m([0m[0mX[0m[0;34m.[0m[0mindices[0m[0;34m.[0m[0mdtype[0m [0;34m!=[0m [0mnp[0m[0;34m.[0m[0mintc[0m [0;32mor[0m [0mX[0m[0;34m.[0m[0mindptr[0m[0;34m.[0m[0mdtype[0m [0;34m!=[0m [0mnp[0m[0;34m.[0m[0mintc[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    604[0m             [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m""No support for np.int64 index based sparse matrices""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py[0m in [0;36m_validate_data[0;34m(self, X, y, reset, validate_separately, **check_params)[0m
[1;32m    563[0m             [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m""Validation should be done on X, y or both.""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    564[0m         [0;32melif[0m [0;32mnot[0m [0mno_val_X[0m [0;32mand[0m [0mno_val_y[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 565[0;31m             [0mX[0m [0;34m=[0m [0mcheck_array[0m[0;34m([0m[0mX[0m[0;34m,[0m [0minput_name[0m[0;34m=[0m[0;34m""X""[0m[0;34m,[0m [0;34m**[0m[0mcheck_params[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    566[0m             [0mout[0m [0;34m=[0m [0mX[0m[0;34m[0m[0;34m[0m[0m
[1;32m    567[0m         [0;32melif[0m [0mno_val_X[0m [0;32mand[0m [0;32mnot[0m [0mno_val_y[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py[0m in [0;36mcheck_array[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)[0m
[1;32m    919[0m [0;34m[0m[0m
[1;32m    920[0m         [0;32mif[0m [0mforce_all_finite[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 921[0;31m             _assert_all_finite(
[0m[1;32m    922[0m                 [0marray[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m    923[0m                 [0minput_name[0m[0;34m=[0m[0minput_name[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py[0m in [0;36m_assert_all_finite[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)[0m
[1;32m    159[0m                 [0;34m""#estimators-that-handle-nan-values""[0m[0;34m[0m[0;34m[0m[0m
[1;32m    160[0m             )
[0;32m--> 161[0;31m         [0;32mraise[0m [0mValueError[0m[0;34m([0m[0mmsg_err[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    162[0m [0;34m[0m[0m
[1;32m    163[0m [0;34m[0m[0m

[0;31mValueError[0m: Input X contains NaN.
RandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values
ValueError: Input X contains NaN.
RandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values
",ValueError,Input X contains NaN.,FALSE,fixed the error
NBspecific_15,ee95ab77-7991-30cf-adcd-c6d007019937,nameerror,name 'src_images' is not defined,"['---------------------------------------------------------------------------', 'NameError                                 Traceback (most recent call last)', '<ipython-input-5-d6e830721ac4> in <module>\n      1 import numpy as np\n      2 import cv2\n----> 3 random_img = np.random.randint(0 , len(src_images))\n      4 fig = plt.figure(figsize = (16, 16))\n      5 src_img = np.reshape(src_images[random_img] , (256 , 256 ,3))\n', ""NameError: name 'src_images' is not defined""]",/junobench_env/NBspecific_15/NBspecific_15_extension.ipynb,2026-01-14T18:32:39.622642,CellExecutionError,"An error occurred while executing the following cell:
------------------
from google.colab import drive
drive.mount('/content/drive')
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mNotImplementedError[0m                       Traceback (most recent call last)
[0;32m<ipython-input-11-d5df0069828e>[0m in [0;36m<cell line: 2>[0;34m()[0m
[1;32m      1[0m [0;32mfrom[0m [0mgoogle[0m[0;34m.[0m[0mcolab[0m [0;32mimport[0m [0mdrive[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 2[0;31m [0mdrive[0m[0;34m.[0m[0mmount[0m[0;34m([0m[0;34m'/content/drive'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py[0m in [0;36mmount[0;34m(mountpoint, force_remount, timeout_ms, readonly)[0m
[1;32m     98[0m [0;32mdef[0m [0mmount[0m[0;34m([0m[0mmountpoint[0m[0;34m,[0m [0mforce_remount[0m[0;34m=[0m[0;32mFalse[0m[0;34m,[0m [0mtimeout_ms[0m[0;34m=[0m[0;36m120000[0m[0;34m,[0m [0mreadonly[0m[0;34m=[0m[0;32mFalse[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m     99[0m   [0;34m""""""Mount your Google Drive at the specified mountpoint path.""""""[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 100[0;31m   return _mount(
[0m[1;32m    101[0m       [0mmountpoint[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m    102[0m       [0mforce_remount[0m[0;34m=[0m[0mforce_remount[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py[0m in [0;36m_mount[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)[0m
[1;32m    116[0m   [0;34m""""""Internal helper to mount Google Drive.""""""[0m[0;34m[0m[0;34m[0m[0m
[1;32m    117[0m   [0;32mif[0m [0;32mnot[0m [0m_os[0m[0;34m.[0m[0mpath[0m[0;34m.[0m[0mexists[0m[0;34m([0m[0;34m'/var/colab/hostname'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 118[0;31m     raise NotImplementedError(
[0m[1;32m    119[0m         [0;34m'Mounting drive is unsupported in this environment. Use PyDrive'[0m[0;34m[0m[0;34m[0m[0m
[1;32m    120[0m         [0;34m' instead. See examples at'[0m[0;34m[0m[0;34m[0m[0m

[0;31mNotImplementedError[0m: Mounting drive is unsupported in this environment. Use PyDrive instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2.
NotImplementedError: Mounting drive is unsupported in this environment. Use PyDrive instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2.
",NotImplementedError,Mounting drive is unsupported in this environment. Use PyDrive instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2.,FALSE,fixed the error
NBspecific_16,12eb745c-9342-33a2-9553-38a616ab2f03,nameerror,name 'skin_df' is not defined,"['---------------------------------------------------------------------------', 'NameError                                 Traceback (most recent call last)', 'Cell In[2], line 1\n----> 1 skin_df.head()\n', ""NameError: name 'skin_df' is not defined""]",/junobench_env/NBspecific_16/NBspecific_16_extension.ipynb,2026-01-14T17:46:39.765527,CellExecutionError,"An error occurred while executing the following cell:
------------------
hidden_reps = model.layers[-4].output  
hidden_model = Model(inputs=model.input, outputs=hidden_reps)
hidden_reps = hidden_model.predict(x_train)
x_test_hidden_reps = hidden_model.predict(x_test)
num_features_from_cnn = hidden_reps.shape[1]
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-15-0603b0500161>[0m in [0;36m<cell line: 2>[0;34m()[0m
[1;32m      1[0m [0mhidden_reps[0m [0;34m=[0m [0mmodel[0m[0;34m.[0m[0mlayers[0m[0;34m[[0m[0;34m-[0m[0;36m4[0m[0;34m][0m[0;34m.[0m[0moutput[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 2[0;31m [0mhidden_model[0m [0;34m=[0m [0mModel[0m[0;34m([0m[0minputs[0m[0;34m=[0m[0mmodel[0m[0;34m.[0m[0minput[0m[0;34m,[0m [0moutputs[0m[0;34m=[0m[0mhidden_reps[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      3[0m [0mhidden_reps[0m [0;34m=[0m [0mhidden_model[0m[0;34m.[0m[0mpredict[0m[0;34m([0m[0mx_train[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0mx_test_hidden_reps[0m [0;34m=[0m [0mhidden_model[0m[0;34m.[0m[0mpredict[0m[0;34m([0m[0mx_test[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0mnum_features_from_cnn[0m [0;34m=[0m [0mhidden_reps[0m[0;34m.[0m[0mshape[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py[0m in [0;36minput[0;34m(self)[0m
[1;32m    252[0m             [0mInput[0m [0mtensor[0m [0;32mor[0m [0mlist[0m [0mof[0m [0minput[0m [0mtensors[0m[0;34m.[0m[0;34m[0m[0;34m[0m[0m
[1;32m    253[0m         """"""
[0;32m--> 254[0;31m         [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_get_node_attribute_at_index[0m[0;34m([0m[0;36m0[0m[0;34m,[0m [0;34m""input_tensors""[0m[0;34m,[0m [0;34m""input""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    255[0m [0;34m[0m[0m
[1;32m    256[0m     [0;34m@[0m[0mproperty[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py[0m in [0;36m_get_node_attribute_at_index[0;34m(self, node_index, attr, attr_name)[0m
[1;32m    283[0m         """"""
[1;32m    284[0m         [0;32mif[0m [0;32mnot[0m [0mself[0m[0;34m.[0m[0m_inbound_nodes[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 285[0;31m             raise ValueError(
[0m[1;32m    286[0m                 [0;34mf""The layer {self.name} has never been called ""[0m[0;34m[0m[0;34m[0m[0m
[1;32m    287[0m                 [0;34mf""and thus has no defined {attr_name}.""[0m[0;34m[0m[0;34m[0m[0m

[0;31mValueError[0m: The layer sequential has never been called and thus has no defined input.
ValueError: The layer sequential has never been called and thus has no defined input.
",ValueError,The layer sequential has never been called and thus has no defined input.,FALSE,fixed the eror
NBspecific_19,493b11b1-5b97-3d24-bdac-2bf9b717dd42,nameerror,name 'torch' is not defined,"['---------------------------------------------------------------------------', 'NameError                                 Traceback (most recent call last)', 'Cell In[1], line 1\n----> 1 X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n      2 predictions = model(X_test_tensor)\n      3 _, predicted_labels = torch.max(predictions, 1)\n', ""NameError: name 'torch' is not defined""]",/junobench_env/NBspecific_19/NBspecific_19_extension.ipynb,2026-01-14T17:38:22.655985,CellExecutionError,"An error occurred while executing the following cell:
------------------
# Extract zip file into folder containing image files
import py7zr

# with py7zr.SevenZipFile('/kaggle/input/cifar-10/test.7z', mode='r') as archive:
#     archive.extractall(path='/kaggle/working/test_data_folder')

# with py7zr.SevenZipFile('/kaggle/input/cifar-10/train.7z', mode='r') as archive:
#     archive.extractall(path='/kaggle/working/train_data_folder')
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mImportError[0m                               Traceback (most recent call last)
[0;32m<ipython-input-3-2c014ed901a8>[0m in [0;36m<cell line: 2>[0;34m()[0m
[1;32m      1[0m [0;31m# Extract zip file into folder containing image files[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 2[0;31m [0;32mimport[0m [0mpy7zr[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      3[0m [0;34m[0m[0m
[1;32m      4[0m [0;31m# with py7zr.SevenZipFile('/kaggle/input/cifar-10/test.7z', mode='r') as archive:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0;31m#     archive.extractall(path='/kaggle/working/test_data_folder')[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/py7zr/__init__.py[0m in [0;36m<module>[0;34m[0m
[1;32m     44[0m     [0mPRESET_EXTREME[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m     45[0m )
[0;32m---> 46[0;31m [0;32mfrom[0m [0mpy7zr[0m[0;34m.[0m[0mpy7zr[0m [0;32mimport[0m [0mArchiveInfo[0m[0;34m,[0m [0mFileInfo[0m[0;34m,[0m [0mSevenZipFile[0m[0;34m,[0m [0mis_7zfile[0m[0;34m,[0m [0mpack_7zarchive[0m[0;34m,[0m [0munpack_7zarchive[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     47[0m [0;32mfrom[0m [0mpy7zr[0m[0;34m.[0m[0mversion[0m [0;32mimport[0m [0m__version__[0m[0;34m[0m[0;34m[0m[0m
[1;32m     48[0m [0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/py7zr/py7zr.py[0m in [0;36m<module>[0;34m[0m
[1;32m     48[0m [0;32mimport[0m [0mmultivolumefile[0m[0;34m[0m[0;34m[0m[0m
[1;32m     49[0m [0;34m[0m[0m
[0;32m---> 50[0;31m [0;32mfrom[0m [0mpy7zr[0m[0;34m.[0m[0marchiveinfo[0m [0;32mimport[0m [0mFolder[0m[0;34m,[0m [0mHeader[0m[0;34m,[0m [0mSignatureHeader[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     51[0m [0;32mfrom[0m [0mpy7zr[0m[0;34m.[0m[0mcallbacks[0m [0;32mimport[0m [0mExtractCallback[0m[0;34m[0m[0;34m[0m[0m
[1;32m     52[0m [0;32mfrom[0m [0mpy7zr[0m[0;34m.[0m[0mcompressor[0m [0;32mimport[0m [0mSupportedMethods[0m[0;34m,[0m [0mget_methods_names[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/py7zr/archiveinfo.py[0m in [0;36m<module>[0;34m[0m
[1;32m     34[0m [0;32mfrom[0m [0mtyping[0m [0;32mimport[0m [0mAny[0m[0;34m,[0m [0mBinaryIO[0m[0;34m,[0m [0mOptional[0m[0;34m,[0m [0mUnion[0m[0;34m[0m[0;34m[0m[0m
[1;32m     35[0m [0;34m[0m[0m
[0;32m---> 36[0;31m [0;32mfrom[0m [0mpy7zr[0m[0;34m.[0m[0mcompressor[0m [0;32mimport[0m [0mSevenZipCompressor[0m[0;34m,[0m [0mSevenZipDecompressor[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     37[0m [0;32mfrom[0m [0mpy7zr[0m[0;34m.[0m[0mexceptions[0m [0;32mimport[0m [0mBad7zFile[0m[0;34m[0m[0;34m[0m[0m
[1;32m     38[0m [0;32mfrom[0m [0mpy7zr[0m[0;34m.[0m[0mhelpers[0m [0;32mimport[0m [0mArchiveTimestamp[0m[0;34m,[0m [0mcalculate_crc32[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/py7zr/compressor.py[0m in [0;36m<module>[0;34m[0m
[1;32m     77[0m     [0;32mfrom[0m [0mcompression[0m [0;32mimport[0m [0mzstd[0m[0;34m[0m[0;34m[0m[0m
[1;32m     78[0m [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 79[0;31m     [0;32mfrom[0m [0mbackports[0m [0;32mimport[0m [0mzstd[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     80[0m [0;34m[0m[0m
[1;32m     81[0m [0;34m[0m[0m

[0;31mImportError[0m: cannot import name 'zstd' from 'backports' (/usr/local/lib/python3.10/dist-packages/setuptools/_vendor/backports/__init__.py)
ImportError: cannot import name 'zstd' from 'backports' (/usr/local/lib/python3.10/dist-packages/setuptools/_vendor/backports/__init__.py)
",ImportError,cannot import name 'zstd' from 'backports' (/usr/local/lib/python3.10/dist-packages/setuptools/_vendor/backports/__init__.py),FALSE,fixed the error
tensorflow_2,cba13bab-dd7e-39ea-a4c9-3f6c40fb26fc,invalidargumenterror,Graph execution error:,"['---------------------------------------------------------------------------', 'InvalidArgumentError                      Traceback (most recent call last)', 'Cell In[46], line 11\n      9     return model\n     10 model = prepare_model()\n---> 11 model.fit_generator(train_generator,\n     12                     validation_data = valid_generator,\n     13                     epochs=5)\n     14 model.evaluate(test_generator)\n', 'File /opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:2810, in Model.fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\n   2798 """"""Fits the model on data yielded batch-by-batch by a Python generator.\n   2799 \n   2800 DEPRECATED:\n   2801   `Model.fit` now supports generators, so there is no longer any need to\n   2802   use this endpoint.\n   2803 """"""\n   2804 warnings.warn(\n   2805     ""`Model.fit_generator` is deprecated and ""\n   2806     ""will be removed in a future version. ""\n   2807     ""Please use `Model.fit`, which supports generators."",\n   2808     stacklevel=2,\n   2809 )\n-> 2810 return self.fit(\n   2811     generator,\n   2812     steps_per_epoch=steps_per_epoch,\n   2813     epochs=epochs,\n   2814     verbose=verbose,\n   2815     callbacks=callbacks,\n   2816     validation_data=validation_data,\n   2817     validation_steps=validation_steps,\n   2818     validation_freq=validation_freq,\n   2819     class_weight=class_weight,\n   2820     max_queue_size=max_queue_size,\n   2821     workers=workers,\n   2822     use_multiprocessing=use_multiprocessing,\n   2823     shuffle=shuffle,\n   2824     initial_epoch=initial_epoch,\n   2825 )\n', 'File /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n     68     # To get the full stack trace, call:\n     69     # `tf.debugging.disable_traceback_filtering()`\n---> 70     raise e.with_traceback(filtered_tb) from None\n     71 finally:\n     72     del filtered_tb\n', 'File /opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\n     51 try:\n     52   ctx.ensure_initialized()\n---> 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n     54                                       inputs, attrs, num_outputs)\n     55 except core._NotOkStatusException as e:\n     56   if name is not None:\n', 'InvalidArgumentError: Graph execution error:\n\nDetected at node \'sequential_5/dense_10/Relu\' defined at (most recent call last):\n    File ""/opt/conda/lib/python3.10/runpy.py"", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File ""/opt/conda/lib/python3.10/runpy.py"", line 86, in _run_code\n      exec(code, run_globals)\n    File ""/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py"", line 17, in <module>\n      app.launch_new_instance()\n    File ""/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py"", line 1043, in launch_instance\n      app.start()\n    File ""/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py"", line 736, in start\n      self.io_loop.start()\n    File ""/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py"", line 195, in start\n      self.asyncio_loop.run_forever()\n    File ""/opt/conda/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever\n      self._run_once()\n    File ""/opt/conda/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once\n      handle._run()\n    File ""/opt/conda/lib/python3.10/asyncio/events.py"", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File ""/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 516, in dispatch_queue\n      await self.process_one()\n    File ""/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 505, in process_one\n      await dispatch(*args)\n    File ""/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 412, in dispatch_shell\n      await result\n    File ""/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 740, in execute_request\n      reply_content = await reply_content\n    File ""/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 422, in do_execute\n      res = shell.run_cell(\n    File ""/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py"", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File ""/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3009, in run_cell\n      result = self._run_cell(\n    File ""/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3064, in _run_cell\n      result = runner(coro)\n    File ""/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File ""/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File ""/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File ""/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File ""/tmp/ipykernel_42/8764930.py"", line 11, in <module>\n      model.fit_generator(train_generator,\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py"", line 2810, in fit_generator\n      return self.fit(\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1338, in train_function\n      return step_function(self, iterator)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1303, in run_step\n      outputs = model.train_step(data)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1080, in train_step\n      y_pred = self(x, training=True)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py"", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/engine/sequential.py"", line 405, in call\n      return super().call(inputs, training=training, mask=mask)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py"", line 255, in call\n      outputs = self.activation(outputs)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/activations.py"", line 321, in relu\n      return backend.relu(\n    File ""/opt/conda/lib/python3.10/site-packages/keras/src/backend.py"", line 5397, in relu\n      x = tf.nn.relu(x)\nNode: \'sequential_5/dense_10/Relu\'\nMatrix size-incompatible: In[0]: [30,387200], In[1]: [76832,16]\n\t [[{{node sequential_5/dense_10/Relu}}]] [Op:__inference_train_function_5471]']",/junobench_env/tensorflow_2/tensorflow_2_extension.ipynb,2026-01-14T17:38:12.325447,CellExecutionError,"An error occurred while executing the following cell:
------------------
def prepare_model():
    model = Sequential()
    model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(100, 100, 3)))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dense(256, activation='relu'))
    model.add(Dense(2, activation='softmax'))
    model.compile(loss=""categorical_crossentropy"",optimizer=""adam"",metrics=['accuracy'])
    return model
model = prepare_model()
model.fit_generator(train_generator,
                    validation_data = valid_generator,
#                     steps_per_epoch = train_generator.n//train_generator.batch_size,
#                     validation_steps = valid_generator.n//valid_generator.batch_size,
                    epochs=5)
model.evaluate(test_generator)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mAttributeError[0m                            Traceback (most recent call last)
[0;32m<ipython-input-8-8c089253bbc3>[0m in [0;36m<cell line: 12>[0;34m()[0m
[1;32m     10[0m     [0;32mreturn[0m [0mmodel[0m[0;34m[0m[0;34m[0m[0m
[1;32m     11[0m [0mmodel[0m [0;34m=[0m [0mprepare_model[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 12[0;31m model.fit_generator(train_generator,
[0m[1;32m     13[0m                     [0mvalidation_data[0m [0;34m=[0m [0mvalid_generator[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m     14[0m [0;31m#                     steps_per_epoch = train_generator.n//train_generator.batch_size,[0m[0;34m[0m[0;34m[0m[0m

[0;31mAttributeError[0m: 'Sequential' object has no attribute 'fit_generator'
AttributeError: 'Sequential' object has no attribute 'fit_generator'
",AttributeError,'Sequential' object has no attribute 'fit_generator',FALSE,fixed the error
tensorflow_4,8d9101a0-6ddc-35a0-8472-a60bf59a1ff0,valueerror,Exception encountered when calling layer 'sequential' (type Sequential).,"['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', ""Cell In[17], line 29\n     26 early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n     28 # train the model\n---> 29 history = model.fit(train_data, train_labels, batch_size=32, epochs=10, validation_split=.2, callbacks=[early_stop])\n     31 # plot accuracy and loss\n     32 import matplotlib.pyplot as plt\n"", 'File /opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n     68     # To get the full stack trace, call:\n     69     # `tf.debugging.disable_traceback_filtering()`\n---> 70     raise e.with_traceback(filtered_tb) from None\n     71 finally:\n     72     del filtered_tb\n', 'File /tmp/__autograph_generated_filep2cz96wq.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)\n     13 try:\n     14     do_return = True\n---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n     16 except:\n     17     do_return = False\n', 'ValueError: in user code:\n\n    File ""/opt/conda/lib/python3.10/site-packages/keras/engine/training.py"", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/engine/training.py"", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File ""/opt/conda/lib/python3.10/site-packages/keras/engine/training.py"", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/engine/training.py"", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File ""/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py"", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \'sequential\' (type Sequential).\n    \n    Input 0 of layer ""lstm"" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 1, 224, 224)\n    \n    Call arguments received by layer \'sequential\' (type Sequential):\n      ‚Ä¢ inputs=tf.Tensor(shape=(None, 1, 224, 224), dtype=float32)\n      ‚Ä¢ training=True\n      ‚Ä¢ mask=None\n']",/junobench_env/tensorflow_4/tensorflow_4_extension.ipynb,2026-01-14T17:44:15.145286,CellExecutionError,"An error occurred while executing the following cell:
------------------
def find_files_in_folder(path, folder_name):
    folder_path = None
    for root, dirs, files in os.walk(path):
        if folder_name in dirs:
            folder_path = os.path.join(root, folder_name)
            break
    img1 = []
    if folder_path:
        files = os.listdir(folder_path)
        for file in files:
            folder_name = os.path.dirname(file)
            file_name = os.path.basename(file)
            img1.append(file_name)
        return img1
    else:
        return None
    

train_data_names = []
test_data_names = []

train_data = []
train_labels = []

real_images = []
forged_images = []

# for per in os.listdir(train_dir):
#     for data in glob.glob(train_dir+'/'+per+'/*.*'):
        
#         train_data_names.append(data)
        
#         if per[-1]=='g':
#             img = cv2.imread(data)
#             img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
#             img = cv2.resize(img, (SIZE,SIZE))
#             forged_images.append([img])
# #             train_labels.append(np.array(1))
#         else:
#             img = cv2.imread(data)
#             img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
#             img = cv2.resize(img, (SIZE,SIZE))
#             real_images.append([img])
#             train_labels.append(np.array(0))

real_images_train, forged_images_train = [], []
for per in os.listdir(train_dir):
    for data in glob.glob(train_dir+'/'+per+'/*.*'):
        img = cv2.imread(data)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        img = cv2.resize(img, (SIZE, SIZE))
        if per[-1] == 'f':
            forged_images_train.append(img)
        else:
            real_images_train.append(img)

# train_data = np.array(train_data)/255.0
# train_labels = np.array(train_labels)
# print(""number of real_images"",len(real_images))

train_data = np.concatenate([real_images_train, forged_images_train])
train_labels = np.concatenate([np.zeros(len(real_images_train)), np.ones(len(forged_images_train))])
print(""number of real_images"",len(real_images))
#Test Data

test_data = []
test_labels = []
real_images_test, forged_images_test = [], []
# for per in os.listdir(test_dir):
#     for data in glob.glob(test_dir+'/'+per+'/*.*'):
#         test_data_names.append(data)
        
#         if per[-1]=='g':
#             img = cv2.imread(data)
#             img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
#             img = cv2.resize(img, (SIZE,SIZE))
#             forged_images.append([img])
# #             test_labels.append(np.array(1))
#         else:
#             img = cv2.imread(data)
#             img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
#             img = cv2.resize(img, (SIZE,SIZE))
#             real_images.append([img])
#             test_labels.append(np.array(0))

# Populate testing images
for per in os.listdir(test_dir):
    for data in glob.glob(test_dir+'/'+per+'/*.*'):
        img = cv2.imread(data)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        img = cv2.resize(img, (SIZE, SIZE))
        if per[-1] == 'f':
            forged_images_test.append(img)
        else:
            real_images_test.append(img)

# test_data = np.array(test_data)/255.0
# test_labels = np.array(test_labels)
test_data = np.concatenate([real_images_test, forged_images_test])
test_labels = np.concatenate([np.zeros(len(real_images_test)), np.ones(len(forged_images_test))])


# # Load the real signature images

# for filename in os.listdir(train_dir):
#     for file in filename:
#         files_in_folder = find_files_in_folder(train_dir, file)
#         if ""-f"" in file:
#             forged_images.append(file)
#         else:
#             real_images.append(file)
#     image = cv2.imread(os.path.join(train_dir, filename))
#     if image is not None:
#         image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
#         image = cv2.resize(image, (224, 224))
#         real_images.append(image)

# # Load the forged signature images

# for filename in os.listdir(test_dir):
#     for file in filename:
#         files_in_folder = find_files_in_folder(test_dir, file)
#         if ""-f"" in file:
#             forged_images.append(file)
#         else:
#             real_images.append(file)
#     image = cv2.imread(os.path.join(forged_path, filename))
#     if image is not None:
#         image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
#         image = cv2.resize(image, (224, 224))
#         forged_images.append(image)

# Convert the images to numpy arrays
real_images = np.array(real_images)
forged_images = np.array(forged_images)

# Create the labels (0 for real, 1 for forged)
real_labels = np.zeros((real_images.shape[0], 1))
forged_labels = np.ones((forged_images.shape[0], 1))

print(""number of real_images"",len(real_images))
print(""number of forged_images"",len(forged_images))
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-9-d6f46251b639>[0m in [0;36m<cell line: 61>[0;34m()[0m
[1;32m     59[0m [0;31m# print(""number of real_images"",len(real_images))[0m[0;34m[0m[0;34m[0m[0m
[1;32m     60[0m [0;34m[0m[0m
[0;32m---> 61[0;31m [0mtrain_data[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0mconcatenate[0m[0;34m([0m[0;34m[[0m[0mreal_images_train[0m[0;34m,[0m [0mforged_images_train[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     62[0m [0mtrain_labels[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0mconcatenate[0m[0;34m([0m[0;34m[[0m[0mnp[0m[0;34m.[0m[0mzeros[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mreal_images_train[0m[0;34m)[0m[0;34m)[0m[0;34m,[0m [0mnp[0m[0;34m.[0m[0mones[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mforged_images_train[0m[0;34m)[0m[0;34m)[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     63[0m [0mprint[0m[0;34m([0m[0;34m""number of real_images""[0m[0;34m,[0m[0mlen[0m[0;34m([0m[0mreal_images[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mValueError[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 1 dimension(s)
ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 1 dimension(s)
",ValueError,"all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 1 dimension(s)",FALSE,no fix
tensorflow_6,23ea6d07-7fc5-3715-b82e-e914a8978002,keyerror,'acc',"['---------------------------------------------------------------------------', 'KeyError                                  Traceback (most recent call last)', 'Cell In[53], line 6\n      3 ax[0].plot(history.history[\'val_loss\'], color=\'r\', label=""validation loss"",axes =ax[0])\n      4 legend = ax[0].legend(loc=\'best\', shadow=True)\n----> 6 ax[1].plot(history.history[\'acc\'], color=\'b\', label=""Training accuracy"")\n      7 ax[1].plot(history.history[\'val_acc\'], color=\'r\',label=""Validation accuracy"")\n      8 legend = ax[1].legend(loc=\'best\', shadow=True)\n', ""KeyError: 'acc'""]",/junobench_env/tensorflow_6/tensorflow_6_extension.ipynb,2026-01-14T18:32:31.101550,CellExecutionError,"An error occurred while executing the following cell:
------------------
model = keras.models.load_model(checkpoint_filepath)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-27-25b03261b4a9>[0m in [0;36m<cell line: 1>[0;34m()[0m
[0;32m----> 1[0;31m [0mmodel[0m [0;34m=[0m [0mkeras[0m[0;34m.[0m[0mmodels[0m[0;34m.[0m[0mload_model[0m[0;34m([0m[0mcheckpoint_filepath[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py[0m in [0;36mload_model[0;34m(filepath, custom_objects, compile, safe_mode)[0m
[1;32m    191[0m         )
[1;32m    192[0m     [0;32melif[0m [0mstr[0m[0;34m([0m[0mfilepath[0m[0;34m)[0m[0;34m.[0m[0mendswith[0m[0;34m([0m[0;34m"".keras""[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 193[0;31m         raise ValueError(
[0m[1;32m    194[0m             [0;34mf""File not found: filepath={filepath}. ""[0m[0;34m[0m[0;34m[0m[0m
[1;32m    195[0m             [0;34m""Please ensure the file is an accessible `.keras` ""[0m[0;34m[0m[0;34m[0m[0m

[0;31mValueError[0m: File not found: filepath=best_model.keras. Please ensure the file is an accessible `.keras` zip file.
ValueError: File not found: filepath=best_model.keras. Please ensure the file is an accessible `.keras` zip file.
",ValueError,File not found: filepath=best_model.keras. Please ensure the file is an accessible `.keras` zip file.,FALSE,fixed the eror
tensorflow_8,85f04894-8153-3f9a-a839-0b2c211b0916,attributeerror,'Functional' object has no attribute 'predict_classes',"['---------------------------------------------------------------------------', 'AttributeError                            Traceback (most recent call last)', 'Cell In[26], line 1\n----> 1 y_pred = model.predict_classes(X_val)\n      2 acc_test = 0\n      4 for i in range(X_val.shape[0]):\n', ""AttributeError: 'Functional' object has no attribute 'predict_classes'""]",/junobench_env/tensorflow_8/tensorflow_8_extension.ipynb,2026-01-14T18:30:29.039126,CellExecutionError,"An error occurred while executing the following cell:
------------------
os.mkdir('data/augmented')
# os.mkdir('data/augmented/benign')
# os.mkdir('data/augmented/malignant')
os.makedirs('data/augmented/benign', exist_ok=True)
os.makedirs('data/augmented/malignant', exist_ok=True)

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileExistsError[0m                           Traceback (most recent call last)
[0;32m<ipython-input-4-02f0c5c35e74>[0m in [0;36m<cell line: 1>[0;34m()[0m
[0;32m----> 1[0;31m [0mos[0m[0;34m.[0m[0mmkdir[0m[0;34m([0m[0;34m'data/augmented'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0;31m# os.mkdir('data/augmented/benign')[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m [0;31m# os.mkdir('data/augmented/malignant')[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0mos[0m[0;34m.[0m[0mmakedirs[0m[0;34m([0m[0;34m'data/augmented/benign'[0m[0;34m,[0m [0mexist_ok[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0mos[0m[0;34m.[0m[0mmakedirs[0m[0;34m([0m[0;34m'data/augmented/malignant'[0m[0;34m,[0m [0mexist_ok[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileExistsError[0m: [Errno 17] File exists: 'data/augmented'
FileExistsError: [Errno 17] File exists: 'data/augmented'
",FileExistsError,[Errno 17] File exists: 'data/augmented',FALSE,fixed the error
tensorflow_9,48241d9e-dcf9-38fb-87d0-f3f542bbb45e,attributeerror,'BertModel' object has no attribute 'built',"['---------------------------------------------------------------------------', 'AttributeError                            Traceback (most recent call last)', 'Cell In[23], line 2\n      1 import tensorflow as tf\n----> 2 tf.keras.utils.plot_model(bert_model)\n', 'File /opt/conda/lib/python3.10/site-packages/keras/utils/vis_utils.py:444, in plot_model(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, layer_range, show_layer_activations, show_trainable)\n    378 @keras_export(""keras.utils.plot_model"")\n    379 def plot_model(\n    380     model,\n   (...)\n    390     show_trainable=False,\n    391 ):\n    392     """"""Converts a Keras model to dot format and save to a file.\n    393 \n    394     Example:\n   (...)\n    441       This enables in-line display of the model plots in notebooks.\n    442     """"""\n--> 444     if not model.built:\n    445         raise ValueError(\n    446             ""This model has not yet been built. ""\n    447             ""Build the model first by calling `build()` or by calling ""\n    448             ""the model on a batch of data.""\n    449         )\n    451     if not check_graphviz():\n', 'File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1614, in Module.__getattr__(self, name)\n   1612     if name in modules:\n   1613         return modules[name]\n-> 1614 raise AttributeError(""\'{}\' object has no attribute \'{}\'"".format(\n   1615     type(self).__name__, name))\n', ""AttributeError: 'BertModel' object has no attribute 'built'""]",/junobench_env/tensorflow_9/tensorflow_9_extension.ipynb,2026-01-14T18:34:36.019327,CellExecutionError,"An error occurred while executing the following cell:
------------------
# input_ids = torch.tensor(bert_input.input_ids)
# attention_mask = torch.tensor(bert_input.attention_mask)
input_ids = bert_input.input_ids.squeeze(0)
attention_mask = bert_input.attention_mask.squeeze(0)

bert_model = BertModel.from_pretrained('bert-base-uncased')
last_hidden_state, pooled_output = bert_model(input_ids=input_ids, attention_mask=attention_mask, return_dict =False)

print(last_hidden_state.shape)
print(bert_model.config.hidden_size)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-20-fa5d9cf12f0a>[0m in [0;36m<cell line: 7>[0;34m()[0m
[1;32m      5[0m [0;34m[0m[0m
[1;32m      6[0m [0mbert_model[0m [0;34m=[0m [0mBertModel[0m[0;34m.[0m[0mfrom_pretrained[0m[0;34m([0m[0;34m'bert-base-uncased'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 7[0;31m [0mlast_hidden_state[0m[0;34m,[0m [0mpooled_output[0m [0;34m=[0m [0mbert_model[0m[0;34m([0m[0minput_ids[0m[0;34m=[0m[0minput_ids[0m[0;34m,[0m [0mattention_mask[0m[0;34m=[0m[0mattention_mask[0m[0;34m,[0m [0mreturn_dict[0m [0;34m=[0m[0;32mFalse[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      8[0m [0;34m[0m[0m
[1;32m      9[0m [0mprint[0m[0;34m([0m[0mlast_hidden_state[0m[0;34m.[0m[0mshape[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
[1;32m   1108[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
[1;32m   1109[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
[0;32m-> 1110[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1111[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1112[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py[0m in [0;36mforward[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)[0m
[1;32m   1061[0m             [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m""You have to specify either input_ids or inputs_embeds""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1062[0m [0;34m[0m[0m
[0;32m-> 1063[0;31m         [0mbatch_size[0m[0;34m,[0m [0mseq_length[0m [0;34m=[0m [0minput_shape[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1064[0m         [0mdevice[0m [0;34m=[0m [0minput_ids[0m[0;34m.[0m[0mdevice[0m [0;32mif[0m [0minput_ids[0m [0;32mis[0m [0;32mnot[0m [0;32mNone[0m [0;32melse[0m [0minputs_embeds[0m[0;34m.[0m[0mdevice[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1065[0m [0;34m[0m[0m

[0;31mValueError[0m: not enough values to unpack (expected 2, got 1)
ValueError: not enough values to unpack (expected 2, got 1)
",ValueError,"not enough values to unpack (expected 2, got 1)",FALSE,fixed the eror
tensorflow_10,085ef695-7b52-3e89-92ef-d481a9ab9cb2,typeerror,"Value passed to parameter 'input' has DataType string not in list of allowed values: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, float16, uint32, uint64, qint8, quint8, qint32, qint16, quint16","['---------------------------------------------------------------------------', 'TypeError                                 Traceback (most recent call last)', ""Cell In[55], line 3\n      1 # Compile and train the model\n      2 model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n----> 3 model.fit(train_data, train_labels, epochs=10)\n"", 'File /opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n     68     # To get the full stack trace, call:\n     69     # `tf.debugging.disable_traceback_filtering()`\n---> 70     raise e.with_traceback(filtered_tb) from None\n     71 finally:\n     72     del filtered_tb\n', 'File /tmp/__autograph_generated_file0uma5t6e.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)\n     13 try:\n     14     do_return = True\n---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n     16 except:\n     17     do_return = False\n', 'File /tmp/__autograph_generated_file_7qcj_31.py:10, in outer_factory.<locals>.inner_factory.<locals>.tf__call(self, inputs, **kwargs)\n      8 do_return = False\n      9 retval_ = ag__.UndefinedReturnValue()\n---> 10 outputs = ag__.converted_call(ag__.ld(self).model, (ag__.ld(inputs),), dict(**ag__.ld(kwargs)), fscope)\n     11 try:\n     12     do_return = True\n', 'File /tmp/__autograph_generated_file4cxfdhhw.py:37, in outer_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs(self, *args, **kwargs)\n     35 try:\n     36     do_return = True\n---> 37     retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n     38 except:\n     39     do_return = False\n', 'File /tmp/__autograph_generated_filei01ktml5.py:31, in outer_factory.<locals>.inner_factory.<locals>.tf__call(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\n     29 do_return = False\n     30 retval_ = ag__.UndefinedReturnValue()\n---> 31 outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n     32 try:\n     33     do_return = True\n', 'File /tmp/__autograph_generated_file4cxfdhhw.py:37, in outer_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs(self, *args, **kwargs)\n     35 try:\n     36     do_return = True\n---> 37     retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n     38 except:\n     39     do_return = False\n', ""File /tmp/__autograph_generated_file8w9ibu71.py:127, in outer_factory.<locals>.inner_factory.<locals>.tf__call(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\n    125     pass\n    126 ag__.if_stmt(ag__.ld(token_type_ids) is None, if_body_6, else_body_6, get_state_6, set_state_6, ('token_type_ids',), 1)\n--> 127 embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(input_ids=ag__.ld(input_ids), position_ids=ag__.ld(position_ids), token_type_ids=ag__.ld(token_type_ids), inputs_embeds=ag__.ld(inputs_embeds), past_key_values_length=ag__.ld(past_key_values_length), training=ag__.ld(training)), fscope)\n    128 attention_mask_shape = ag__.converted_call(ag__.ld(shape_list), (ag__.ld(attention_mask),), None, fscope)\n    129 mask_seq_length = ag__.ld(seq_length) + ag__.ld(past_key_values_length)\n"", ""File /tmp/__autograph_generated_filev0vda2ze.py:46, in outer_factory.<locals>.inner_factory.<locals>.tf__call(self, input_ids, position_ids, token_type_ids, inputs_embeds, past_key_values_length, training)\n     44     nonlocal inputs_embeds\n     45     pass\n---> 46 ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('inputs_embeds',), 1)\n     47 input_shape = ag__.converted_call(ag__.ld(shape_list), (ag__.ld(inputs_embeds),), None, fscope)[:-1]\n     49 def get_state_2():\n"", 'File /tmp/__autograph_generated_filev0vda2ze.py:40, in outer_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.if_body_1()\n     38 def if_body_1():\n     39     nonlocal inputs_embeds\n---> 40     ag__.converted_call(ag__.ld(check_embeddings_within_bounds), (ag__.ld(input_ids), ag__.ld(self).config.vocab_size), None, fscope)\n     41     inputs_embeds = ag__.converted_call(ag__.ld(tf).gather, (), dict(params=ag__.ld(self).weight, indices=ag__.ld(input_ids)), fscope)\n', 'File /tmp/__autograph_generated_file29zlsorl.py:17, in outer_factory.<locals>.inner_factory.<locals>.tf__check_embeddings_within_bounds(tensor, embed_dim, tensor_name)\n      7         """"""\n      8 `tf.gather`, on which TF embedding layers are based, won\'t check positive out of bound indices on GPU, returning\n      9 zeros instead. This function adds a check against that dangerous silent behavior.\n   (...)\n     14     tensor_name (`str`, *optional*): The name of the tensor to use in the error message.\n     15 """"""\n     16         with ag__.FunctionScope(\'check_embeddings_within_bounds\', \'fscope\', ag__.STD) as fscope:\n---> 17             ag__.converted_call(ag__.ld(tf).debugging.assert_less, (ag__.ld(tensor), ag__.converted_call(ag__.ld(tf).cast, (ag__.ld(embed_dim),), dict(dtype=ag__.ld(tensor).dtype), fscope)), dict(message=f""The maximum value of {ag__.ld(tensor_name)} ({ag__.converted_call(ag__.ld(tf).math.reduce_max, (ag__.ld(tensor),), None, fscope)}) must be smaller than the embedding layer\'s input dimension ({ag__.ld(embed_dim)}). The likely cause is some problem at tokenization time.""), fscope)\n', 'TypeError: in user code:\n\n    File ""/opt/conda/lib/python3.10/site-packages/keras/engine/training.py"", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/engine/training.py"", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File ""/opt/conda/lib/python3.10/site-packages/keras/engine/training.py"", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/engine/training.py"", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File ""/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File ""/tmp/__autograph_generated_file_7qcj_31.py"", line 10, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).model, (ag__.ld(inputs),), dict(**ag__.ld(kwargs)), fscope)\n    File ""/tmp/__autograph_generated_file4cxfdhhw.py"", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File ""/tmp/__autograph_generated_filei01ktml5.py"", line 31, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File ""/tmp/__autograph_generated_file4cxfdhhw.py"", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File ""/tmp/__autograph_generated_file8w9ibu71.py"", line 127, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(input_ids=ag__.ld(input_ids), position_ids=ag__.ld(position_ids), token_type_ids=ag__.ld(token_type_ids), inputs_embeds=ag__.ld(inputs_embeds), past_key_values_length=ag__.ld(past_key_values_length), training=ag__.ld(training)), fscope)\n    File ""/tmp/__autograph_generated_filev0vda2ze.py"", line 46, in tf__call\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, (\'inputs_embeds\',), 1)\n    File ""/tmp/__autograph_generated_filev0vda2ze.py"", line 40, in if_body_1\n        ag__.converted_call(ag__.ld(check_embeddings_within_bounds), (ag__.ld(input_ids), ag__.ld(self).config.vocab_size), None, fscope)\n    File ""/tmp/__autograph_generated_file29zlsorl.py"", line 17, in tf__check_embeddings_within_bounds\n        ag__.converted_call(ag__.ld(tf).debugging.assert_less, (ag__.ld(tensor), ag__.converted_call(ag__.ld(tf).cast, (ag__.ld(embed_dim),), dict(dtype=ag__.ld(tensor).dtype), fscope)), dict(message=f""The maximum value of {ag__.ld(tensor_name)} ({ag__.converted_call(ag__.ld(tf).math.reduce_max, (ag__.ld(tensor),), None, fscope)}) must be smaller than the embedding layer\'s input dimension ({ag__.ld(embed_dim)}). The likely cause is some problem at tokenization time.""), fscope)\n\n    TypeError: Exception encountered when calling layer \'hugging_face_layer_3\' (type HuggingFaceLayer).\n    \n    in user code:\n    \n        File ""/tmp/ipykernel_32/1009480242.py"", line 14, in call  *\n            outputs = self.model(inputs, **kwargs)\n        File ""/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File ""/tmp/__autograph_generated_file4cxfdhhw.py"", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File ""/tmp/__autograph_generated_filei01ktml5.py"", line 31, in tf__call\n            outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n        File ""/tmp/__autograph_generated_file4cxfdhhw.py"", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File ""/tmp/__autograph_generated_file8w9ibu71.py"", line 127, in tf__call\n            embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(input_ids=ag__.ld(input_ids), position_ids=ag__.ld(position_ids), token_type_ids=ag__.ld(token_type_ids), inputs_embeds=ag__.ld(inputs_embeds), past_key_values_length=ag__.ld(past_key_values_length), training=ag__.ld(training)), fscope)\n        File ""/tmp/__autograph_generated_filev0vda2ze.py"", line 46, in tf__call\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, (\'inputs_embeds\',), 1)\n        File ""/tmp/__autograph_generated_filev0vda2ze.py"", line 40, in if_body_1\n            ag__.converted_call(ag__.ld(check_embeddings_within_bounds), (ag__.ld(input_ids), ag__.ld(self).config.vocab_size), None, fscope)\n        File ""/tmp/__autograph_generated_file29zlsorl.py"", line 17, in tf__check_embeddings_within_bounds\n            ag__.converted_call(ag__.ld(tf).debugging.assert_less, (ag__.ld(tensor), ag__.converted_call(ag__.ld(tf).cast, (ag__.ld(embed_dim),), dict(dtype=ag__.ld(tensor).dtype), fscope)), dict(message=f""The maximum value of {ag__.ld(tensor_name)} ({ag__.converted_call(ag__.ld(tf).math.reduce_max, (ag__.ld(tensor),), None, fscope)}) must be smaller than the embedding layer\'s input dimension ({ag__.ld(embed_dim)}). The likely cause is some problem at tokenization time.""), fscope)\n    \n        TypeError: Exception encountered when calling layer \'tf_bert_model_3\' (type TFBertModel).\n        \n        in user code:\n        \n            File ""/opt/conda/lib/python3.10/site-packages/transformers/modeling_tf_utils.py"", line 1061, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File ""/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py"", line 1088, in call  *\n                outputs = self.bert(\n            File ""/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File ""/tmp/__autograph_generated_file4cxfdhhw.py"", line 37, in tf__run_call_with_unpacked_inputs\n                retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n            File ""/tmp/__autograph_generated_file8w9ibu71.py"", line 127, in tf__call\n                embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(input_ids=ag__.ld(input_ids), position_ids=ag__.ld(position_ids), token_type_ids=ag__.ld(token_type_ids), inputs_embeds=ag__.ld(inputs_embeds), past_key_values_length=ag__.ld(past_key_values_length), training=ag__.ld(training)), fscope)\n            File ""/tmp/__autograph_generated_filev0vda2ze.py"", line 46, in tf__call\n                ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, (\'inputs_embeds\',), 1)\n            File ""/tmp/__autograph_generated_filev0vda2ze.py"", line 40, in if_body_1\n                ag__.converted_call(ag__.ld(check_embeddings_within_bounds), (ag__.ld(input_ids), ag__.ld(self).config.vocab_size), None, fscope)\n            File ""/tmp/__autograph_generated_file29zlsorl.py"", line 17, in tf__check_embeddings_within_bounds\n                ag__.converted_call(ag__.ld(tf).debugging.assert_less, (ag__.ld(tensor), ag__.converted_call(ag__.ld(tf).cast, (ag__.ld(embed_dim),), dict(dtype=ag__.ld(tensor).dtype), fscope)), dict(message=f""The maximum value of {ag__.ld(tensor_name)} ({ag__.converted_call(ag__.ld(tf).math.reduce_max, (ag__.ld(tensor),), None, fscope)}) must be smaller than the embedding layer\'s input dimension ({ag__.ld(embed_dim)}). The likely cause is some problem at tokenization time.""), fscope)\n        \n            TypeError: Exception encountered when calling layer \'bert\' (type TFBertMainLayer).\n            \n            in user code:\n            \n                File ""/opt/conda/lib/python3.10/site-packages/transformers/modeling_tf_utils.py"", line 1061, in run_call_with_unpacked_inputs  *\n                    return func(self, **unpacked_inputs)\n                File ""/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py"", line 780, in call  *\n                    embedding_output = self.embeddings(\n                File ""/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File ""/tmp/__autograph_generated_filev0vda2ze.py"", line 46, in tf__call\n                    ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, (\'inputs_embeds\',), 1)\n                File ""/tmp/__autograph_generated_filev0vda2ze.py"", line 40, in if_body_1\n                    ag__.converted_call(ag__.ld(check_embeddings_within_bounds), (ag__.ld(input_ids), ag__.ld(self).config.vocab_size), None, fscope)\n                File ""/tmp/__autograph_generated_file29zlsorl.py"", line 17, in tf__check_embeddings_within_bounds\n                    ag__.converted_call(ag__.ld(tf).debugging.assert_less, (ag__.ld(tensor), ag__.converted_call(ag__.ld(tf).cast, (ag__.ld(embed_dim),), dict(dtype=ag__.ld(tensor).dtype), fscope)), dict(message=f""The maximum value of {ag__.ld(tensor_name)} ({ag__.converted_call(ag__.ld(tf).math.reduce_max, (ag__.ld(tensor),), None, fscope)}) must be smaller than the embedding layer\'s input dimension ({ag__.ld(embed_dim)}). The likely cause is some problem at tokenization time.""), fscope)\n            \n                TypeError: Exception encountered when calling layer \'embeddings\' (type TFBertEmbeddings).\n                \n                in user code:\n                \n                    File ""/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py"", line 202, in call  *\n                        check_embeddings_within_bounds(input_ids, self.config.vocab_size)\n                    File ""/opt/conda/lib/python3.10/site-packages/transformers/tf_utils.py"", line 161, in check_embeddings_within_bounds  *\n                        tf.debugging.assert_less(\n                \n                    TypeError: Value passed to parameter \'input\' has DataType string not in list of allowed values: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, float16, uint32, uint64, qint8, quint8, qint32, qint16, quint16\n                \n                \n                Call arguments received by layer \'embeddings\' (type TFBertEmbeddings):\n                  ‚Ä¢ input_ids=tf.Tensor(shape=(None, 1), dtype=string)\n                  ‚Ä¢ position_ids=None\n                  ‚Ä¢ token_type_ids=tf.Tensor(shape=(None, 1), dtype=int32)\n                  ‚Ä¢ inputs_embeds=None\n                  ‚Ä¢ past_key_values_length=0\n                  ‚Ä¢ training=True\n            \n            \n            Call arguments received by layer \'bert\' (type TFBertMainLayer):\n              ‚Ä¢ input_ids=tf.Tensor(shape=(None, 1), dtype=string)\n              ‚Ä¢ attention_mask=None\n              ‚Ä¢ token_type_ids=None\n              ‚Ä¢ position_ids=None\n              ‚Ä¢ head_mask=None\n              ‚Ä¢ inputs_embeds=None\n              ‚Ä¢ encoder_hidden_states=None\n              ‚Ä¢ encoder_attention_mask=None\n              ‚Ä¢ past_key_values=None\n              ‚Ä¢ use_cache=True\n              ‚Ä¢ output_attentions=False\n              ‚Ä¢ output_hidden_states=False\n              ‚Ä¢ return_dict=True\n              ‚Ä¢ training=True\n        \n        \n        Call arguments received by layer \'tf_bert_model_3\' (type TFBertModel):\n          ‚Ä¢ input_ids=tf.Tensor(shape=(None, 1), dtype=string)\n          ‚Ä¢ attention_mask=None\n          ‚Ä¢ token_type_ids=Non",/junobench_env/tensorflow_10/tensorflow_10_extension.ipynb,2026-01-14T17:55:37.488332,CellExecutionError,"An error occurred while executing the following cell:
------------------
# Compile and train the model
# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# model.fit(train_data, train_labels, epochs=10)

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-20-9dc4154d65b9>[0m in [0;36m<cell line: 6>[0;34m()[0m
[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m [0mmodel[0m[0;34m.[0m[0mcompile[0m[0;34m([0m[0moptimizer[0m[0;34m=[0m[0;34m'adam'[0m[0;34m,[0m [0mloss[0m[0;34m=[0m[0;34m'binary_crossentropy'[0m[0;34m,[0m [0mmetrics[0m[0;34m=[0m[0;34m[[0m[0;34m'accuracy'[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 6[0;31m [0mmodel[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0mX_train[0m[0;34m,[0m [0my_train[0m[0;34m,[0m [0mepochs[0m[0;34m=[0m[0;36m10[0m[0;34m,[0m [0mvalidation_data[0m[0;34m=[0m[0;34m([0m[0mX_valid[0m[0;34m,[0m [0my_valid[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py[0m in [0;36merror_handler[0;34m(*args, **kwargs)[0m
[1;32m    120[0m             [0;31m# To get the full stack trace, call:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    121[0m             [0;31m# `keras.config.disable_traceback_filtering()`[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 122[0;31m             [0;32mraise[0m [0me[0m[0;34m.[0m[0mwith_traceback[0m[0;34m([0m[0mfiltered_tb[0m[0;34m)[0m [0;32mfrom[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    123[0m         [0;32mfinally[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    124[0m             [0;32mdel[0m [0mfiltered_tb[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py[0m in [0;36m__call__[0;34m(self, *args, **kwargs)[0m
[1;32m    787[0m                     [0;32mand[0m [0marg[0m [0;32mis[0m [0;32mnot[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
[1;32m    788[0m                 ):
[0;32m--> 789[0;31m                     raise ValueError(
[0m[1;32m    790[0m                         [0;34m""Only input tensors may be passed as ""[0m[0;34m[0m[0;34m[0m[0m
[1;32m    791[0m                         [0;34m""positional arguments. The following argument value ""[0m[0;34m[0m[0;34m[0m[0m

[0;31mValueError[0m: Exception encountered when calling Sequential.call().

[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor 'sequential_1/hugging_face_layer_1/tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(32, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'sequential_1/hugging_face_layer_1/tf_bert_model/bert/pooler/dense/Tanh:0' shape=(32, 768) dtype=float32>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None) (of type <class 'transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions'>)[0m

Arguments received by Sequential.call():
  ‚Ä¢ inputs={'input_ids': 'tf.Tensor(shape=(32, 128), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(32, 128), dtype=int32)', 'token_type_ids': 'tf.Tensor(shape=(32, 128), dtype=int32)'}
  ‚Ä¢ training=True
  ‚Ä¢ mask={'input_ids': 'None', 'attention_mask': 'None', 'token_type_ids': 'None'}
ValueError: Exception encountered when calling Sequential.call().

[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor 'sequential_1/hugging_face_layer_1/tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(32, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'sequential_1/hugging_face_layer_1/tf_bert_model/bert/pooler/dense/Tanh:0' shape=(32, 768) dtype=float32>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None) (of type <class 'transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions'>)[0m

Arguments received by Sequential.call():
  ‚Ä¢ inputs={'input_ids': 'tf.Tensor(shape=(32, 128), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(32, 128), dtype=int32)', 'token_type_ids': 'tf.Tensor(shape=(32, 128), dtype=int32)'}
  ‚Ä¢ training=True
  ‚Ä¢ mask={'input_ids': 'None', 'attention_mask': 'None', 'token_type_ids': 'None'}
",ValueError,Exception encountered when calling Sequential.call().,FALSE,No fix
tensorflow_12,58e8fcb0-046c-3e4f-bed2-76fec5d9a381,valueerror,Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).,"['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', '/tmp/ipykernel_23/3987115718.py in <module>\n----> 1 history = model.fit(train_images, train_labels, batch_size = 16, epochs=5, validation_data=(val_images, val_labels), verbose = 1)\n', '/opt/conda/lib/python3.7/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n   1146           use_multiprocessing=use_multiprocessing,\n   1147           model=self,\n-> 1148           steps_per_execution=self._steps_per_execution)\n   1149 \n   1150       # Container that configures and calls `tf.keras.Callback`s.\n', '/opt/conda/lib/python3.7/site-packages/keras/engine/data_adapter.py in get_data_handler(*args, **kwargs)\n   1381   if getattr(kwargs[""model""], ""_cluster_coordinator"", None):\n   1382     return _ClusterCoordinatorDataHandler(*args, **kwargs)\n-> 1383   return DataHandler(*args, **kwargs)\n   1384 \n   1385 \n', '/opt/conda/lib/python3.7/site-packages/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\n   1148         use_multiprocessing=use_multiprocessing,\n   1149         distribution_strategy=tf.distribute.get_strategy(),\n-> 1150         model=model)\n   1151 \n   1152     strategy = tf.distribute.get_strategy()\n', '/opt/conda/lib/python3.7/site-packages/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\n    228                **kwargs):\n    229     super(TensorLikeDataAdapter, self).__init__(x, y, **kwargs)\n--> 230     x, y, sample_weights = _process_tensorlike((x, y, sample_weights))\n    231     sample_weight_modes = broadcast_sample_weight_modes(\n    232         sample_weights, sample_weight_modes)\n', '/opt/conda/lib/python3.7/site-packages/keras/engine/data_adapter.py in _process_tensorlike(inputs)\n   1029     return x\n   1030 \n-> 1031   inputs = tf.nest.map_structure(_convert_numpy_and_scipy, inputs)\n   1032   return tf.__internal__.nest.list_to_tuple(inputs)\n   1033 \n', '/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)\n    867 \n    868   return pack_sequence_as(\n--> 869       structure[0], [func(*x) for x in entries],\n    870       expand_composites=expand_composites)\n    871 \n', '/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)\n    867 \n    868   return pack_sequence_as(\n--> 869       structure[0], [func(*x) for x in entries],\n    870       expand_composites=expand_composites)\n    871 \n', '/opt/conda/lib/python3.7/site-packages/keras/engine/data_adapter.py in _convert_numpy_and_scipy(x)\n   1024       if issubclass(x.dtype.type, np.floating):\n   1025         dtype = backend.floatx()\n-> 1026       return tf.convert_to_tensor(x, dtype=dtype)\n   1027     elif _is_scipy_sparse(x):\n   1028       return _scipy_sparse_to_sparse_tensor(x)\n', '/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\n    204     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""\n    205     try:\n--> 206       return target(*args, **kwargs)\n    207     except (TypeError, ValueError):\n    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a\n', '/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2_with_dispatch(value, dtype, dtype_hint, name)\n   1429   """"""\n   1430   return convert_to_tensor_v2(\n-> 1431       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n   1432 \n   1433 \n', '/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)\n   1439       name=name,\n   1440       preferred_dtype=dtype_hint,\n-> 1441       as_ref=False)\n   1442 \n   1443 \n', '/opt/conda/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py in wrapped(*args, **kwargs)\n    161         with Trace(trace_name, **trace_kwargs):\n    162           return func(*args, **kwargs)\n--> 163       return func(*args, **kwargs)\n    164 \n    165     return wrapped\n', '/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\n   1564 \n   1565     if ret is None:\n-> 1566       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n   1567 \n   1568     if ret is NotImplemented:\n', '/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)\n     50 def _default_conversion_function(value, dtype, name, as_ref):\n     51   del as_ref  # Unused.\n---> 52   return constant_op.constant(value, dtype, name=name)\n     53 \n     54 \n', '/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\n    270   """"""\n    271   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n--> 272                         allow_broadcast=True)\n    273 \n    274 \n', '/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\n    281       with trace.Trace(""tf.constant""):\n    282         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n--> 283     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n    284 \n    285   g = ops.get_default_graph()\n', '/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n    306 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):\n    307   """"""Creates a constant on the current device.""""""\n--> 308   t = convert_to_eager_tensor(value, ctx, dtype)\n    309   if shape is None:\n    310     return t\n', '/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\n    104       dtype = dtypes.as_dtype(dtype).as_datatype_enum\n    105   ctx.ensure_initialized()\n--> 106   return ops.EagerTensor(value, ctx.device_name, dtype)\n    107 \n    108 \n', 'ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).']",/junobench_env/tensorflow_12/tensorflow_12_extension.ipynb,2026-01-14T17:37:30.606640,CellExecutionError,"An error occurred while executing the following cell:
------------------
# history = model.fit(train_images, train_labels, batch_size = 16, epochs=2, validation_data=(val_images, val_labels), verbose = 1)
model.compile(optimizer=Adam(0.0001), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])
history = model.fit(
    train_images, train_labels,
    batch_size=16,
    epochs=20,
    validation_data=(val_images, val_labels),
    verbose=1
)

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mInvalidArgumentError[0m                      Traceback (most recent call last)
[0;32m<ipython-input-11-9ce01cfc5385>[0m in [0;36m<cell line: 3>[0;34m()[0m
[1;32m      1[0m [0;31m# history = model.fit(train_images, train_labels, batch_size = 16, epochs=2, validation_data=(val_images, val_labels), verbose = 1)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      2[0m [0mmodel[0m[0;34m.[0m[0mcompile[0m[0;34m([0m[0moptimizer[0m[0;34m=[0m[0mAdam[0m[0;34m([0m[0;36m0.0001[0m[0;34m)[0m[0;34m,[0m [0mloss[0m[0;34m=[0m[0mtf[0m[0;34m.[0m[0mkeras[0m[0;34m.[0m[0mlosses[0m[0;34m.[0m[0mSparseCategoricalCrossentropy[0m[0;34m([0m[0mfrom_logits[0m[0;34m=[0m[0;32mFalse[0m[0;34m)[0m[0;34m,[0m [0mmetrics[0m[0;34m=[0m[0;34m[[0m[0;34m'accuracy'[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 3[0;31m history = model.fit(
[0m[1;32m      4[0m     [0mtrain_images[0m[0;34m,[0m [0mtrain_labels[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m     [0mbatch_size[0m[0;34m=[0m[0;36m16[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py[0m in [0;36merror_handler[0;34m(*args, **kwargs)[0m
[1;32m    120[0m             [0;31m# To get the full stack trace, call:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    121[0m             [0;31m# `keras.config.disable_traceback_filtering()`[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 122[0;31m             [0;32mraise[0m [0me[0m[0;34m.[0m[0mwith_traceback[0m[0;34m([0m[0mfiltered_tb[0m[0;34m)[0m [0;32mfrom[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    123[0m         [0;32mfinally[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    124[0m             [0;32mdel[0m [0mfiltered_tb[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py[0m in [0;36mquick_execute[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)[0m
[1;32m     51[0m   [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m     52[0m     [0mctx[0m[0;34m.[0m[0mensure_initialized[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 53[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
[0m[1;32m     54[0m                                         inputs, attrs, num_outputs)
[1;32m     55[0m   [0;32mexcept[0m [0mcore[0m[0;34m.[0m[0m_NotOkStatusException[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;31mInvalidArgumentError[0m: Graph execution error:

Detected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):
  File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main

  File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 16, in <module>

  File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start

  File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once

  File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 377, in dispatch_queue

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 250, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 748, in __init__

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code

  File ""<ipython-input-11-9ce01cfc5385>"", line 3, in <cell line: 3>

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 318, in fit

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 121, in one_step_on_iterator

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 108, in one_step_on_data

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 54, in train_step

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py"", line 357, in _compute_loss

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py"", line 325, in compute_loss

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py"", line 609, in __call__

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py"", line 645, in call

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py"", line 43, in __call__

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py"", line 27, in call

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py"", line 1853, in sparse_categorical_crossentropy

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py"", line 1567, in sparse_categorical_crossentropy

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py"", line 645, in sparse_categorical_crossentropy

Received a label value of 4 which is outside the valid range of [0, 4).  Label values: 2 2 1 1 2 1 1 1 2 3 1 1 3 1 1 4
	 [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_11104]
InvalidArgumentError: Graph execution error:

Detected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):
  File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main

  File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 16, in <module>

  File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start

  File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once

  File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 377, in dispatch_queue

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 250, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 748, in __init__

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code

  File ""<ipython-input-11-9ce01cfc5385>"", line 3, in <cell line: 3>

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 318, in fit

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 121, in one_step_on_iterator

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 108, in one_step_on_data

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 54, in train_step

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py"", line 357, in _compute_loss

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py"", line 325, in compute_loss

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py"", line 609, in __call__

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py"", line 645, in call

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py"", line 43, in __call__

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py"", line 27, in call

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py"", line 1853, in sparse_categorical_crossentropy

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py"", line 1567, in sparse_categorical_crossentropy

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py"", line 645, in sparse_categorical_crossentropy

Received a label value of 4 which is outside the valid range of [0, 4).  Label values: 2 2 1 1 2 1 1 1 2 3 1 1 3 1 1 4
	 [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_11104]
",InvalidArgumentError,Graph execution error:,FALSE,no fix
tensorflow_13,20e73553-36d2-36dc-9796-3d6637e81257,valueerror,`sequences` must be a list of iterables. Found non-iterable: 101,"['---------------------------------------------------------------------------', 'TypeError                                 Traceback (most recent call last)', 'File /opt/conda/lib/python3.10/site-packages/keras/utils/data_utils.py:1094, in pad_sequences(sequences, maxlen, dtype, padding, truncating, value)\n   1093 try:\n-> 1094     lengths.append(len(x))\n   1095     if flag and len(x):\n', ""TypeError: object of type 'int' has no len()"", '\nThe above exception was the direct cause of the following exception:\n', 'ValueError                                Traceback (most recent call last)', 'Cell In[40], line 7\n      3 from tensorflow.keras.preprocessing.text import Tokenizer\n      5 max_length = 768\n----> 7 docs = np.array([np.array(pad_sequences(tokenize_text(i)), maxlen=max_length) for i in docs])\n', 'Cell In[40], line 7, in <listcomp>(.0)\n      3 from tensorflow.keras.preprocessing.text import Tokenizer\n      5 max_length = 768\n----> 7 docs = np.array([np.array(pad_sequences(tokenize_text(i)), maxlen=max_length) for i in docs])\n', 'File /opt/conda/lib/python3.10/site-packages/keras/utils/data_utils.py:1099, in pad_sequences(sequences, maxlen, dtype, padding, truncating, value)\n   1097             flag = False\n   1098     except TypeError as e:\n-> 1099         raise ValueError(\n   1100             ""`sequences` must be a list of iterables. ""\n   1101             f""Found non-iterable: {str(x)}""\n   1102         ) from e\n   1104 if maxlen is None:\n   1105     maxlen = np.max(lengths)\n', 'ValueError: `sequences` must be a list of iterables. Found non-iterable: 101']",/junobench_env/tensorflow_13/tensorflow_13_extension.ipynb,2026-01-14T16:33:35.184338,CellExecutionError,"An error occurred while executing the following cell:
------------------
# def create_embedding_matrix(input_ids, glove_embeddings, bert_embeddings):
#     embedding_dim = bert_embeddings.shape[2]
# #     print(bert_embeddings.shape[1]
#     embedding_matrix = np.zeros((len(input_ids), embedding_dim))  # Exclude special tokens
#     for i, token_id in enumerate(input_ids[1:-1]):  # Exclude special tokens
#         token = tokenizer.convert_ids_to_tokens(token_id)
#         token_str = token[0]  # Convert list to string
#         glove_embedding = glove_embeddings.get(token_str)
#         if glove_embedding is not None:
#             embedding_matrix[i] = glove_embedding
#         else:
#             embedding_matrix[i] = bert_embeddings[0, i + 1, :]  # Adjust indexing and shape
#     return embedding_matrix

def create_embedding_matrix_for_sentence(input_ids, glove_embeddings, bert_embeddings, bert_tokenizer):
    target_embedding_dim = bert_embeddings.shape[2]  # 768
    num_actual_words = len(input_ids) - 2
    embedding_matrix = np.zeros((num_actual_words, target_embedding_dim))
    
    for i, token_id in enumerate(input_ids[1:-1]):  # skip [CLS] and [SEP]
        token_str = bert_tokenizer.convert_ids_to_tokens(token_id)
        glove_embedding = glove_embeddings.get(token_str)
        if glove_embedding is not None:
            if glove_embedding.shape[0] < target_embedding_dim:
                padded_glove = np.pad(glove_embedding, (0, target_embedding_dim - glove_embedding.shape[0]), 'constant')
                embedding_matrix[i] = padded_glove
            else:
                embedding_matrix[i] = glove_embedding[:target_embedding_dim]
        else:
            embedding_matrix[i] = bert_embeddings[0, i + 1, :]
    return embedding_matrix

# Example usage
embedding_matrix = create_embedding_matrix_for_sentence(input_ids, glove_embeddings, bert_embeddings)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mTypeError[0m                                 Traceback (most recent call last)
[0;32m<ipython-input-4-caf5086516d8>[0m in [0;36m<cell line: 34>[0;34m()[0m
[1;32m     32[0m [0;34m[0m[0m
[1;32m     33[0m [0;31m# Example usage[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 34[0;31m [0membedding_matrix[0m [0;34m=[0m [0mcreate_embedding_matrix_for_sentence[0m[0;34m([0m[0minput_ids[0m[0;34m,[0m [0mglove_embeddings[0m[0;34m,[0m [0mbert_embeddings[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;31mTypeError[0m: create_embedding_matrix_for_sentence() missing 1 required positional argument: 'bert_tokenizer'
TypeError: create_embedding_matrix_for_sentence() missing 1 required positional argument: 'bert_tokenizer'
",TypeError,create_embedding_matrix_for_sentence() missing 1 required positional argument: 'bert_tokenizer',FALSE,fixed the eror
tensorflow_14,e62e29da-fd8a-346f-b488-fec35357fd34,valueerror,"Layer count mismatch when loading weights from file. Model expected 13 layers, found 16 saved layers.","['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', ""Cell In[9], line 36\n     33 base_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n     35 # Load the previously saved model weights\n---> 36 base_model.load_weights('/kaggle/input/face-recog/vgg_face_weights.h5')\n     38 # Continue training the model\n     39 base_model.fit(\n     40     train_generator,\n     41     steps_per_epoch=len(train_generator),\n     42     epochs=10,  # You can adjust the number of epochs\n     43 )\n"", 'File /opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n     68     # To get the full stack trace, call:\n     69     # `tf.debugging.disable_traceback_filtering()`\n---> 70     raise e.with_traceback(filtered_tb) from None\n     71 finally:\n     72     del filtered_tb\n', 'File /opt/conda/lib/python3.10/site-packages/keras/saving/legacy/hdf5_format.py:808, in load_weights_from_hdf5_group(f, model)\n    806 layer_names = filtered_layer_names\n    807 if len(layer_names) != len(filtered_layers):\n--> 808     raise ValueError(\n    809         ""Layer count mismatch when loading weights from file. ""\n    810         f""Model expected {len(filtered_layers)} layers, found ""\n    811         f""{len(layer_names)} saved layers.""\n    812     )\n    814 # We batch weight value assignments in a single backend call\n    815 # which provides a speedup in TensorFlow.\n    816 weight_value_tuples = []\n', 'ValueError: Layer count mismatch when loading weights from file. Model expected 13 layers, found 16 saved layers.']",/junobench_env/tensorflow_14/tensorflow_14_extension.ipynb,2026-01-14T16:32:10.191402,CellExecutionError,"An error occurred while executing the following cell:
------------------
# Set the main data directory where subdirectories represent classes/labels
main_data_directory = 'data/train-data-imgs'

# Define the input size for the VGG16 model
input_size = (224, 224)

# Create a data generator for training data
train_datagen = ImageDataGenerator(
    rescale=1.0/255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.2
)

train_generator = train_datagen.flow_from_directory(
    main_data_directory,
    target_size=input_size,
    batch_size=32,
    class_mode='categorical',
    shuffle=True
)

# Load the VGG16 model without the top classification layer
base_model = VGG16(weights='imagenet', include_top=False)

# Make the layers in the base model non-trainable
for layer in base_model.layers:
    layer.trainable = False

# Compile the model with an appropriate optimizer, loss function, and metrics
base_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

x = Flatten()(base_model.output) 
x = Dense(256, activation='relu')(x) 
x = Dropout(0.5)(x) 
predictions = Dense(7, activation='softmax')(x) # 7 classes 
model_for_training = Model(inputs=base_model.input, outputs=predictions) # Compile the new model 
model_for_training.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) 
# Continue training the model 
model_for_training.fit( train_generator, steps_per_epoch=len(train_generator), epochs=10, ) 
# Save the model after additional training 
model_for_training.save('data/updated_vgg16_transfer_weights.h5')

# Load the previously saved model weights
# base_model.load_weights('data/vgg_face_weights.h5')

# # Continue training the model
# base_model.fit(
#     train_generator,
#     steps_per_epoch=len(train_generator),
#     epochs=10,  # You can adjust the number of epochs
# )

# # Save the model after additional training
# base_model.save('data/updated_vgg_face_weights.h5')

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-5-1fbefc451ee9>[0m in [0;36m<cell line: 36>[0;34m()[0m
[1;32m     34[0m [0;34m[0m[0m
[1;32m     35[0m [0mx[0m [0;34m=[0m [0mFlatten[0m[0;34m([0m[0;34m)[0m[0;34m([0m[0mbase_model[0m[0;34m.[0m[0moutput[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 36[0;31m [0mx[0m [0;34m=[0m [0mDense[0m[0;34m([0m[0;36m256[0m[0;34m,[0m [0mactivation[0m[0;34m=[0m[0;34m'relu'[0m[0;34m)[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     37[0m [0mx[0m [0;34m=[0m [0mDropout[0m[0;34m([0m[0;36m0.5[0m[0;34m)[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     38[0m [0mpredictions[0m [0;34m=[0m [0mDense[0m[0;34m([0m[0;36m7[0m[0;34m,[0m [0mactivation[0m[0;34m=[0m[0;34m'softmax'[0m[0;34m)[0m[0;34m([0m[0mx[0m[0;34m)[0m [0;31m# 7 classes[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py[0m in [0;36merror_handler[0;34m(*args, **kwargs)[0m
[1;32m    120[0m             [0;31m# To get the full stack trace, call:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    121[0m             [0;31m# `keras.config.disable_traceback_filtering()`[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 122[0;31m             [0;32mraise[0m [0me[0m[0;34m.[0m[0mwith_traceback[0m[0;34m([0m[0mfiltered_tb[0m[0;34m)[0m [0;32mfrom[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    123[0m         [0;32mfinally[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    124[0m             [0;32mdel[0m [0mfiltered_tb[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/common/variables.py[0m in [0;36m_validate_shape[0;34m(self, shape)[0m
[1;32m    184[0m         [0mshape[0m [0;34m=[0m [0mstandardize_shape[0m[0;34m([0m[0mshape[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    185[0m         [0;32mif[0m [0;32mNone[0m [0;32min[0m [0mshape[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 186[0;31m             raise ValueError(
[0m[1;32m    187[0m                 [0;34m""Shapes used to initialize variables must be ""[0m[0;34m[0m[0;34m[0m[0m
[1;32m    188[0m                 [0;34m""fully-defined (no `None` dimensions). Received: ""[0m[0;34m[0m[0;34m[0m[0m

[0;31mValueError[0m: Shapes used to initialize variables must be fully-defined (no `None` dimensions). Received: shape=(None, 256) for variable path='dense/kernel'
ValueError: Shapes used to initialize variables must be fully-defined (no `None` dimensions). Received: shape=(None, 256) for variable path='dense/kernel'
",ValueError,"Shapes used to initialize variables must be fully-defined (no `None` dimensions). Received: shape=(None, 256) for variable path='dense/kernel'",FALSE,no fix
sklearn_1,2328d822-cd0a-36f9-91ab-ec042fff6bae,typeerror,compute_class_weight() takes 1 positional argument but 3 were given,"['---------------------------------------------------------------------------', 'TypeError                                 Traceback (most recent call last)', ""Cell In[10], line 2\n      1 from sklearn.utils.class_weight import compute_class_weight\n----> 2 class_weights = compute_class_weight('balanced', np.unique(train_dataset.labels), train_dataset.labels)\n      3 class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n      5 # Compile the model with class weights\n"", 'TypeError: compute_class_weight() takes 1 positional argument but 3 were given']",/junobench_env/sklearn_1/sklearn_1_extension.ipynb,2026-01-14T18:17:43.457845,CellExecutionError,"An error occurred while executing the following cell:
------------------
from sklearn.utils.class_weight import compute_class_weight
# class_weights = compute_class_weight('balanced', np.unique(train_dataset.labels), train_dataset.labels)
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_dataset.classes),
    y=train_dataset.classes
)
class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}

# Compile the model with class weights
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=METRICS, class_weight=class_weight_dict)

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mTypeError[0m                                 Traceback (most recent call last)
[0;32m<ipython-input-10-09704a284e6f>[0m in [0;36m<cell line: 11>[0;34m()[0m
[1;32m      9[0m [0;34m[0m[0m
[1;32m     10[0m [0;31m# Compile the model with class weights[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 11[0;31m [0mmodel[0m[0;34m.[0m[0mcompile[0m[0;34m([0m[0moptimizer[0m[0;34m=[0m[0;34m'Adam'[0m[0;34m,[0m [0mloss[0m[0;34m=[0m[0;34m'categorical_crossentropy'[0m[0;34m,[0m [0mmetrics[0m[0;34m=[0m[0mMETRICS[0m[0;34m,[0m [0mclass_weight[0m[0;34m=[0m[0mclass_weight_dict[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py[0m in [0;36merror_handler[0;34m(*args, **kwargs)[0m
[1;32m    120[0m             [0;31m# To get the full stack trace, call:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    121[0m             [0;31m# `keras.config.disable_traceback_filtering()`[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 122[0;31m             [0;32mraise[0m [0me[0m[0;34m.[0m[0mwith_traceback[0m[0;34m([0m[0mfiltered_tb[0m[0;34m)[0m [0;32mfrom[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    123[0m         [0;32mfinally[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    124[0m             [0;32mdel[0m [0mfiltered_tb[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/tracking.py[0m in [0;36mwrapper[0;34m(*args, **kwargs)[0m
[1;32m     24[0m     [0;32mdef[0m [0mwrapper[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m     25[0m         [0;32mwith[0m [0mDotNotTrackScope[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 26[0;31m             [0;32mreturn[0m [0mfn[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     27[0m [0;34m[0m[0m
[1;32m     28[0m     [0;32mreturn[0m [0mwrapper[0m[0;34m[0m[0;34m[0m[0m

[0;31mTypeError[0m: Trainer.compile() got an unexpected keyword argument 'class_weight'
TypeError: Trainer.compile() got an unexpected keyword argument 'class_weight'
",TypeError,Trainer.compile() got an unexpected keyword argument 'class_weight',FALSE,no fix
numpy_1,bf118e95-53d6-3590-a2f1-27af2f11890c,indexerror,index 16 is out of bounds for axis 0 with size 16,"['---------------------------------------------------------------------------', 'IndexError                                Traceback (most recent call last)', ""Cell In[40], line 18\n     16             plt.axis('off')\n     17             k+=1\n---> 18 visual()\n"", 'Cell In[40], line 13, in visual()\n     11 for j in range(n):\n     12     ax=plt.subplot(n,n,k+1)\n---> 13     img = (out[k]+1)/2\n     14     img = np.transpose(img,(1,2,0))\n     15     plt.imshow(img)\n', 'IndexError: index 16 is out of bounds for axis 0 with size 16']",/junobench_env/numpy_1/numpy_1_extension.ipynb,2026-01-14T16:40:49.877922,CellExecutionError,"An error occurred while executing the following cell:
------------------
from torchsummary import summary
g = Gen(100,3,64)
d = Discriminator(3)
print(summary(d, input_size=[1, 3, 64, 64]))
summary(g, input_size=[1, 100, 1, 1])
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mTypeError[0m                                 Traceback (most recent call last)
[0;32m<ipython-input-16-29f5823dbee2>[0m in [0;36m<cell line: 4>[0;34m()[0m
[1;32m      2[0m [0mg[0m [0;34m=[0m [0mGen[0m[0;34m([0m[0;36m100[0m[0;34m,[0m[0;36m3[0m[0;34m,[0m[0;36m64[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m [0md[0m [0;34m=[0m [0mDiscriminator[0m[0;34m([0m[0;36m3[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 4[0;31m [0mprint[0m[0;34m([0m[0msummary[0m[0;34m([0m[0md[0m[0;34m,[0m [0minput_size[0m[0;34m=[0m[0;34m[[0m[0;36m1[0m[0;34m,[0m [0;36m3[0m[0;34m,[0m [0;36m64[0m[0;34m,[0m [0;36m64[0m[0;34m][0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      5[0m [0msummary[0m[0;34m([0m[0mg[0m[0;34m,[0m [0minput_size[0m[0;34m=[0m[0;34m[[0m[0;36m1[0m[0;34m,[0m [0;36m100[0m[0;34m,[0m [0;36m1[0m[0;34m,[0m [0;36m1[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py[0m in [0;36msummary[0;34m(model, input_size, batch_size, device)[0m
[1;32m     58[0m [0;34m[0m[0m
[1;32m     59[0m     [0;31m# batch_size of 2 for batchnorm[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 60[0;31m     [0mx[0m [0;34m=[0m [0;34m[[0m[0mtorch[0m[0;34m.[0m[0mrand[0m[0;34m([0m[0;36m2[0m[0;34m,[0m [0;34m*[0m[0min_size[0m[0;34m)[0m[0;34m.[0m[0mtype[0m[0;34m([0m[0mdtype[0m[0;34m)[0m [0;32mfor[0m [0min_size[0m [0;32min[0m [0minput_size[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     61[0m     [0;31m# print(type(x[0]))[0m[0;34m[0m[0;34m[0m[0m
[1;32m     62[0m [0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py[0m in [0;36m<listcomp>[0;34m(.0)[0m
[1;32m     58[0m [0;34m[0m[0m
[1;32m     59[0m     [0;31m# batch_size of 2 for batchnorm[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 60[0;31m     [0mx[0m [0;34m=[0m [0;34m[[0m[0mtorch[0m[0;34m.[0m[0mrand[0m[0;34m([0m[0;36m2[0m[0;34m,[0m [0;34m*[0m[0min_size[0m[0;34m)[0m[0;34m.[0m[0mtype[0m[0;34m([0m[0mdtype[0m[0;34m)[0m [0;32mfor[0m [0min_size[0m [0;32min[0m [0minput_size[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     61[0m     [0;31m# print(type(x[0]))[0m[0;34m[0m[0;34m[0m[0m
[1;32m     62[0m [0;34m[0m[0m

[0;31mTypeError[0m: Value after * must be an iterable, not int
TypeError: Value after * must be an iterable, not int
",TypeError,"Value after * must be an iterable, not int",FALSE,fixed the error
torch_2,62b1ea91-a9e0-350d-a67f-552087ddef56,runtimeerror,"
method cannot be used as a value:
  File ""/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py"", line 1129
                self.in_proj_weight,
                self.in_proj_bias,
                self.out_proj.weight,
                ~~~~~~~~~~~~~~~~~~~~ <--- HERE
                self.out_proj.bias,
            )
","['---------------------------------------------------------------------------', 'RuntimeError                              Traceback (most recent call last)', 'Cell In[39], line 4\n      2 weights = ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\n      3 model = vit_b_16(weights=weights)\n----> 4 quantized_vit = model_quantization(model=model, save=True)\n      6 accuracy, duration = inference(model=model, dataloader=dataloader, class_dict=class_dict, device=device, image_num_stop=100)\n      8 print(""Inference took {} minutes"".format(duration))\n', 'Cell In[36], line 17, in model_quantization(model, backend, save)\n     14 torch.backends.quantized.engine = backend\n     16 quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)\n---> 17 scripted_quantized_model = torch.jit.script(quantized_model)\n     18 if save:\n     19     scripted_quantized_model.save(""vit_scripted_quantized.pt"")\n', 'File /opt/conda/lib/python3.10/site-packages/torch/jit/_script.py:1284, in script(obj, optimize, _frames_up, _rcb, example_inputs)\n   1282 if isinstance(obj, torch.nn.Module):\n   1283     obj = call_prepare_scriptable_func(obj)\n-> 1284     return torch.jit._recursive.create_script_module(\n   1285         obj, torch.jit._recursive.infer_methods_to_compile\n   1286     )\n   1288 if isinstance(obj, dict):\n   1289     return create_script_dict(obj)\n', 'File /opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:480, in create_script_module(nn_module, stubs_fn, share_types, is_tracing)\n    478 if not is_tracing:\n    479     AttributeTypeIsSupportedChecker().check(nn_module)\n--> 480 return create_script_module_impl(nn_module, concrete_type, stubs_fn)\n', 'File /opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:542, in create_script_module_impl(nn_module, concrete_type, stubs_fn)\n    539     script_module._concrete_type = concrete_type\n    541 # Actually create the ScriptModule, initializing it with the function we just defined\n--> 542 script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)\n    544 # Compile methods if necessary\n    545 if concrete_type not in concrete_type_store.methods_compiled:\n', 'File /opt/conda/lib/python3.10/site-packages/torch/jit/_script.py:614, in RecursiveScriptModule._construct(cpp_module, init_fn)\n    601 """"""\n    602 Construct a RecursiveScriptModule that\'s ready for use. PyTorch\n    603 code should use this to construct a RecursiveScriptModule instead\n   (...)\n    611     init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\n    612 """"""\n    613 script_module = RecursiveScriptModule(cpp_module)\n--> 614 init_fn(script_module)\n    616 # Finalize the ScriptModule: replace the nn.Module state with our\n    617 # custom implementations and flip the _initializing bit.\n    618 RecursiveScriptModule._finalize_scriptmodule(script_module)\n', 'File /opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:520, in create_script_module_impl.<locals>.init_fn(script_module)\n    517     scripted = orig_value\n    518 else:\n    519     # always reuse the provided stubs_fn to infer the methods to compile\n--> 520     scripted = create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n    522 cpp_module.setattr(name, scripted)\n    523 script_module._modules[name] = scripted\n', 'File /opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:542, in create_script_module_impl(nn_module, concrete_type, stubs_fn)\n    539     script_module._concrete_type = concrete_type\n    541 # Actually create the ScriptModule, initializing it with the function we just defined\n--> 542 script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)\n    544 # Compile methods if necessary\n    545 if concrete_type not in concrete_type_store.methods_compiled:\n', 'File /opt/conda/lib/python3.10/site-packages/torch/jit/_script.py:614, in RecursiveScriptModule._construct(cpp_module, init_fn)\n    601 """"""\n    602 Construct a RecursiveScriptModule that\'s ready for use. PyTorch\n    603 code should use this to construct a RecursiveScriptModule instead\n   (...)\n    611     init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\n    612 """"""\n    613 script_module = RecursiveScriptModule(cpp_module)\n--> 614 init_fn(script_module)\n    616 # Finalize the ScriptModule: replace the nn.Module state with our\n    617 # custom implementations and flip the _initializing bit.\n    618 RecursiveScriptModule._finalize_scriptmodule(script_module)\n', 'File /opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:520, in create_script_module_impl.<locals>.init_fn(script_module)\n    517     scripted = orig_value\n    518 else:\n    519     # always reuse the provided stubs_fn to infer the methods to compile\n--> 520     scripted = create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n    522 cpp_module.setattr(name, scripted)\n    523 script_module._modules[name] = scripted\n', '    [... skipping similar frames: RecursiveScriptModule._construct at line 614 (1 times), create_script_module_impl at line 542 (1 times), create_script_module_impl.<locals>.init_fn at line 520 (1 times)]\n', 'File /opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:542, in create_script_module_impl(nn_module, concrete_type, stubs_fn)\n    539     script_module._concrete_type = concrete_type\n    541 # Actually create the ScriptModule, initializing it with the function we just defined\n--> 542 script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)\n    544 # Compile methods if necessary\n    545 if concrete_type not in concrete_type_store.methods_compiled:\n', 'File /opt/conda/lib/python3.10/site-packages/torch/jit/_script.py:614, in RecursiveScriptModule._construct(cpp_module, init_fn)\n    601 """"""\n    602 Construct a RecursiveScriptModule that\'s ready for use. PyTorch\n    603 code should use this to construct a RecursiveScriptModule instead\n   (...)\n    611     init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\n    612 """"""\n    613 script_module = RecursiveScriptModule(cpp_module)\n--> 614 init_fn(script_module)\n    616 # Finalize the ScriptModule: replace the nn.Module state with our\n    617 # custom implementations and flip the _initializing bit.\n    618 RecursiveScriptModule._finalize_scriptmodule(script_module)\n', 'File /opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:520, in create_script_module_impl.<locals>.init_fn(script_module)\n    517     scripted = orig_value\n    518 else:\n    519     # always reuse the provided stubs_fn to infer the methods to compile\n--> 520     scripted = create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n    522 cpp_module.setattr(name, scripted)\n    523 script_module._modules[name] = scripted\n', ""File /opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:546, in create_script_module_impl(nn_module, concrete_type, stubs_fn)\n    544 # Compile methods if necessary\n    545 if concrete_type not in concrete_type_store.methods_compiled:\n--> 546     create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)\n    547     # Create hooks after methods to ensure no name collisions between hooks and methods.\n    548     # If done before, hooks can overshadow methods that aren't exported.\n    549     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n"", 'File /opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:397, in create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)\n    394 property_defs = [p.def_ for p in property_stubs]\n    395 property_rcbs = [p.resolution_callback for p in property_stubs]\n--> 397 concrete_type._create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults)\n', 'RuntimeError: \nmethod cannot be used as a value:\n  File ""/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py"", line 1129\n                self.in_proj_weight,\n                self.in_proj_bias,\n                self.out_proj.weight,\n                ~~~~~~~~~~~~~~~~~~~~ <--- HERE\n                self.out_proj.bias,\n            )\n']",/junobench_env/torch_2/torch_2_extension.ipynb,2026-01-14T18:32:43.881208,CellExecutionError,"An error occurred while executing the following cell:
------------------
from torchvision.io import read_image
from torchvision.models import vit_b_16, ViT_B_16_Weights, list_models
from torchvision.datasets import ImageNet, ImageFolder
from torch.utils.data import DataLoader
from torchmetrics.classification import MulticlassAccuracy
import torch
import time
from torchvision.transforms import transforms
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m<ipython-input-1-fd60bcbf1e25>[0m in [0;36m<cell line: 1>[0;34m()[0m
[0;32m----> 1[0;31m [0;32mfrom[0m [0mtorchvision[0m[0;34m.[0m[0mio[0m [0;32mimport[0m [0mread_image[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0;32mfrom[0m [0mtorchvision[0m[0;34m.[0m[0mmodels[0m [0;32mimport[0m [0mvit_b_16[0m[0;34m,[0m [0mViT_B_16_Weights[0m[0;34m,[0m [0mlist_models[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m [0;32mfrom[0m [0mtorchvision[0m[0;34m.[0m[0mdatasets[0m [0;32mimport[0m [0mImageNet[0m[0;34m,[0m [0mImageFolder[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0;32mfrom[0m [0mtorch[0m[0;34m.[0m[0mutils[0m[0;34m.[0m[0mdata[0m [0;32mimport[0m [0mDataLoader[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0;32mfrom[0m [0mtorchmetrics[0m[0;34m.[0m[0mclassification[0m [0;32mimport[0m [0mMulticlassAccuracy[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py[0m in [0;36m<module>[0;34m[0m
[1;32m      8[0m [0;31m# .extensions) before entering _meta_registrations.[0m[0;34m[0m[0;34m[0m[0m
[1;32m      9[0m [0;32mfrom[0m [0;34m.[0m[0mextension[0m [0;32mimport[0m [0m_HAS_OPS[0m  [0;31m# usort:skip[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 10[0;31m [0;32mfrom[0m [0mtorchvision[0m [0;32mimport[0m [0m_meta_registrations[0m[0;34m,[0m [0mdatasets[0m[0;34m,[0m [0mio[0m[0;34m,[0m [0mmodels[0m[0;34m,[0m [0mops[0m[0;34m,[0m [0mtransforms[0m[0;34m,[0m [0mutils[0m  [0;31m# usort:skip[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     11[0m [0;34m[0m[0m
[1;32m     12[0m [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/_meta_registrations.py[0m in [0;36m<module>[0;34m[0m
[1;32m      2[0m [0;34m[0m[0m
[1;32m      3[0m [0;32mimport[0m [0mtorch[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 4[0;31m [0;32mimport[0m [0mtorch[0m[0;34m.[0m[0m_custom_ops[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      5[0m [0;32mimport[0m [0mtorch[0m[0;34m.[0m[0mlibrary[0m[0;34m[0m[0;34m[0m[0m
[1;32m      6[0m [0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'torch._custom_ops'
ModuleNotFoundError: No module named 'torch._custom_ops'
",ModuleNotFoundError,No module named 'torch._custom_ops',FALSE,no fix
numpy_2,58c74029-abf0-35c8-9de9-c0531b72c44a,attributeerror,'numpy.ndarray' object has no attribute 'join',"['---------------------------------------------------------------------------', 'AttributeError                            Traceback (most recent call last)', 'Cell In[23], line 1\n----> 1 train_houseprice=X_train.join(Y_train)\n', ""AttributeError: 'numpy.ndarray' object has no attribute 'join'""]",/junobench_env/numpy_2/numpy_2_extension.ipynb,2026-01-14T18:17:31.134636,CellExecutionError,"An error occurred while executing the following cell:
------------------
arr = np.concatenate((arr1, arr2), axis=1)

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m<ipython-input-23-102734ed91e7>[0m in [0;36m<cell line: 1>[0;34m()[0m
[0;32m----> 1[0;31m [0marr[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0mconcatenate[0m[0;34m([0m[0;34m([0m[0marr1[0m[0;34m,[0m [0marr2[0m[0;34m)[0m[0;34m,[0m [0maxis[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;31mNameError[0m: name 'arr1' is not defined
NameError: name 'arr1' is not defined
",NameError,name 'arr1' is not defined,FALSE,fixed the error
numpy_4,1bf30d81-9479-3ddf-a67c-8a6326914d7a,attributeerror,'numpy.ndarray' object has no attribute 'nparray',"['---------------------------------------------------------------------------', 'AttributeError                            Traceback (most recent call last)', '/tmp/ipykernel_23/3824881073.py in <module>\n      8   prediction_classes = np.concatenate([prediction_classes,\n      9                        np.argmax(model.predict(x), axis = -1)])\n---> 10   true_classes = np.concatenate([true_classes, np.argmax(y.nparray(), axis=-1)])\n     11 \n     12 \n', ""AttributeError: 'numpy.ndarray' object has no attribute 'nparray'""]",/junobench_env/numpy_4/numpy_4_extension.ipynb,2026-01-14T18:13:40.923602,CellExecutionError,"An error occurred while executing the following cell:
------------------
model.summary()
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-16-5f15418b3570>[0m in [0;36m<cell line: 1>[0;34m()[0m
[0;32m----> 1[0;31m [0mmodel[0m[0;34m.[0m[0msummary[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py[0m in [0;36merror_handler[0;34m(*args, **kwargs)[0m
[1;32m    120[0m             [0;31m# To get the full stack trace, call:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    121[0m             [0;31m# `keras.config.disable_traceback_filtering()`[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 122[0;31m             [0;32mraise[0m [0me[0m[0;34m.[0m[0mwith_traceback[0m[0;34m([0m[0mfiltered_tb[0m[0;34m)[0m [0;32mfrom[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    123[0m         [0;32mfinally[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    124[0m             [0;32mdel[0m [0mfiltered_tb[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/optree/ops.py[0m in [0;36mtree_map[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)[0m
[1;32m    745[0m     [0mleaves[0m[0;34m,[0m [0mtreespec[0m [0;34m=[0m [0m_C[0m[0;34m.[0m[0mflatten[0m[0;34m([0m[0mtree[0m[0;34m,[0m [0mis_leaf[0m[0;34m,[0m [0mnone_is_leaf[0m[0;34m,[0m [0mnamespace[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    746[0m     [0mflat_args[0m [0;34m=[0m [0;34m[[0m[0mleaves[0m[0;34m][0m [0;34m+[0m [0;34m[[0m[0mtreespec[0m[0;34m.[0m[0mflatten_up_to[0m[0;34m([0m[0mr[0m[0;34m)[0m [0;32mfor[0m [0mr[0m [0;32min[0m [0mrests[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 747[0;31m     [0;32mreturn[0m [0mtreespec[0m[0;34m.[0m[0munflatten[0m[0;34m([0m[0mmap[0m[0;34m([0m[0mfunc[0m[0;34m,[0m [0;34m*[0m[0mflat_args[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    748[0m [0;34m[0m[0m
[1;32m    749[0m [0;34m[0m[0m

[0;31mValueError[0m: Undefined shapes are not supported.
ValueError: Undefined shapes are not supported.
",ValueError,Undefined shapes are not supported.,FALSE,no fix
torch_5,521b0219-1ec1-3d72-b873-e072e726afbf,typeerror,"conv2d() received an invalid combination of arguments - got (torch.device, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:","['---------------------------------------------------------------------------', 'TypeError                                 Traceback (most recent call last)', 'Cell In[130], line 4\n      1 if __name__ == ""__main__"":\n      2     \n      3     # Let\'s build our model\n----> 4     train(5)\n      5     print(\'Finished Training\')\n      7     # Test which classes performed well\n', 'Cell In[128], line 56, in train(num_epochs)\n     54 optimizer.zero_grad()\n     55 # predict classes using images from the training set\n---> 56 outputs = model(device)\n     57 # compute the loss based on model output and real labels\n     58 loss = loss_fn(outputs, class_names)\n', ""File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have any hooks, we want to skip the rest of the logic in\n   1497 # this function, and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1499         or _global_backward_pre_hooks or _global_backward_hooks\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1501     return forward_call(*args, **kwargs)\n   1502 # Do not call functions when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks = [], []\n"", 'Cell In[126], line 23, in Network.forward(self, input)\n     22 def forward(self, input):\n---> 23     output = F.relu(self.bn1(self.conv1(input)))      \n     24     output = F.relu(self.bn2(self.conv2(output)))     \n     25     output = self.pool(output)                        \n', ""File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have any hooks, we want to skip the rest of the logic in\n   1497 # this function, and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1499         or _global_backward_pre_hooks or _global_backward_hooks\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1501     return forward_call(*args, **kwargs)\n   1502 # Do not call functions when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks = [], []\n"", 'File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:463, in Conv2d.forward(self, input)\n    462 def forward(self, input: Tensor) -> Tensor:\n--> 463     return self._conv_forward(input, self.weight, self.bias)\n', ""File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:459, in Conv2d._conv_forward(self, input, weight, bias)\n    455 if self.padding_mode != 'zeros':\n    456     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n    457                     weight, bias, self.stride,\n    458                     _pair(0), self.dilation, self.groups)\n--> 459 return F.conv2d(input, weight, bias, self.stride,\n    460                 self.padding, self.dilation, self.groups)\n"", ""TypeError: conv2d() received an invalid combination of arguments - got (torch.device, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!torch.device!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!torch.device!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n""]",/junobench_env/torch_5/torch_5_extension.ipynb,2026-01-14T18:32:47.621035,CellExecutionError,"An error occurred while executing the following cell:
------------------
import os, time
import numpy as np
import random
random.seed(42)
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report

import torch
torch.manual_seed(42)
from torch import nn
from torch.optim import SGD, Adam
from torch.utils.data import DataLoader, RandomSampler
from torch.utils.data.dataset import Dataset
from torchvision.models import resnet
from torchvision import transforms, datasets, models
from torch.optim.lr_scheduler import ReduceLROnPlateau
import torch
import torch.nn as nn
import torchvision
import torch.nn.functional as F
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m<ipython-input-1-6a55ac603f3b>[0m in [0;36m<cell line: 16>[0;34m()[0m
[1;32m     14[0m [0;32mfrom[0m [0mtorch[0m[0;34m.[0m[0mutils[0m[0;34m.[0m[0mdata[0m [0;32mimport[0m [0mDataLoader[0m[0;34m,[0m [0mRandomSampler[0m[0;34m[0m[0;34m[0m[0m
[1;32m     15[0m [0;32mfrom[0m [0mtorch[0m[0;34m.[0m[0mutils[0m[0;34m.[0m[0mdata[0m[0;34m.[0m[0mdataset[0m [0;32mimport[0m [0mDataset[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 16[0;31m [0;32mfrom[0m [0mtorchvision[0m[0;34m.[0m[0mmodels[0m [0;32mimport[0m [0mresnet[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     17[0m [0;32mfrom[0m [0mtorchvision[0m [0;32mimport[0m [0mtransforms[0m[0;34m,[0m [0mdatasets[0m[0;34m,[0m [0mmodels[0m[0;34m[0m[0;34m[0m[0m
[1;32m     18[0m [0;32mfrom[0m [0mtorch[0m[0;34m.[0m[0moptim[0m[0;34m.[0m[0mlr_scheduler[0m [0;32mimport[0m [0mReduceLROnPlateau[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py[0m in [0;36m<module>[0;34m[0m
[1;32m      8[0m [0;31m# .extensions) before entering _meta_registrations.[0m[0;34m[0m[0;34m[0m[0m
[1;32m      9[0m [0;32mfrom[0m [0;34m.[0m[0mextension[0m [0;32mimport[0m [0m_HAS_OPS[0m  [0;31m# usort:skip[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 10[0;31m [0;32mfrom[0m [0mtorchvision[0m [0;32mimport[0m [0m_meta_registrations[0m[0;34m,[0m [0mdatasets[0m[0;34m,[0m [0mio[0m[0;34m,[0m [0mmodels[0m[0;34m,[0m [0mops[0m[0;34m,[0m [0mtransforms[0m[0;34m,[0m [0mutils[0m  [0;31m# usort:skip[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     11[0m [0;34m[0m[0m
[1;32m     12[0m [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/_meta_registrations.py[0m in [0;36m<module>[0;34m[0m
[1;32m      2[0m [0;34m[0m[0m
[1;32m      3[0m [0;32mimport[0m [0mtorch[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 4[0;31m [0;32mimport[0m [0mtorch[0m[0;34m.[0m[0m_custom_ops[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      5[0m [0;32mimport[0m [0mtorch[0m[0;34m.[0m[0mlibrary[0m[0;34m[0m[0;34m[0m[0m
[1;32m      6[0m [0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'torch._custom_ops'
ModuleNotFoundError: No module named 'torch._custom_ops'
",ModuleNotFoundError,No module named 'torch._custom_ops',FALSE,no fix
numpy_6,d2d5322a-22d2-350a-a5e4-1618d73b3ab1,indexerror,"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices","['---------------------------------------------------------------------------', 'IndexError                                Traceback (most recent call last)', ""Cell In[39], line 13\n     11     x = np.expand_dims(img.copy(), axis=0)\n     12     X_data[i] = x / 255.0\n---> 13 X_data['id'] = train_labels['id']\n     15 # Printing train image and one hot encode shape & size\n     16 print('\\nTrain Images shape: ',X_data.shape,' size: {:,}'.format(X_data.size))\n"", 'IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices']",/junobench_env/numpy_6/numpy_6_extension.ipynb,2026-01-14T16:40:59.245283,CellExecutionError,"An error occurred while executing the following cell:
------------------
# Create X_data as a pandas dataframe
X_data = pd.DataFrame(columns=['image'])

# Loop over the images and load them into X_data
for i, file_path in enumerate(train_labels):
    img = cv2.imread(file_path)
    print(i, file_path, img)
    x = np.expand_dims(img,axis=0)
    print(i, x)
    X_data.loc[i] = x[i] / 255.0

# Create train_labels as a pandas dataframe
# train_labels = pd.read_csv('/kaggle/input/dog-breeding/New folder/train')

# Merge X_data and train_labels on the 'id' column
# train_data = pd.merge(X_data, train_labels, on='id')

# Print train image and one hot encode shape & size
print('\nTrain Images shape:', X_data.shape, ' size: {:,}'.format(X_data.size))
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mTypeError[0m                                 Traceback (most recent call last)
[0;32m<ipython-input-9-2d8c0b6b9b90>[0m in [0;36m<cell line: 5>[0;34m()[0m
[1;32m      8[0m     [0mx[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0mexpand_dims[0m[0;34m([0m[0mimg[0m[0;34m,[0m[0maxis[0m[0;34m=[0m[0;36m0[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      9[0m     [0mprint[0m[0;34m([0m[0mi[0m[0;34m,[0m [0mx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 10[0;31m     [0mX_data[0m[0;34m.[0m[0mloc[0m[0;34m[[0m[0mi[0m[0;34m][0m [0;34m=[0m [0mx[0m[0;34m[[0m[0mi[0m[0;34m][0m [0;34m/[0m [0;36m255.0[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     11[0m [0;34m[0m[0m
[1;32m     12[0m [0;31m# Create train_labels as a pandas dataframe[0m[0;34m[0m[0;34m[0m[0m

[0;31mTypeError[0m: unsupported operand type(s) for /: 'NoneType' and 'float'
TypeError: unsupported operand type(s) for /: 'NoneType' and 'float'
",TypeError,unsupported operand type(s) for /: 'NoneType' and 'float',FALSE,no fix
torch_6,3d372934-eff5-34fd-98a0-3fedf1531021,runtimeerror,The size of tensor a (64) must match the size of tensor b (6294) at non-singleton dimension 1,"['---------------------------------------------------------------------------', 'RuntimeError                              Traceback (most recent call last)', 'Cell In[55], line 7\n      4 scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=.1, threshold=1e-6)\n      6 for e in range(epoch):\n----> 7     pred,labels,loss = run_model(model,train_dataloader,optimizer)\n      8     #training accuracy\n      9     correct=0\n', 'Cell In[54], line 37, in run_model(model, dataloader, optimizer, train)\n     35 optimizer.step()\n     36 _, predicted_labels = torch.max(output, dim=1)\n---> 37 correct = (predicted_labels == label).sum().item()\n     38 total_correct += correct\n     39 total_samples += label.size(0)\n', 'RuntimeError: The size of tensor a (64) must match the size of tensor b (6294) at non-singleton dimension 1']",/junobench_env/torch_6/torch_6_extension.ipynb,2026-01-14T17:49:13.703775,CellExecutionError,"An error occurred while executing the following cell:
------------------
#no longer a testing block
device = ""cuda"" if torch.cuda.is_available() else ""cpu""
mo, preprocess = clip.load(""ViT-L/14"", device=device)

# X_train=pd.DataFrame.from_dict(X_train)
cols=t.columns
# X_train=X_train.to_numpy()
# X_train=pd.DataFrame(X_train,columns=cols)
print(X_train.shape)
path=""data_small/train/train/""
image_pro_stack = torch.empty((1,3,224,224)).to(device)
image_array =[]
img_vectors=[]
imageS=[]
all_features=torch.empty((1,1536)).to(device)
for i in range(X_train[""image""].shape[0]):
    image_array.append((path+X_train['image'][i]))
image_array = np.array(image_array)
all_tensors=[]
batch=20
for i in range(0,image_array.shape[0],batch):
    if (i+batch)<image_array.shape[0]:
        img_dirs=image_array[i:i+batch] 
        for img in img_dirs:
            image = preprocess(Image.open(img)).unsqueeze(0).to(device)
            image_pro_stack=torch.cat((image_pro_stack,image)).to(device)
        temp=range(i,i+batch)
        # text = clip.tokenize(df.loc[temp,""question""]).to(device)
        text = clip.tokenize(X_train.loc[temp, ""question""]).to(device)
    elif ((i+batch)>= image_array.shape[0]) and (i < image_array.shape[0]):
        img_dirs=image_array[i:image_array.shape[0]]
        for img in img_dirs:
            image = preprocess(Image.open(img)).unsqueeze(0).to(device)
            image_pro_stack=torch.cat((image_pro_stack,image)).to(device)
        temp=range(i,image_array.shape[0])
        # text = clip.tokenize(df.loc[temp,""question""]).to(device)
        text = clip.tokenize(X_train.loc[temp, ""question""]).to(device)



    with torch.no_grad():
        text_features = mo.encode_text(text).to(device)
        image_pro_stack = image_pro_stack[1:image_pro_stack.shape[0]].to(device)
        img_features=mo.encode_image(image_pro_stack).to(device)
        #print(""size of image is"")
        #print(img_features.shape)
        #print(""size of text is "")
        #print(text_features.shape)
        #print(i)
        feature_vector=torch.cat((img_features,text_features),dim=1).to(device)
        all_features=torch.cat((all_features,feature_vector)).to(device)
    #print(feature_vector.shape)
    image_pro_stack = torch.empty((1,3,224,224)).to(device)

all_features = all_features[1:all_features.shape[0]].to(device)
print(""done"")
    

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mKeyError[0m                                  Traceback (most recent call last)
[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py[0m in [0;36mget_loc[0;34m(self, key)[0m
[1;32m   3652[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 3653[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_engine[0m[0;34m.[0m[0mget_loc[0m[0;34m([0m[0mcasted_key[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   3654[0m         [0;32mexcept[0m [0mKeyError[0m [0;32mas[0m [0merr[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx[0m in [0;36mpandas._libs.index.IndexEngine.get_loc[0;34m()[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx[0m in [0;36mpandas._libs.index.IndexEngine.get_loc[0;34m()[0m

[0;32mpandas/_libs/hashtable_class_helper.pxi[0m in [0;36mpandas._libs.hashtable.Int64HashTable.get_item[0;34m()[0m

[0;32mpandas/_libs/hashtable_class_helper.pxi[0m in [0;36mpandas._libs.hashtable.Int64HashTable.get_item[0;34m()[0m

[0;31mKeyError[0m: 11

The above exception was the direct cause of the following exception:

[0;31mKeyError[0m                                  Traceback (most recent call last)
[0;32m<ipython-input-18-a26b24fd7234>[0m in [0;36m<cell line: 16>[0;34m()[0m
[1;32m     15[0m [0mall_features[0m[0;34m=[0m[0mtorch[0m[0;34m.[0m[0mempty[0m[0;34m([0m[0;34m([0m[0;36m1[0m[0;34m,[0m[0;36m1536[0m[0;34m)[0m[0;34m)[0m[0;34m.[0m[0mto[0m[0;34m([0m[0mdevice[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     16[0m [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mX_train[0m[0;34m[[0m[0;34m""image""[0m[0;34m][0m[0;34m.[0m[0mshape[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 17[0;31m     [0mimage_array[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0;34m([0m[0mpath[0m[0;34m+[0m[0mX_train[0m[0;34m[[0m[0;34m'image'[0m[0;34m][0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     18[0m [0mimage_array[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0marray[0m[0;34m([0m[0mimage_array[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     19[0m [0mall_tensors[0m[0;34m=[0m[0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py[0m in [0;36m__getitem__[0;34m(self, key)[0m
[1;32m   1005[0m [0;34m[0m[0m
[1;32m   1006[0m         [0;32melif[0m [0mkey_is_scalar[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1007[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_get_value[0m[0;34m([0m[0mkey[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1008[0m [0;34m[0m[0m
[1;32m   1009[0m         [0;32mif[0m [0mis_hashable[0m[0;34m([0m[0mkey[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py[0m in [0;36m_get_value[0;34m(self, label, takeable)[0m
[1;32m   1114[0m [0;34m[0m[0m
[1;32m   1115[0m         [0;31m# Similar to Index.get_value, but we do not fall back to positional[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1116[0;31m         [0mloc[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mindex[0m[0;34m.[0m[0mget_loc[0m[0;34m([0m[0mlabel[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1117[0m [0;34m[0m[0m
[1;32m   1118[0m         [0;32mif[0m [0mis_integer[0m[0;34m([0m[0mloc[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py[0m in [0;36mget_loc[0;34m(self, key)[0m
[1;32m   3653[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_engine[0m[0;34m.[0m[0mget_loc[0m[0;34m([0m[0mcasted_key[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m   3654[0m         [0;32mexcept[0m [0mKeyError[0m [0;32mas[0m [0merr[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 3655[0;31m             [0;32mraise[0m [0mKeyError[0m[0;34m([0m[0mkey[0m[0;34m)[0m [0;32mfrom[0m [0merr[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   3656[0m         [0;32mexcept[0m [0mTypeError[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   3657[0m             [0;31m# If we have a listlike key, _check_indexing_error will raise[0m[0;34m[0m[0;34m[0m[0m

[0;31mKeyError[0m: 11
KeyError: 11
",KeyError,11,FALSE,no fix
numpy_7,127ce557-6d79-3486-bb70-0a08287a1e5c,typeerror,only integer scalar arrays can be converted to a scalar index,"['---------------------------------------------------------------------------', 'TypeError                                 Traceback (most recent call last)', ""Cell In[16], line 168\n    166 print('\\n--- Predicting image from test set ---')\n    167 image_idx = 40                                                          # index of image to predict\n--> 168 predict(model, image_idx)\n    170 print('\\n--- Plotting weight distributions ---')\n    171 plot_weights(model)\n"", ""Cell In[16], line 71, in predict(model, image_idx)\n     68 image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n     69 pred = np.argmax(model.predict(image))\n---> 71 plot_sample(dataset['test_images'][image_idx], classes[dataset['test_labels'][image_idx]], classes[pred])\n     73 # extracting the output and appending to outputs\n     74 feature_maps = []\n"", 'TypeError: only integer scalar arrays can be converted to a scalar index']",/junobench_env/numpy_7/numpy_7_extension.ipynb,2026-01-14T16:31:36.855604,CellExecutionError,"An error occurred while executing the following cell:
------------------
from keras.regularizers import l2
from keras.optimizers import SGD
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten
from keras.utils import to_categorical
from keras.callbacks import Callback
from keras import Model
from keras.models import load_model
import numpy as np
import os
import matplotlib.pyplot as plt


class History(Callback):
    def __init__(self, model, validation_images, validation_labels):
        self.model_ = model
        self.validation_images = validation_images
        self.validation_labels = validation_labels
        self.accuracy = [0]
        self.loss = [5]
        self.val_accuracy = [0]
        self.val_loss = [5]

    def on_batch_end(self, batch, logs={}):
        scores = self.model_.evaluate(
            self.validation_images,
            self.validation_labels,
            verbose=0
        )
        print('\n', scores, '\n')
        self.loss.append(logs.get('loss'))
        self.accuracy.append(logs.get('accuracy'))
        self.val_loss.append(scores[0])
        self.val_accuracy.append(scores[1])


def train(model, train_images, train_labels, validation_images, validation_labels, batch_size, num_epochs, learning_rate, verbose):
    opt = SGD(learning_rate)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])         # compile model

    history = History(model, validation_images, validation_labels)                              # train model
    model.fit(
        train_images,
        train_labels,
        batch_size=batch_size,
        epochs=num_epochs,
        callbacks=[history]
    )

    if verbose:
        plot_learning_curve(history.loss)
        plot_accuracy_curve(history.accuracy, history.val_accuracy)


def evaluate(model):
    scores = model.evaluate(X_test, test_labels, verbose=1)            # evaluate model
    print('Test loss:', scores[0])
    print('Test accuracy:', scores[1])


# def predict(model, image_idx):
#     layer_names = ['conv1', 'conv2', 'conv3', 'conv4']          # names of layers from which we will take the output
#     num_features = 4                                            # number of feature maps to display per layer

#     dataset = load_cifar()
#     #dataset['test_images'] = np.moveaxis(dataset['test_images'], 1, 3)
#     image = test_images[image_idx]
#     image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
#     pred = np.argmax(model.predict(image))

#     plot_sample(dataset['test_images'][image_idx], classes[dataset['test_labels'][image_idx]], classes[pred])

#     # extracting the output and appending to outputs
#     feature_maps = []
#     for name in layer_names:
#         tmp_model = Model(inputs=model.input, outputs=model.get_layer(name).output)
#         feature_maps.append(tmp_model.predict(image))

#     fig, ax = plt.subplots(nrows=len(feature_maps), ncols=num_features, figsize=(20, 20))
#     for i in range(len(feature_maps)):
#         for z in range(num_features):
#             ax[i][z].imshow(feature_maps[i][0, :, :, z])
#             ax[i][z].set_title(layer_names[i])
#             ax[i][z].set_xticks([])
#             ax[i][z].set_yticks([])
#     plt.savefig('feature_maps.png')

def predict(model, image_idx, test_images, test_labels, classes): 
    image = test_images[image_idx] 
    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2])) 
    pred = np.argmax(model.predict(image))
    plot_sample(test_images[image_idx], classes[test_labels[image_idx].item()], classes[pred]) 


def plot_weights(model):
    for layer in model.layers:
        if 'conv' in layer.name:
            weights, _ = layer.get_weights()
            plot_histogram(layer.name, np.reshape(weights, -1))


if __name__ == '__main__':

    classes = [                                                             # CIFAR-10 classes
        ""airplane"",
        ""automobile"",
        ""bird"",
        ""cat"",
        ""deer"",
        ""dog"",
        ""frog"",
        ""horse"",
        ""ship"",
        ""truck""
    ]

    num_epochs = 1 #10                                                        # hyper parameters
    learning_rate = 0.005
    batch_size = 2000 #100
    lam = 0.01
    verbose = 1

    print('\n--- Loading mnist dataset ---')                                # load dataset
    dataset = load_cifar()

    print('\n--- Processing the dataset ---')                               # pre process dataset
    dataset = preprocess(dataset)

    # train_images = np.moveaxis(dataset['train_images'], 1, 3)               # pre process data for keras
    # validation_images = np.moveaxis(dataset['validation_images'], 1, 3)
    # test_images = np.moveaxis(dataset['test_images'], 1, 3)
    train_labels = to_categorical(dataset['train_labels'].flatten())
    validation_labels = to_categorical(dataset['validation_labels'])
    test_labels = to_categorical(dataset['test_labels'])

    if os.path.isfile('model.h5'):                                          # load model
        print('\n--- Loading model ---')
        model = load_model('model.h5')
    else:                                                                   # build model
        print('\n--- Building model ---')
        model = Sequential()
        model.add(Conv2D(32, 3, name='conv1', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(lam), input_shape=(32, 32, 3)))
        model.add(Conv2D(32, 3, name='conv2', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(lam)))
        model.add(MaxPooling2D(2, name='pool1'))
        model.add(Conv2D(64, 3, name='conv3', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(lam)))
        model.add(Conv2D(64, 3, name='conv4', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(lam)))
        model.add(MaxPooling2D(2, name='pool2'))
        model.add(Flatten())
        model.add(Dense(256, name='fullyconnected', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(lam)))
        model.add(Dense(10, name='dense', activation='softmax'))
        
        # train_images = np.moveaxis(train_images, -1, 1)
        # validation_images = np.moveaxis(validation_images, -1, 1)
        # test_images = np.moveaxis(test_images, -1, 1)


    train(
        model,
        X_train,
        train_labels,
        X_val,
        validation_labels,
        batch_size,
        num_epochs,
        learning_rate,
        verbose
    )

    print('\n--- Testing the model ---')                                    # test model
    evaluate(model)

    print('\n--- Predicting image from test set ---')
    image_idx = 40                                                          # index of image to predict
    predict(model, image_idx, X_test, test_labels, classes)

    print('\n--- Plotting weight distributions ---')
    plot_weights(model)

    print('\n--- Saving the model ---')                                     # save model
#     model.save('model.h5')
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-6-f4dbdd22d4e8>[0m in [0;36m<cell line: 102>[0;34m()[0m
[1;32m    172[0m     [0mprint[0m[0;34m([0m[0;34m'\n--- Predicting image from test set ---'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    173[0m     [0mimage_idx[0m [0;34m=[0m [0;36m40[0m                                                          [0;31m# index of image to predict[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 174[0;31m     [0mpredict[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mimage_idx[0m[0;34m,[0m [0mX_test[0m[0;34m,[0m [0mtest_labels[0m[0;34m,[0m [0mclasses[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    175[0m [0;34m[0m[0m
[1;32m    176[0m     [0mprint[0m[0;34m([0m[0;34m'\n--- Plotting weight distributions ---'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m<ipython-input-6-f4dbdd22d4e8>[0m in [0;36mpredict[0;34m(model, image_idx, test_images, test_labels, classes)[0m
[1;32m     90[0m     [0mimage[0m [0;34m=[0m [0mimage[0m[0;34m.[0m[0mreshape[0m[0;34m([0m[0;34m([0m[0;36m1[0m[0;34m,[0m [0mimage[0m[0;34m.[0m[0mshape[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m,[0m [0mimage[0m[0;34m.[0m[0mshape[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mimage[0m[0;34m.[0m[0mshape[0m[0;34m[[0m[0;36m2[0m[0;34m][0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     91[0m     [0mpred[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0margmax[0m[0;34m([0m[0mmodel[0m[0;34m.[0m[0mpredict[0m[0;34m([0m[0mimage[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 92[0;31m     [0mplot_sample[0m[0;34m([0m[0mtest_images[0m[0;34m[[0m[0mimage_idx[0m[0;34m][0m[0;34m,[0m [0mclasses[0m[0;34m[[0m[0mtest_labels[0m[0;34m[[0m[0mimage_idx[0m[0;34m][0m[0;34m.[0m[0mitem[0m[0;34m([0m[0;34m)[0m[0;34m][0m[0;34m,[0m [0mclasses[0m[0;34m[[0m[0mpred[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     93[0m [0;34m[0m[0m
[1;32m     94[0m [0;34m[0m[0m

[0;31mValueError[0m: can only convert an array of size 1 to a Python scalar
ValueError: can only convert an array of size 1 to a Python scalar
",ValueError,can only convert an array of size 1 to a Python scalar,FALSE,no fix
torch_8,73d9fc7d-49ff-3a19-a77e-86cd37e29978,runtimeerror,"Given groups=1, weight of size [40, 3, 3, 3], expected input[64, 196, 196, 3] to have 3 channels, but got 196 channels instead","['---------------------------------------------------------------------------', 'RuntimeError                              Traceback (most recent call last)', '/tmp/ipykernel_24/1699604028.py in <module>\n      3 \n      4 model_ft, train_acc_history, val_acc_history = train_model(\n----> 5     model_ft, train_loader, test_loader, criterion, optimizer, num_epochs=3\n      6 )\n', '/tmp/ipykernel_24/2233534305.py in train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs)\n     27             with torch.set_grad_enabled(True):\n     28                 with torch.cuda.amp.autocast():\n---> 29                     outputs = model(inputs)\n     30                     loss = criterion(outputs, labels)\n     31 \n', '/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1188         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1189                 or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1190             return forward_call(*input, **kwargs)\n   1191         # Do not call functions when jit is used\n   1192         full_backward_hooks, non_full_backward_hooks = [], []\n', '/opt/conda/lib/python3.7/site-packages/timm/models/efficientnet.py in forward(self, x)\n    555 \n    556     def forward(self, x):\n--> 557         x = self.forward_features(x)\n    558         x = self.forward_head(x)\n    559         return x\n', '/opt/conda/lib/python3.7/site-packages/timm/models/efficientnet.py in forward_features(self, x)\n    538 \n    539     def forward_features(self, x):\n--> 540         x = self.conv_stem(x)\n    541         x = self.bn1(x)\n    542         if self.grad_checkpointing and not torch.jit.is_scripting():\n', '/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1188         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1189                 or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1190             return forward_call(*input, **kwargs)\n   1191         # Do not call functions when jit is used\n   1192         full_backward_hooks, non_full_backward_hooks = [], []\n', '/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py in forward(self, input)\n    461 \n    462     def forward(self, input: Tensor) -> Tensor:\n--> 463         return self._conv_forward(input, self.weight, self.bias)\n    464 \n    465 class Conv3d(_ConvNd):\n', '/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight, bias)\n    458                             _pair(0), self.dilation, self.groups)\n    459         return F.conv2d(input, weight, bias, self.stride,\n--> 460                         self.padding, self.dilation, self.groups)\n    461 \n    462     def forward(self, input: Tensor) -> Tensor:\n', 'RuntimeError: Given groups=1, weight of size [40, 3, 3, 3], expected input[64, 196, 196, 3] to have 3 channels, but got 196 channels instead']",/junobench_env/torch_8/torch_8_extension.ipynb,2026-01-14T17:44:54.522637,CellExecutionError,"An error occurred while executing the following cell:
------------------
model_ft = model_ft.cuda()
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mRuntimeError[0m                              Traceback (most recent call last)
[0;32m<ipython-input-22-bc652b219146>[0m in [0;36m<cell line: 1>[0;34m()[0m
[0;32m----> 1[0;31m [0mmodel_ft[0m [0;34m=[0m [0mmodel_ft[0m[0;34m.[0m[0mcuda[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py[0m in [0;36mcuda[0;34m(self, device)[0m
[1;32m    914[0m             [0mModule[0m[0;34m:[0m [0mself[0m[0;34m[0m[0;34m[0m[0m
[1;32m    915[0m         """"""
[0;32m--> 916[0;31m         [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_apply[0m[0;34m([0m[0;32mlambda[0m [0mt[0m[0;34m:[0m [0mt[0m[0;34m.[0m[0mcuda[0m[0;34m([0m[0mdevice[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    917[0m [0;34m[0m[0m
[1;32m    918[0m     [0;32mdef[0m [0mipu[0m[0;34m([0m[0mself[0m[0;34m:[0m [0mT[0m[0;34m,[0m [0mdevice[0m[0;34m:[0m [0mOptional[0m[0;34m[[0m[0mUnion[0m[0;34m[[0m[0mint[0m[0;34m,[0m [0mdevice[0m[0;34m][0m[0;34m][0m [0;34m=[0m [0;32mNone[0m[0;34m)[0m [0;34m->[0m [0mT[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py[0m in [0;36m_apply[0;34m(self, fn, recurse)[0m
[1;32m    778[0m         [0;32mif[0m [0mrecurse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    779[0m             [0;32mfor[0m [0mmodule[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mchildren[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 780[0;31m                 [0mmodule[0m[0;34m.[0m[0m_apply[0m[0;34m([0m[0mfn[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    781[0m [0;34m[0m[0m
[1;32m    782[0m         [0;32mdef[0m [0mcompute_should_use_set_data[0m[0;34m([0m[0mtensor[0m[0;34m,[0m [0mtensor_applied[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py[0m in [0;36m_apply[0;34m(self, fn, recurse)[0m
[1;32m    803[0m             [0;31m# `with torch.no_grad():`[0m[0;34m[0m[0;34m[0m[0m
[1;32m    804[0m             [0;32mwith[0m [0mtorch[0m[0;34m.[0m[0mno_grad[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 805[0;31m                 [0mparam_applied[0m [0;34m=[0m [0mfn[0m[0;34m([0m[0mparam[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    806[0m             [0mp_should_use_set_data[0m [0;34m=[0m [0mcompute_should_use_set_data[0m[0;34m([0m[0mparam[0m[0;34m,[0m [0mparam_applied[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    807[0m [0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py[0m in [0;36m<lambda>[0;34m(t)[0m
[1;32m    914[0m             [0mModule[0m[0;34m:[0m [0mself[0m[0;34m[0m[0;34m[0m[0m
[1;32m    915[0m         """"""
[0;32m--> 916[0;31m         [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_apply[0m[0;34m([0m[0;32mlambda[0m [0mt[0m[0;34m:[0m [0mt[0m[0;34m.[0m[0mcuda[0m[0;34m([0m[0mdevice[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    917[0m [0;34m[0m[0m
[1;32m    918[0m     [0;32mdef[0m [0mipu[0m[0;34m([0m[0mself[0m[0;34m:[0m [0mT[0m[0;34m,[0m [0mdevice[0m[0;34m:[0m [0mOptional[0m[0;34m[[0m[0mUnion[0m[0;34m[[0m[0mint[0m[0;34m,[0m [0mdevice[0m[0;34m][0m[0;34m][0m [0;34m=[0m [0;32mNone[0m[0;34m)[0m [0;34m->[0m [0mT[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py[0m in [0;36m_lazy_init[0;34m()[0m
[1;32m    312[0m         [0;32mif[0m [0;34m""CUDA_MODULE_LOADING""[0m [0;32mnot[0m [0;32min[0m [0mos[0m[0;34m.[0m[0menviron[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    313[0m             [0mos[0m[0;34m.[0m[0menviron[0m[0;34m[[0m[0;34m""CUDA_MODULE_LOADING""[0m[0;34m][0m [0;34m=[0m [0;34m""LAZY""[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 314[0;31m         [0mtorch[0m[0;34m.[0m[0m_C[0m[0;34m.[0m[0m_cuda_init[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    315[0m         [0;31m# Some of the queued calls may reentrantly call _lazy_init();[0m[0;34m[0m[0;34m[0m[0m
[1;32m    316[0m         [0;31m# we need to just return without initializing in that case.[0m[0;34m[0m[0;34m[0m[0m

[0;31mRuntimeError[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
",RuntimeError,Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx,FALSE,no fix
numpy_9,1d03d99d-faf5-3477-b4a4-875c09c93683,valueerror,zero-dimensional arrays cannot be concatenated,"['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', ""Cell In[14], line 20\n     17 regression_weights_directory = '/kaggle/input/adjusted-survival-2019'\n     19 # Load initial regression loss weights from the directory\n---> 20 regression_weight = load_loss_weights_from_directory(regression_weights_directory)\n     22 # Training loop for regression task\n     23 num_epochs_update_regression = 5  # Number of epochs to update the regression weight\n"", 'Cell In[14], line 9, in load_loss_weights_from_directory(directory_path)\n      7 weight_files = [filename for filename in os.listdir(directory_path) if filename.endswith("".npy"")]\n      8 weights = [np.load(os.path.join(directory_path, filename)) for filename in weight_files]\n----> 9 return np.concatenate(weights)\n', 'File <__array_function__ internals>:180, in concatenate(*args, **kwargs)\n', 'ValueError: zero-dimensional arrays cannot be concatenated']",/junobench_env/numpy_9/numpy_9_extension.ipynb,2026-01-14T15:59:41.776840,CellExecutionError,"An error occurred while executing the following cell:
------------------
##### OLD model for finding survival in days ( regression) based ############

# -*- coding: utf-8 -*-
""""""
Created on Wed Jan 19 10:30:33 2022

@author: MIDL
""""""
################## keras data generator ###########################

from sklearn.model_selection import train_test_split
import os
import tensorflow as tf
import numpy as np

# lists of directories with studies
# train_and_val_directories = [f.path for f in os.scandir('C:/Users/marya/Downloads/Brats 2020 adjusted') if f.is_dir()]
case_path1 = '../input/combine-all-2109'
case_path2 = '../input/adjustedmask2019'  
case_path3 = '../input/adjusted-survival-2019'
case_path4 = '../input/adjustedlabels2019'


# case_main = 'C:/Users/MIDL/Downloads/3d_model_december/Brats 2020 adjusted'

train_directory1 = [f.path for f in os.scandir(case_path1) ]
# train_directory2 = [f.path for f in os.scandir(case_path2) ]

def pathListIntoIds(dirList):
    x = []
    for i in range(0,len(dirList)):
#         print(dirList[i][dirList[i].rfind('/')+1:])
        x.append(dirList[i][dirList[i].rfind('\\')+1:]) #for local system
        # x.append(dirList[i][dirList[i].rfind('/')+1:]) #for KAGGLE
    return x

train_and_test_ids1 = pathListIntoIds(train_directory1); 

    
train_test_ids, val_ids = train_test_split(train_and_test_ids1,test_size=0.1) 
train_ids, test_ids = train_test_split(train_test_ids,test_size=0.22) 


# train_and_test_ids2 = pathListIntoIds(train_directory2);

# masks_test_ids, masks_val_ids = train_test_split(train_and_test_ids,test_size=0.3) 
#train_ids, test_ids = train_test_split(train_test_ids,test_size=0.5) 


------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m<ipython-input-1-9232de8f7400>[0m in [0;36m<cell line: 26>[0;34m()[0m
[1;32m     24[0m [0;31m# case_main = 'C:/Users/MIDL/Downloads/3d_model_december/Brats 2020 adjusted'[0m[0;34m[0m[0;34m[0m[0m
[1;32m     25[0m [0;34m[0m[0m
[0;32m---> 26[0;31m [0mtrain_directory1[0m [0;34m=[0m [0;34m[[0m[0mf[0m[0;34m.[0m[0mpath[0m [0;32mfor[0m [0mf[0m [0;32min[0m [0mos[0m[0;34m.[0m[0mscandir[0m[0;34m([0m[0mcase_path1[0m[0;34m)[0m [0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     27[0m [0;31m# train_directory2 = [f.path for f in os.scandir(case_path2) ][0m[0;34m[0m[0;34m[0m[0m
[1;32m     28[0m [0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: '../input/combine-all-2109'
FileNotFoundError: [Errno 2] No such file or directory: '../input/combine-all-2109'
",FileNotFoundError,[Errno 2] No such file or directory: '../input/combine-all-2109',FALSE,fixed the eror
torch_11,105af724-52f4-3631-832d-7b0de11d044b,runtimeerror,"stack expects each tensor to be equal size, but got [19] at entry 0 and [21] at entry 1","---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-24-db70380867c1> in <cell line: 1>()
----> 1 train(st_train,2)

<ipython-input-20-e67e75c40a2c> in train(traindata, epochs)
      8         bloss = []
      9         numb = 0
---> 10         for idx ,(cx, cy)  in enumerate(datal):
     11             optimizer.zero_grad()
     12 #             print(cx.shape,cy.shape)

/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py in __next__(self)
    628                 # TODO(https://github.com/pytorch/pytorch/issues/76750)
    629                 self._reset()  # type: ignore[call-arg]
--> 630             data = self._next_data()
    631             self._num_yielded += 1
    632             if self._dataset_kind == _DatasetKind.Iterable and \

/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py in _next_data(self)
    671     def _next_data(self):
    672         index = self._next_index()  # may raise StopIteration
--> 673         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    674         if self._pin_memory:
    675             data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)

/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
     53         else:
     54             data = self.dataset[possibly_batched_index]
---> 55         return self.collate_fn(data)

/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py in default_collate(batch)
    315         >>> default_collate(batch)  # Handle `CustomType` automatically
    316     """"""
--> 317     return collate(batch, collate_fn_map=default_collate_fn_map)

/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py in collate(batch, collate_fn_map)
    172 
    173         if isinstance(elem, tuple):
--> 174             return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
    175         else:
    176             try:

/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py in <listcomp>(.0)
    172 
    173         if isinstance(elem, tuple):
--> 174             return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
    175         else:
    176             try:

/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py in collate(batch, collate_fn_map)
    140     if collate_fn_map is not None:
    141         if elem_type in collate_fn_map:
--> 142             return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
    143 
    144         for collate_type in collate_fn_map:

/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py in collate_tensor_fn(batch, collate_fn_map)
    212         storage = elem._typed_storage()._new_shared(numel, device=elem.device)
    213         out = elem.new(storage).resize_(len(batch), *list(elem.size()))
--> 214     return torch.stack(batch, 0, out=out)
    215 
    216 

RuntimeError: stack expects each tensor to be equal size, but got [19] at entry 0 and [21] at entry 1",/junobench_env/torch_11/torch_11_extension.ipynb,2026-01-14T15:58:51.012140,CellExecutionError,"An error occurred while executing the following cell:
------------------
tokens , max_len = tokenize(sentences)
vocab , int2text , text2int = build_vocab(tokens)
X,Y = build_input(tokens , word2index,text2int)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mKeyError[0m                                  Traceback (most recent call last)
[0;32m<ipython-input-11-0a12095c959f>[0m in [0;36m<cell line: 3>[0;34m()[0m
[1;32m      1[0m [0mtokens[0m [0;34m,[0m [0mmax_len[0m [0;34m=[0m [0mtokenize[0m[0;34m([0m[0msentences[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      2[0m [0mvocab[0m [0;34m,[0m [0mint2text[0m [0;34m,[0m [0mtext2int[0m [0;34m=[0m [0mbuild_vocab[0m[0;34m([0m[0mtokens[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 3[0;31m [0mX[0m[0;34m,[0m[0mY[0m [0;34m=[0m [0mbuild_input[0m[0;34m([0m[0mtokens[0m [0;34m,[0m [0mword2index[0m[0;34m,[0m[0mtext2int[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;32m<ipython-input-9-d3d8c91761e9>[0m in [0;36mbuild_input[0;34m(sentences, word2index, text2int)[0m
[1;32m     27[0m [0;34m[0m[0m
[1;32m     28[0m         [0;32mfor[0m [0mtoken[0m [0;32min[0m [0mtokens[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 29[0;31m             [0mcurx[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mword2index[0m[0;34m.[0m[0mget[0m[0;34m([0m[0mtoken[0m[0;34m,[0m [0mword2index[0m[0;34m[[0m[0;34m'_unk_'[0m[0;34m][0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     30[0m             [0mcury[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mtext2int[0m[0;34m.[0m[0mget[0m[0;34m([0m[0mtoken[0m[0;34m,[0m [0munk_idx[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     31[0m [0;34m[0m[0m

[0;31mKeyError[0m: '_unk_'
KeyError: '_unk_'
",KeyError,'_unk_',FALSE,no fix
sklearn_7,be156106-9540-36c1-8314-5e1815918005,valueerror,could not convert string to float: 'Male',"['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', '/tmp/ipykernel_28/2403320104.py in <module>\n     11 # Train the model\n     12 rf = RandomForestRegressor(n_estimators=100, random_state=42)\n---> 13 rf.fit(X_train, y_train)\n     14 \n     15 \n', '/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_forest.py in fit(self, X, y, sample_weight)\n    326             raise ValueError(""sparse multilabel-indicator for y is not supported."")\n    327         X, y = self._validate_data(\n--> 328             X, y, multi_output=True, accept_sparse=""csc"", dtype=DTYPE\n    329         )\n    330         if sample_weight is not None:\n', '/opt/conda/lib/python3.7/site-packages/sklearn/base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)\n    579                 y = check_array(y, **check_y_params)\n    580             else:\n--> 581                 X, y = check_X_y(X, y, **check_params)\n    582             out = X, y\n    583 \n', '/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\n    974         ensure_min_samples=ensure_min_samples,\n    975         ensure_min_features=ensure_min_features,\n--> 976         estimator=estimator,\n    977     )\n    978 \n', '/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\n    744                     array = array.astype(dtype, casting=""unsafe"", copy=False)\n    745                 else:\n--> 746                     array = np.asarray(array, order=order, dtype=dtype)\n    747             except ComplexWarning as complex_warning:\n    748                 raise ValueError(\n', '/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py in __array__(self, dtype)\n   1991 \n   1992     def __array__(self, dtype: NpDtype | None = None) -> np.ndarray:\n-> 1993         return np.asarray(self._values, dtype=dtype)\n   1994 \n   1995     def __array_wrap__(\n', ""ValueError: could not convert string to float: 'Male'""]",/junobench_env/sklearn_7/sklearn_7_extension.ipynb,2026-01-14T18:13:13.606233,CellExecutionError,"An error occurred while executing the following cell:
------------------
# Calculate average annual income by family size
family_income = data.groupby('Family Size')['Annual Income ($)'].mean().reset_index()

# Create column chart
plt.bar(family_income['Family Size'], family_income['Annual Income ($)'])
plt.title('Average Annual Income by Family Size')
plt.xlabel('Family Size')

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m<ipython-input-12-1b5341736ef7>[0m in [0;36m<cell line: 2>[0;34m()[0m
[1;32m      1[0m [0;31m# Calculate average annual income by family size[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 2[0;31m [0mfamily_income[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0mgroupby[0m[0;34m([0m[0;34m'Family Size'[0m[0;34m)[0m[0;34m[[0m[0;34m'Annual Income ($)'[0m[0;34m][0m[0;34m.[0m[0mmean[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mreset_index[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      3[0m [0;34m[0m[0m
[1;32m      4[0m [0;31m# Create column chart[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0mplt[0m[0;34m.[0m[0mbar[0m[0;34m([0m[0mfamily_income[0m[0;34m[[0m[0;34m'Family Size'[0m[0;34m][0m[0;34m,[0m [0mfamily_income[0m[0;34m[[0m[0;34m'Annual Income ($)'[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mNameError[0m: name 'data' is not defined
NameError: name 'data' is not defined
",NameError,name 'data' is not defined,FALSE,no fix
sklearn_8,c0f5d509-c3c3-3e26-b07f-056418c17830,valueerror,Classification metrics can't handle a mix of continuous-multioutput and multiclass targets,"['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', 'Cell In[243], line 4\n      1 # y_pred = RF.predict(X_val)\n      2 \n      3 # Calculating the accuracy of the model\n----> 4 accuracy = accuracy_score(x, y)\n      6 print(""Accuracy: "", accuracy)\n', 'File /opt/conda/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:192, in validate_params.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\n    187 validate_parameter_constraints(\n    188     parameter_constraints, params, caller_name=func.__qualname__\n    189 )\n    191 try:\n--> 192     return func(*args, **kwargs)\n    193 except InvalidParameterError as e:\n    194     # When the function is just a wrapper around an estimator, we allow\n    195     # the function to delegate validation to the estimator, but we replace\n    196     # the name of the estimator by the name of the function in the error\n    197     # message to avoid confusion.\n    198     msg = re.sub(\n    199         r""parameter of \\w+ must be"",\n    200         f""parameter of {func.__qualname__} must be"",\n    201         str(e),\n    202     )\n', 'File /opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:221, in accuracy_score(y_true, y_pred, normalize, sample_weight)\n    155 """"""Accuracy classification score.\n    156 \n    157 In multilabel classification, this function computes subset accuracy:\n   (...)\n    217 0.5\n    218 """"""\n    220 # Compute accuracy for each possible representation\n--> 221 y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n    222 check_consistent_length(y_true, y_pred, sample_weight)\n    223 if y_type.startswith(""multilabel""):\n', 'File /opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:95, in _check_targets(y_true, y_pred)\n     92     y_type = {""multiclass""}\n     94 if len(y_type) > 1:\n---> 95     raise ValueError(\n     96         ""Classification metrics can\'t handle a mix of {0} and {1} targets"".format(\n     97             type_true, type_pred\n     98         )\n     99     )\n    101 # We can\'t have more than one value on y_type => The set is no more needed\n    102 y_type = y_type.pop()\n', ""ValueError: Classification metrics can't handle a mix of continuous-multioutput and multiclass targets""]",/junobench_env/sklearn_8/sklearn_8_extension.ipynb,2026-01-14T18:17:27.234944,CellExecutionError,"An error occurred while executing the following cell:
------------------
# Create a DataFrame
test_predictions = RF.predict(
    insurance_test.drop('Id', axis=1)
)

submission = pd.DataFrame({
    'Id': insurance_test['Id'],
    'Response': test_predictions
})

# submission = pd.DataFrame({'Id': test_data['Id'], 'Response': predicted_classes})

# Save it as a CSV file
submission.to_csv('submission.csv', index=False)

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-26-51ccbf17abdc>[0m in [0;36m<cell line: 2>[0;34m()[0m
[1;32m      1[0m [0;31m# Create a DataFrame[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 2[0;31m test_predictions = RF.predict(
[0m[1;32m      3[0m     [0minsurance_test[0m[0;34m.[0m[0mdrop[0m[0;34m([0m[0;34m'Id'[0m[0;34m,[0m [0maxis[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m )
[1;32m      5[0m [0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py[0m in [0;36mpredict[0;34m(self, X)[0m
[1;32m    818[0m             [0mThe[0m [0mpredicted[0m [0mclasses[0m[0;34m.[0m[0;34m[0m[0;34m[0m[0m
[1;32m    819[0m         """"""
[0;32m--> 820[0;31m         [0mproba[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mpredict_proba[0m[0;34m([0m[0mX[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    821[0m [0;34m[0m[0m
[1;32m    822[0m         [0;32mif[0m [0mself[0m[0;34m.[0m[0mn_outputs_[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py[0m in [0;36mpredict_proba[0;34m(self, X)[0m
[1;32m    860[0m         [0mcheck_is_fitted[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    861[0m         [0;31m# Check data[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 862[0;31m         [0mX[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_validate_X_predict[0m[0;34m([0m[0mX[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    863[0m [0;34m[0m[0m
[1;32m    864[0m         [0;31m# Assign chunk of trees to jobs[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py[0m in [0;36m_validate_X_predict[0;34m(self, X)[0m
[1;32m    600[0m         Validate X whenever one tries to predict, apply, predict_proba.""""""
[1;32m    601[0m         [0mcheck_is_fitted[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 602[0;31m         [0mX[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_validate_data[0m[0;34m([0m[0mX[0m[0;34m,[0m [0mdtype[0m[0;34m=[0m[0mDTYPE[0m[0;34m,[0m [0maccept_sparse[0m[0;34m=[0m[0;34m""csr""[0m[0;34m,[0m [0mreset[0m[0;34m=[0m[0;32mFalse[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    603[0m         [0;32mif[0m [0missparse[0m[0;34m([0m[0mX[0m[0;34m)[0m [0;32mand[0m [0;34m([0m[0mX[0m[0;34m.[0m[0mindices[0m[0;34m.[0m[0mdtype[0m [0;34m!=[0m [0mnp[0m[0;34m.[0m[0mintc[0m [0;32mor[0m [0mX[0m[0;34m.[0m[0mindptr[0m[0;34m.[0m[0mdtype[0m [0;34m!=[0m [0mnp[0m[0;34m.[0m[0mintc[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    604[0m             [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m""No support for np.int64 index based sparse matrices""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py[0m in [0;36m_validate_data[0;34m(self, X, y, reset, validate_separately, **check_params)[0m
[1;32m    546[0m             [0mvalidated[0m[0;34m.[0m[0;34m[0m[0;34m[0m[0m
[1;32m    547[0m         """"""
[0;32m--> 548[0;31m         [0mself[0m[0;34m.[0m[0m_check_feature_names[0m[0;34m([0m[0mX[0m[0;34m,[0m [0mreset[0m[0;34m=[0m[0mreset[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    549[0m [0;34m[0m[0m
[1;32m    550[0m         [0;32mif[0m [0my[0m [0;32mis[0m [0;32mNone[0m [0;32mand[0m [0mself[0m[0;34m.[0m[0m_get_tags[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;34m""requires_y""[0m[0;34m][0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py[0m in [0;36m_check_feature_names[0;34m(self, X, reset)[0m
[1;32m    479[0m                 )
[1;32m    480[0m [0;34m[0m[0m
[0;32m--> 481[0;31m             [0;32mraise[0m [0mValueError[0m[0;34m([0m[0mmessage[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    482[0m [0;34m[0m[0m
[1;32m    483[0m     def _validate_data(

[0;31mValueError[0m: The feature names should match those that were passed during fit.
Feature names unseen at fit time:
- InsuredInfo_7

ValueError: The feature names should match those that were passed during fit.
Feature names unseen at fit time:
- InsuredInfo_7

",ValueError,The feature names should match those that were passed during fit.,FALSE,fixed the eror
pandas_2,3d4c9485-1096-3c59-92b5-6df10382af18,keyerror,"""['LotFrontage', 'Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'] not found in axis""","['---------------------------------------------------------------------------', 'KeyError                                  Traceback (most recent call last)', ""Cell In[35], line 12\n      9     num.append(x[a])\n     10 a=a+1\n---> 12 data.drop(['LotFrontage','Alley','FireplaceQu','PoolQC','Fence', 'MiscFeature'], axis=1, inplace=True)\n"", 'File /opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331, in deprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper(*args, **kwargs)\n    325 if len(args) > num_allow_args:\n    326     warnings.warn(\n    327         msg.format(arguments=_format_argument_list(allow_args)),\n    328         FutureWarning,\n    329         stacklevel=find_stack_level(),\n    330     )\n--> 331 return func(*args, **kwargs)\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:5399, in DataFrame.drop(self, labels, axis, index, columns, level, inplace, errors)\n   5251 @deprecate_nonkeyword_arguments(version=None, allowed_args=[""self"", ""labels""])\n   5252 def drop(  # type: ignore[override]\n   5253     self,\n   (...)\n   5260     errors: IgnoreRaise = ""raise"",\n   5261 ) -> DataFrame | None:\n   5262     """"""\n   5263     Drop specified labels from rows or columns.\n   5264 \n   (...)\n   5397             weight  1.0     0.8\n   5398     """"""\n-> 5399     return super().drop(\n   5400         labels=labels,\n   5401         axis=axis,\n   5402         index=index,\n   5403         columns=columns,\n   5404         level=level,\n   5405         inplace=inplace,\n   5406         errors=errors,\n   5407     )\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331, in deprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper(*args, **kwargs)\n    325 if len(args) > num_allow_args:\n    326     warnings.warn(\n    327         msg.format(arguments=_format_argument_list(allow_args)),\n    328         FutureWarning,\n    329         stacklevel=find_stack_level(),\n    330     )\n--> 331 return func(*args, **kwargs)\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:4505, in NDFrame.drop(self, labels, axis, index, columns, level, inplace, errors)\n   4503 for axis, labels in axes.items():\n   4504     if labels is not None:\n-> 4505         obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n   4507 if inplace:\n   4508     self._update_inplace(obj)\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:4546, in NDFrame._drop_axis(self, labels, axis, level, errors, only_slice)\n   4544         new_axis = axis.drop(labels, level=level, errors=errors)\n   4545     else:\n-> 4546         new_axis = axis.drop(labels, errors=errors)\n   4547     indexer = axis.get_indexer(new_axis)\n   4549 # Case for non-unique axis\n   4550 else:\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:6934, in Index.drop(self, labels, errors)\n   6932 if mask.any():\n   6933     if errors != ""ignore"":\n-> 6934         raise KeyError(f""{list(labels[mask])} not found in axis"")\n   6935     indexer = indexer[~mask]\n   6936 return self.delete(indexer)\n', 'KeyError: ""[\'LotFrontage\', \'Alley\', \'FireplaceQu\', \'PoolQC\', \'Fence\', \'MiscFeature\'] not found in axis""']",/junobench_env/pandas_2/pandas_2_extension.ipynb,2026-01-14T17:53:55.317348,CellExecutionError,"An error occurred while executing the following cell:
------------------
train_df.corr()
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-12-a7a80d20a44e>[0m in [0;36m<cell line: 1>[0;34m()[0m
[0;32m----> 1[0;31m [0mtrain_df[0m[0;34m.[0m[0mcorr[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py[0m in [0;36mcorr[0;34m(self, method, min_periods, numeric_only)[0m
[1;32m  10052[0m         [0mcols[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0mcolumns[0m[0;34m[0m[0;34m[0m[0m
[1;32m  10053[0m         [0midx[0m [0;34m=[0m [0mcols[0m[0;34m.[0m[0mcopy[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m> 10054[0;31m         [0mmat[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0mto_numpy[0m[0;34m([0m[0mdtype[0m[0;34m=[0m[0mfloat[0m[0;34m,[0m [0mna_value[0m[0;34m=[0m[0mnp[0m[0;34m.[0m[0mnan[0m[0;34m,[0m [0mcopy[0m[0;34m=[0m[0;32mFalse[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m  10055[0m [0;34m[0m[0m
[1;32m  10056[0m         [0;32mif[0m [0mmethod[0m [0;34m==[0m [0;34m""pearson""[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py[0m in [0;36mto_numpy[0;34m(self, dtype, copy, na_value)[0m
[1;32m   1836[0m         [0;32mif[0m [0mdtype[0m [0;32mis[0m [0;32mnot[0m [0;32mNone[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1837[0m             [0mdtype[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0mdtype[0m[0;34m([0m[0mdtype[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1838[0;31m         [0mresult[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_mgr[0m[0;34m.[0m[0mas_array[0m[0;34m([0m[0mdtype[0m[0;34m=[0m[0mdtype[0m[0;34m,[0m [0mcopy[0m[0;34m=[0m[0mcopy[0m[0;34m,[0m [0mna_value[0m[0;34m=[0m[0mna_value[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1839[0m         [0;32mif[0m [0mresult[0m[0;34m.[0m[0mdtype[0m [0;32mis[0m [0;32mnot[0m [0mdtype[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1840[0m             [0mresult[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0marray[0m[0;34m([0m[0mresult[0m[0;34m,[0m [0mdtype[0m[0;34m=[0m[0mdtype[0m[0;34m,[0m [0mcopy[0m[0;34m=[0m[0;32mFalse[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py[0m in [0;36mas_array[0;34m(self, dtype, copy, na_value)[0m
[1;32m   1730[0m                 [0marr[0m[0;34m.[0m[0mflags[0m[0;34m.[0m[0mwriteable[0m [0;34m=[0m [0;32mFalse[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1731[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1732[0;31m             [0marr[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_interleave[0m[0;34m([0m[0mdtype[0m[0;34m=[0m[0mdtype[0m[0;34m,[0m [0mna_value[0m[0;34m=[0m[0mna_value[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1733[0m             [0;31m# The underlying data was copied within _interleave, so no need[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1734[0m             [0;31m# to further copy if copy=True or setting na_value[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py[0m in [0;36m_interleave[0;34m(self, dtype, na_value)[0m
[1;32m   1792[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1793[0m                 [0marr[0m [0;34m=[0m [0mblk[0m[0;34m.[0m[0mget_values[0m[0;34m([0m[0mdtype[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1794[0;31m             [0mresult[0m[0;34m[[0m[0mrl[0m[0;34m.[0m[0mindexer[0m[0;34m][0m [0;34m=[0m [0marr[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1795[0m             [0mitemmask[0m[0;34m[[0m[0mrl[0m[0;34m.[0m[0mindexer[0m[0;34m][0m [0;34m=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1796[0m [0;34m[0m[0m

[0;31mValueError[0m: could not convert string to float: 'RL'
ValueError: could not convert string to float: 'RL'
",ValueError,could not convert string to float: 'RL',FALSE,fixed the eror
numpy_10,efef4f9c-d212-3462-bbd4-acaa7a5e4eba,valueerror,a must be 1-dimensional,"['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', ""Cell In[48], line 9\n      6 model = load_checkpoint('checkpoint.pth')\n      8 # Testing with a random image from the test set\n----> 9 test_image_path = np.random.choice(test_ds.imgs)[0]\n     10 display_image(test_image_path)\n     12 probs, classes = predict(test_image_path, model)\n"", 'File mtrand.pyx:911, in numpy.random.mtrand.RandomState.choice()\n', 'ValueError: a must be 1-dimensional']",/junobench_env/numpy_10/numpy_10_extension.ipynb,2026-01-14T18:15:33.714394,CellExecutionError,"An error occurred while executing the following cell:
------------------
# Imports here
%matplotlib inline
%config InlinBackend.figure_format = 'retina'
import numpy as np
import random
import torch
from torch import nn
from torch import optim
import torch.nn.functional as F
import ast
import torchvision.transforms as transforms
from torchvision import datasets, models, transforms
import torchvision.models as models
from torch.autograd import Variable
from collections import OrderedDict
from PIL import Image
import json
import time
import warnings
warnings.filterwarnings('ignore')
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m<ipython-input-4-7c53b0e71201>[0m in [0;36m<cell line: 11>[0;34m()[0m
[1;32m      9[0m [0;32mimport[0m [0mtorch[0m[0;34m.[0m[0mnn[0m[0;34m.[0m[0mfunctional[0m [0;32mas[0m [0mF[0m[0;34m[0m[0;34m[0m[0m
[1;32m     10[0m [0;32mimport[0m [0mast[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 11[0;31m [0;32mimport[0m [0mtorchvision[0m[0;34m.[0m[0mtransforms[0m [0;32mas[0m [0mtransforms[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     12[0m [0;32mfrom[0m [0mtorchvision[0m [0;32mimport[0m [0mdatasets[0m[0;34m,[0m [0mmodels[0m[0;34m,[0m [0mtransforms[0m[0;34m[0m[0;34m[0m[0m
[1;32m     13[0m [0;32mimport[0m [0mtorchvision[0m[0;34m.[0m[0mmodels[0m [0;32mas[0m [0mmodels[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py[0m in [0;36m<module>[0;34m[0m
[1;32m      8[0m [0;31m# .extensions) before entering _meta_registrations.[0m[0;34m[0m[0;34m[0m[0m
[1;32m      9[0m [0;32mfrom[0m [0;34m.[0m[0mextension[0m [0;32mimport[0m [0m_HAS_OPS[0m  [0;31m# usort:skip[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 10[0;31m [0;32mfrom[0m [0mtorchvision[0m [0;32mimport[0m [0m_meta_registrations[0m[0;34m,[0m [0mdatasets[0m[0;34m,[0m [0mio[0m[0;34m,[0m [0mmodels[0m[0;34m,[0m [0mops[0m[0;34m,[0m [0mtransforms[0m[0;34m,[0m [0mutils[0m  [0;31m# usort:skip[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     11[0m [0;34m[0m[0m
[1;32m     12[0m [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/_meta_registrations.py[0m in [0;36m<module>[0;34m[0m
[1;32m      2[0m [0;34m[0m[0m
[1;32m      3[0m [0;32mimport[0m [0mtorch[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 4[0;31m [0;32mimport[0m [0mtorch[0m[0;34m.[0m[0m_custom_ops[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      5[0m [0;32mimport[0m [0mtorch[0m[0;34m.[0m[0mlibrary[0m[0;34m[0m[0;34m[0m[0m
[1;32m      6[0m [0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'torch._custom_ops'
ModuleNotFoundError: No module named 'torch._custom_ops'
",ModuleNotFoundError,No module named 'torch._custom_ops',FALSE,no fix
pandas_3,0632671a-dd6f-30b9-93b8-e2a121561569,valueerror,"columns overlap but no suffix specified: Index(['name_count'], dtype='object')","['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', 'Cell In[94], line 1\n----> 1 train = procData(train, name_count, RescuerID)\n      2 test = procData(test, name_count, RescuerID)\n', 'Cell In[93], line 14, in procData(data, name_count, RescuerID)\n     12 data[""NameLen""] = data[""Name""].fillna("""").str.len()\n     13 data[""SinNombre""] = data[""Name""].str.lower().replace("" "", """") == ""nonameyet""\n---> 14 data = data.join(name_count, on=""Name"")\n     15 data[""name_count""] = data[""name_count""].fillna(0)\n     17 data = data.drop([""Name"", ""RescuerID"", ""Description""], axis=1)\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:9729, in DataFrame.join(self, other, on, how, lsuffix, rsuffix, sort, validate)\n   9566 def join(\n   9567     self,\n   9568     other: DataFrame | Series | Iterable[DataFrame | Series],\n   (...)\n   9574     validate: str | None = None,\n   9575 ) -> DataFrame:\n   9576     """"""\n   9577     Join columns of another DataFrame.\n   9578 \n   (...)\n   9727     5  K1  A5   B1\n   9728     """"""\n-> 9729     return self._join_compat(\n   9730         other,\n   9731         on=on,\n   9732         how=how,\n   9733         lsuffix=lsuffix,\n   9734         rsuffix=rsuffix,\n   9735         sort=sort,\n   9736         validate=validate,\n   9737     )\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:9768, in DataFrame._join_compat(self, other, on, how, lsuffix, rsuffix, sort, validate)\n   9758     if how == ""cross"":\n   9759         return merge(\n   9760             self,\n   9761             other,\n   (...)\n   9766             validate=validate,\n   9767         )\n-> 9768     return merge(\n   9769         self,\n   9770         other,\n   9771         left_on=on,\n   9772         how=how,\n   9773         left_index=on is None,\n   9774         right_index=True,\n   9775         suffixes=(lsuffix, rsuffix),\n   9776         sort=sort,\n   9777         validate=validate,\n   9778     )\n   9779 else:\n   9780     if on is not None:\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/reshape/merge.py:162, in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\n    131 @Substitution(""\\nleft : DataFrame or named Series"")\n    132 @Appender(_merge_doc, indents=0)\n    133 def merge(\n   (...)\n    146     validate: str | None = None,\n    147 ) -> DataFrame:\n    148     op = _MergeOperation(\n    149         left,\n    150         right,\n   (...)\n    160         validate=validate,\n    161     )\n--> 162     return op.get_result(copy=copy)\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/reshape/merge.py:811, in _MergeOperation.get_result(self, copy)\n    807     self.left, self.right = self._indicator_pre_merge(self.left, self.right)\n    809 join_index, left_indexer, right_indexer = self._get_join_info()\n--> 811 result = self._reindex_and_concat(\n    812     join_index, left_indexer, right_indexer, copy=copy\n    813 )\n    814 result = result.__finalize__(self, method=self._merge_type)\n    816 if self.indicator:\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/reshape/merge.py:763, in _MergeOperation._reindex_and_concat(self, join_index, left_indexer, right_indexer, copy)\n    760 left = self.left[:]\n    761 right = self.right[:]\n--> 763 llabels, rlabels = _items_overlap_with_suffix(\n    764     self.left._info_axis, self.right._info_axis, self.suffixes\n    765 )\n    767 if left_indexer is not None and not is_range_indexer(left_indexer, len(left)):\n    768     # Pinning the index here (and in the right code just below) is not\n    769     #  necessary, but makes the `.take` more performant if we have e.g.\n    770     #  a MultiIndex for left.index.\n    771     lmgr = left._mgr.reindex_indexer(\n    772         join_index,\n    773         left_indexer,\n   (...)\n    778         use_na_proxy=True,\n    779     )\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/reshape/merge.py:2604, in _items_overlap_with_suffix(left, right, suffixes)\n   2601 lsuffix, rsuffix = suffixes\n   2603 if not lsuffix and not rsuffix:\n-> 2604     raise ValueError(f""columns overlap but no suffix specified: {to_rename}"")\n   2606 def renamer(x, suffix):\n   2607     """"""\n   2608     Rename the left and right indices.\n   2609 \n   (...)\n   2620     x : renamed column name\n   2621     """"""\n', ""ValueError: columns overlap but no suffix specified: Index(['name_count'], dtype='object')""]",/junobench_env/pandas_3/pandas_3_extension.ipynb,2026-01-14T17:54:49.649873,CellExecutionError,"An error occurred while executing the following cell:
------------------
train = procData(train, name_count_series, rescuer_count_series, RescuerID_encoder)
test = procData(test, name_count_series, rescuer_count_series, RescuerID_encoder)
data = train.copy()
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mKeyError[0m                                  Traceback (most recent call last)
[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py[0m in [0;36m_encode[0;34m(values, uniques, check_unknown)[0m
[1;32m    223[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 224[0;31m             [0;32mreturn[0m [0m_map_to_integer[0m[0;34m([0m[0mvalues[0m[0;34m,[0m [0muniques[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    225[0m         [0;32mexcept[0m [0mKeyError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py[0m in [0;36m_map_to_integer[0;34m(values, uniques)[0m
[1;32m    163[0m     [0mtable[0m [0;34m=[0m [0m_nandict[0m[0;34m([0m[0;34m{[0m[0mval[0m[0;34m:[0m [0mi[0m [0;32mfor[0m [0mi[0m[0;34m,[0m [0mval[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0muniques[0m[0;34m)[0m[0;34m}[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 164[0;31m     [0;32mreturn[0m [0mnp[0m[0;34m.[0m[0marray[0m[0;34m([0m[0;34m[[0m[0mtable[0m[0;34m[[0m[0mv[0m[0;34m][0m [0;32mfor[0m [0mv[0m [0;32min[0m [0mvalues[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    165[0m [0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py[0m in [0;36m<listcomp>[0;34m(.0)[0m
[1;32m    163[0m     [0mtable[0m [0;34m=[0m [0m_nandict[0m[0;34m([0m[0;34m{[0m[0mval[0m[0;34m:[0m [0mi[0m [0;32mfor[0m [0mi[0m[0;34m,[0m [0mval[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0muniques[0m[0;34m)[0m[0;34m}[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 164[0;31m     [0;32mreturn[0m [0mnp[0m[0;34m.[0m[0marray[0m[0;34m([0m[0;34m[[0m[0mtable[0m[0;34m[[0m[0mv[0m[0;34m][0m [0;32mfor[0m [0mv[0m [0;32min[0m [0mvalues[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    165[0m [0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py[0m in [0;36m__missing__[0;34m(self, key)[0m
[1;32m    157[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0mnan_value[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 158[0;31m         [0;32mraise[0m [0mKeyError[0m[0;34m([0m[0mkey[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    159[0m [0;34m[0m[0m

[0;31mKeyError[0m: '2ece3b2573dcdcebd774e635dca15fd9'

During handling of the above exception, another exception occurred:

[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-4-ea39d1f24022>[0m in [0;36m<cell line: 2>[0;34m()[0m
[1;32m      1[0m [0mtrain[0m [0;34m=[0m [0mprocData[0m[0;34m([0m[0mtrain[0m[0;34m,[0m [0mname_count_series[0m[0;34m,[0m [0mrescuer_count_series[0m[0;34m,[0m [0mRescuerID_encoder[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 2[0;31m [0mtest[0m [0;34m=[0m [0mprocData[0m[0;34m([0m[0mtest[0m[0;34m,[0m [0mname_count_series[0m[0;34m,[0m [0mrescuer_count_series[0m[0;34m,[0m [0mRescuerID_encoder[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      3[0m [0mdata[0m [0;34m=[0m [0mtrain[0m[0;34m.[0m[0mcopy[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m<ipython-input-3-a3b1bbb10725>[0m in [0;36mprocData[0;34m(data, name_count_series, rescuer_count_series, RescuerID_encoder)[0m
[1;32m     30[0m [0;34m[0m[0m
[1;32m     31[0m     [0;31m# Encode RescuerID[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 32[0;31m     [0mdata[0m[0;34m[[0m[0;34m""RescuerID_encoded""[0m[0;34m][0m [0;34m=[0m [0mRescuerID_encoder[0m[0;34m.[0m[0mtransform[0m[0;34m([0m[0mdata[0m[0;34m[[0m[0;34m""RescuerID""[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     33[0m [0;34m[0m[0m
[1;32m     34[0m     [0;31m# Name-based features[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py[0m in [0;36mwrapped[0;34m(self, X, *args, **kwargs)[0m
[1;32m    138[0m     [0;34m@[0m[0mwraps[0m[0;34m([0m[0mf[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    139[0m     [0;32mdef[0m [0mwrapped[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mX[0m[0;34m,[0m [0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 140[0;31m         [0mdata_to_wrap[0m [0;34m=[0m [0mf[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mX[0m[0;34m,[0m [0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    141[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mdata_to_wrap[0m[0;34m,[0m [0mtuple[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    142[0m             [0;31m# only wrap the first output for cross decomposition[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py[0m in [0;36mtransform[0;34m(self, y)[0m
[1;32m    137[0m             [0;32mreturn[0m [0mnp[0m[0;34m.[0m[0marray[0m[0;34m([0m[0;34m[[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    138[0m [0;34m[0m[0m
[0;32m--> 139[0;31m         [0;32mreturn[0m [0m_encode[0m[0;34m([0m[0my[0m[0;34m,[0m [0muniques[0m[0;34m=[0m[0mself[0m[0;34m.[0m[0mclasses_[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    140[0m [0;34m[0m[0m
[1;32m    141[0m     [0;32mdef[0m [0minverse_transform[0m[0;34m([0m[0mself[0m[0;34m,[0m [0my[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py[0m in [0;36m_encode[0;34m(values, uniques, check_unknown)[0m
[1;32m    224[0m             [0;32mreturn[0m [0m_map_to_integer[0m[0;34m([0m[0mvalues[0m[0;34m,[0m [0muniques[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    225[0m         [0;32mexcept[0m [0mKeyError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 226[0;31m             [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34mf""y contains previously unseen labels: {str(e)}""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    227[0m     [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    228[0m         [0;32mif[0m [0mcheck_unknown[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;31mValueError[0m: y contains previously unseen labels: '2ece3b2573dcdcebd774e635dca15fd9'
ValueError: y contains previously unseen labels: '2ece3b2573dcdcebd774e635dca15fd9'
",ValueError,y contains previously unseen labels: '2ece3b2573dcdcebd774e635dca15fd9',FALSE,no fix
torch_12,27304d3d-9c0f-35f4-ae23-1b6737dc9f5a,typeerror,"ne() received an invalid combination of arguments - got (NoneType), but expected one of:","['---------------------------------------------------------------------------', 'TypeError                                 Traceback (most recent call last)', 'Cell In[7], line 25\n     18 # Generate response\n     19 with torch.no_grad():\n     20     output = model.generate(\n     21         input_ids,\n     22         max_length=input_ids.size(1) + 50,  # Adjust the additional tokens as needed\n     23         num_return_sequences=1,\n     24         pad_token_id=tokenizer.eos_token_id,\n---> 25         attention_mask=input_ids.ne(tokenizer.pad_token_id)\n     26     )\n     28 # Pad the generated sequence\n     29 padded_output = output[:, input_ids.size(1):]\n', ""TypeError: ne() received an invalid combination of arguments - got (NoneType), but expected one of:\n * (Tensor other)\n      didn't match because some of the arguments have invalid types: (!NoneType!)\n * (Number other)\n      didn't match because some of the arguments have invalid types: (!NoneType!)\n""]",/junobench_env/torch_12/torch_12_extension.ipynb,2026-01-14T17:54:59.223799,CellExecutionError,"An error occurred while executing the following cell:
------------------
#By Ranamalla Nithin Reddy https://www.kaggle.com/code/nithinreddy90/chatpgpt-prompts

import matplotlib.pyplot as plt

# Create histograms for prompt and action token counts
plt.figure(figsize=(10, 6))
plt.hist(df['instruction_tokens'], bins=20, alpha=0.5, label='Instruction Tokens')
plt.hist(df['output_tokens'], bins=20, alpha=0.5, label='Output Tokens')
plt.xlabel('Token Count')
plt.ylabel('Frequency')
plt.title('Token Count Distribution')
plt.legend()
plt.show()
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mKeyError[0m                                  Traceback (most recent call last)
[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py[0m in [0;36mget_loc[0;34m(self, key)[0m
[1;32m   3652[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 3653[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_engine[0m[0;34m.[0m[0mget_loc[0m[0;34m([0m[0mcasted_key[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   3654[0m         [0;32mexcept[0m [0mKeyError[0m [0;32mas[0m [0merr[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx[0m in [0;36mpandas._libs.index.IndexEngine.get_loc[0;34m()[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx[0m in [0;36mpandas._libs.index.IndexEngine.get_loc[0;34m()[0m

[0;32mpandas/_libs/hashtable_class_helper.pxi[0m in [0;36mpandas._libs.hashtable.PyObjectHashTable.get_item[0;34m()[0m

[0;32mpandas/_libs/hashtable_class_helper.pxi[0m in [0;36mpandas._libs.hashtable.PyObjectHashTable.get_item[0;34m()[0m

[0;31mKeyError[0m: 'instruction_tokens'

The above exception was the direct cause of the following exception:

[0;31mKeyError[0m                                  Traceback (most recent call last)
[0;32m<ipython-input-11-392fb836e3d8>[0m in [0;36m<cell line: 7>[0;34m()[0m
[1;32m      5[0m [0;31m# Create histograms for prompt and action token counts[0m[0;34m[0m[0;34m[0m[0m
[1;32m      6[0m [0mplt[0m[0;34m.[0m[0mfigure[0m[0;34m([0m[0mfigsize[0m[0;34m=[0m[0;34m([0m[0;36m10[0m[0;34m,[0m [0;36m6[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 7[0;31m [0mplt[0m[0;34m.[0m[0mhist[0m[0;34m([0m[0mdf[0m[0;34m[[0m[0;34m'instruction_tokens'[0m[0;34m][0m[0;34m,[0m [0mbins[0m[0;34m=[0m[0;36m20[0m[0;34m,[0m [0malpha[0m[0;34m=[0m[0;36m0.5[0m[0;34m,[0m [0mlabel[0m[0;34m=[0m[0;34m'Instruction Tokens'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      8[0m [0mplt[0m[0;34m.[0m[0mhist[0m[0;34m([0m[0mdf[0m[0;34m[[0m[0;34m'output_tokens'[0m[0;34m][0m[0;34m,[0m [0mbins[0m[0;34m=[0m[0;36m20[0m[0;34m,[0m [0malpha[0m[0;34m=[0m[0;36m0.5[0m[0;34m,[0m [0mlabel[0m[0;34m=[0m[0;34m'Output Tokens'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      9[0m [0mplt[0m[0;34m.[0m[0mxlabel[0m[0;34m([0m[0;34m'Token Count'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py[0m in [0;36m__getitem__[0;34m(self, key)[0m
[1;32m   3759[0m             [0;32mif[0m [0mself[0m[0;34m.[0m[0mcolumns[0m[0;34m.[0m[0mnlevels[0m [0;34m>[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   3760[0m                 [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_getitem_multilevel[0m[0;34m([0m[0mkey[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 3761[0;31m             [0mindexer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mcolumns[0m[0;34m.[0m[0mget_loc[0m[0;34m([0m[0mkey[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   3762[0m             [0;32mif[0m [0mis_integer[0m[0;34m([0m[0mindexer[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   3763[0m                 [0mindexer[0m [0;34m=[0m [0;34m[[0m[0mindexer[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py[0m in [0;36mget_loc[0;34m(self, key)[0m
[1;32m   3653[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_engine[0m[0;34m.[0m[0mget_loc[0m[0;34m([0m[0mcasted_key[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m   3654[0m         [0;32mexcept[0m [0mKeyError[0m [0;32mas[0m [0merr[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 3655[0;31m             [0;32mraise[0m [0mKeyError[0m[0;34m([0m[0mkey[0m[0;34m)[0m [0;32mfrom[0m [0merr[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   3656[0m         [0;32mexcept[0m [0mTypeError[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   3657[0m             [0;31m# If we have a listlike key, _check_indexing_error will raise[0m[0;34m[0m[0;34m[0m[0m

[0;31mKeyError[0m: 'instruction_tokens'
KeyError: 'instruction_tokens'
",KeyError,'instruction_tokens',FALSE,No fix
sklearn_9,9cce2078-6201-38a6-b6b6-18a2aa3638bd,valueerror,"Logistic Regression supports only penalties in ['l1', 'l2'], got elasticnet.","['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', ""<ipython-input-63-9903cc3cf9bb> in <module>()\n      1 #elasticnet\n      2 log_reg2 = LogisticRegression(max_iter=1000, solver='liblinear', penalty='elasticnet')\n----> 3 log_reg2.fit(X_train, y_train)\n      4 y_pred = log_reg2.predict(X_valid)\n      5 \n"", '/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py in fit(self, X, y, sample_weight)\n   1278                              ""positive; got (tol=%r)"" % self.tol)\n   1279 \n-> 1280         solver = _check_solver(self.solver, self.penalty, self.dual)\n   1281 \n   1282         if solver in [\'newton-cg\']:\n', '/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py in _check_solver(solver, penalty, dual)\n    441     if penalty not in all_penalties:\n    442         raise ValueError(""Logistic Regression supports only penalties in %s,""\n--> 443                          "" got %s."" % (all_penalties, penalty))\n    444 \n    445     if solver not in [\'liblinear\', \'saga\'] and penalty != \'l2\':\n', ""ValueError: Logistic Regression supports only penalties in ['l1', 'l2'], got elasticnet.""]",/junobench_env/sklearn_9/sklearn_9_extension.ipynb,2026-01-14T18:13:15.307055,CellExecutionError,"An error occurred while executing the following cell:
------------------
# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
print(os.listdir(""../input""))

# Any results you write to the current directory are saved as output.
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m<ipython-input-1-f341d1164f35>[0m in [0;36m<cell line: 5>[0;34m()[0m
[1;32m      3[0m [0;34m[0m[0m
[1;32m      4[0m [0;32mimport[0m [0mos[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 5[0;31m [0mprint[0m[0;34m([0m[0mos[0m[0;34m.[0m[0mlistdir[0m[0;34m([0m[0;34m""../input""[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m [0;31m# Any results you write to the current directory are saved as output.[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: '../input'
FileNotFoundError: [Errno 2] No such file or directory: '../input'
",FileNotFoundError,[Errno 2] No such file or directory: '../input',FALSE,fixed the eror
pandas_4,5706cd60-4b1c-347a-a5c1-0ce967ee0ec6,indexerror,single positional indexer is out-of-bounds,"['---------------------------------------------------------------------------', 'IndexError                                Traceback (most recent call last)', ""<ipython-input-28-f6e807894ad0> in <module>\n     11 plt.figure(0, figsize=(16,10))\n     12 for i in range(1,8):\n---> 13     face = data[data['emotion'] == i-1].iloc[0]\n     14     img = row2image(face)\n     15     plt.subplot(2,4,i)\n"", '/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py in __getitem__(self, key)\n   1476 \n   1477             maybe_callable = com._apply_if_callable(key, self.obj)\n-> 1478             return self._getitem_axis(maybe_callable, axis=axis)\n   1479 \n   1480     def _is_scalar_access(self, key):\n', '/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py in _getitem_axis(self, key, axis)\n   2100 \n   2101             # validate the location\n-> 2102             self._validate_integer(key, axis)\n   2103 \n   2104             return self._get_loc(key, axis=axis)\n', '/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py in _validate_integer(self, key, axis)\n   2007         l = len(ax)\n   2008         if key >= l or key < -l:\n-> 2009             raise IndexError(""single positional indexer is out-of-bounds"")\n   2010 \n   2011     def _getitem_tuple(self, tup):\n', 'IndexError: single positional indexer is out-of-bounds']",/junobench_env/pandas_4/pandas_4_extension.ipynb,2026-01-14T17:54:43.360465,CellExecutionError,"An error occurred while executing the following cell:
------------------
# barplot class distribution of train, val and test
emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

def setup_axe(axe,df,title):
    df['emotion'].value_counts(sort=False).plot(ax=axe, kind='bar', rot=0)
    axe.set_xticklabels(emotion_labels)
    axe.set_xlabel(""Emotions"")
    axe.set_ylabel(""Number"")
    axe.set_title(title)
    
    # set individual bar lables using above list
    for i in axe.patches:
        # get_x pulls left or right; get_height pushes up or down
        axe.text(i.get_x()-.05, i.get_height()+120, \
                str(round((i.get_height()), 2)), fontsize=14, color='dimgrey',
                    rotation=0)

   
fig, axes = plt.subplots(1,3, figsize=(20,8), sharey=True)
setup_axe(axes[0],data_train,'train')
setup_axe(axes[1],data_val,'validation')
setup_axe(axes[2],data_test,'test')
plt.show()
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-12-dc6ea77e11f3>[0m in [0;36m<cell line: 20>[0;34m()[0m
[1;32m     18[0m [0;34m[0m[0m
[1;32m     19[0m [0mfig[0m[0;34m,[0m [0maxes[0m [0;34m=[0m [0mplt[0m[0;34m.[0m[0msubplots[0m[0;34m([0m[0;36m1[0m[0;34m,[0m[0;36m3[0m[0;34m,[0m [0mfigsize[0m[0;34m=[0m[0;34m([0m[0;36m20[0m[0;34m,[0m[0;36m8[0m[0;34m)[0m[0;34m,[0m [0msharey[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 20[0;31m [0msetup_axe[0m[0;34m([0m[0maxes[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m,[0m[0mdata_train[0m[0;34m,[0m[0;34m'train'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     21[0m [0msetup_axe[0m[0;34m([0m[0maxes[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m[0mdata_val[0m[0;34m,[0m[0;34m'validation'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     22[0m [0msetup_axe[0m[0;34m([0m[0maxes[0m[0;34m[[0m[0;36m2[0m[0;34m][0m[0;34m,[0m[0mdata_test[0m[0;34m,[0m[0;34m'test'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m<ipython-input-12-dc6ea77e11f3>[0m in [0;36msetup_axe[0;34m(axe, df, title)[0m
[1;32m      4[0m [0;32mdef[0m [0msetup_axe[0m[0;34m([0m[0maxe[0m[0;34m,[0m[0mdf[0m[0;34m,[0m[0mtitle[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m     [0mdf[0m[0;34m[[0m[0;34m'emotion'[0m[0;34m][0m[0;34m.[0m[0mvalue_counts[0m[0;34m([0m[0msort[0m[0;34m=[0m[0;32mFalse[0m[0;34m)[0m[0;34m.[0m[0mplot[0m[0;34m([0m[0max[0m[0;34m=[0m[0maxe[0m[0;34m,[0m [0mkind[0m[0;34m=[0m[0;34m'bar'[0m[0;34m,[0m [0mrot[0m[0;34m=[0m[0;36m0[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 6[0;31m     [0maxe[0m[0;34m.[0m[0mset_xticklabels[0m[0;34m([0m[0memotion_labels[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      7[0m     [0maxe[0m[0;34m.[0m[0mset_xlabel[0m[0;34m([0m[0;34m""Emotions""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      8[0m     [0maxe[0m[0;34m.[0m[0mset_ylabel[0m[0;34m([0m[0;34m""Number""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py[0m in [0;36mwrapper[0;34m(self, *args, **kwargs)[0m
[1;32m     72[0m [0;34m[0m[0m
[1;32m     73[0m         [0;32mdef[0m [0mwrapper[0m[0;34m([0m[0mself[0m[0;34m,[0m [0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 74[0;31m             [0;32mreturn[0m [0mget_method[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     75[0m [0;34m[0m[0m
[1;32m     76[0m         [0mwrapper[0m[0;34m.[0m[0m__module__[0m [0;34m=[0m [0mowner[0m[0;34m.[0m[0m__module__[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/_api/deprecation.py[0m in [0;36mwrapper[0;34m(*args, **kwargs)[0m
[1;32m    295[0m                 f""for the old name will be dropped %(removal)s."")
[1;32m    296[0m             [0mkwargs[0m[0;34m[[0m[0mnew[0m[0;34m][0m [0;34m=[0m [0mkwargs[0m[0;34m.[0m[0mpop[0m[0;34m([0m[0mold[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 297[0;31m         [0;32mreturn[0m [0mfunc[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    298[0m [0;34m[0m[0m
[1;32m    299[0m     [0;31m# wrapper() must keep the same documented signature as func(): if we[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py[0m in [0;36mset_ticklabels[0;34m(self, labels, minor, fontdict, **kwargs)[0m
[1;32m   1967[0m             [0;31m# remove all tick labels, so only error for > 0 labels[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1968[0m             [0;32mif[0m [0mlen[0m[0;34m([0m[0mlocator[0m[0;34m.[0m[0mlocs[0m[0;34m)[0m [0;34m!=[0m [0mlen[0m[0;34m([0m[0mlabels[0m[0;34m)[0m [0;32mand[0m [0mlen[0m[0;34m([0m[0mlabels[0m[0;34m)[0m [0;34m!=[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1969[0;31m                 raise ValueError(
[0m[1;32m   1970[0m                     [0;34m""The number of FixedLocator locations""[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1971[0m                     [0;34mf"" ({len(locator.locs)}), usually from a call to""[0m[0;34m[0m[0;34m[0m[0m

[0;31mValueError[0m: The number of FixedLocator locations (6), usually from a call to set_ticks, does not match the number of labels (7).
ValueError: The number of FixedLocator locations (6), usually from a call to set_ticks, does not match the number of labels (7).
",ValueError,"The number of FixedLocator locations (6), usually from a call to set_ticks, does not match the number of labels (7).",FALSE,fixed the eror
pandas_5,641e0062-b0cd-3cf6-a379-f2dad08f87ce,valueerror,invalid literal for int() with base 10: '46.0',"['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', ""Cell In[104], line 4\n      1 # didn't work in original one also\n      3 for column in cat_feat:\n----> 4     x_train[column] = x_train[column].astype('int')\n      5     x_test[column] = x_test[column].astype('int')\n"", 'File /opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:6324, in NDFrame.astype(self, dtype, copy, errors)\n   6317     results = [\n   6318         self.iloc[:, i].astype(dtype, copy=copy)\n   6319         for i in range(len(self.columns))\n   6320     ]\n   6322 else:\n   6323     # else, only a single dtype is given\n-> 6324     new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n   6325     return self._constructor(new_data).__finalize__(self, method=""astype"")\n   6327 # GH 33113: handle empty frame or series\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/internals/managers.py:451, in BaseBlockManager.astype(self, dtype, copy, errors)\n    448 elif using_copy_on_write():\n    449     copy = False\n--> 451 return self.apply(\n    452     ""astype"",\n    453     dtype=dtype,\n    454     copy=copy,\n    455     errors=errors,\n    456     using_cow=using_copy_on_write(),\n    457 )\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/internals/managers.py:352, in BaseBlockManager.apply(self, f, align_keys, **kwargs)\n    350         applied = b.apply(f, **kwargs)\n    351     else:\n--> 352         applied = getattr(b, f)(**kwargs)\n    353     result_blocks = extend_blocks(applied, result_blocks)\n    355 out = type(self).from_blocks(result_blocks, self.axes)\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:511, in Block.astype(self, dtype, copy, errors, using_cow)\n    491 """"""\n    492 Coerce to the new dtype.\n    493 \n   (...)\n    507 Block\n    508 """"""\n    509 values = self.values\n--> 511 new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n    513 new_values = maybe_coerce_values(new_values)\n    515 refs = None\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:242, in astype_array_safe(values, dtype, copy, errors)\n    239     dtype = dtype.numpy_dtype\n    241 try:\n--> 242     new_values = astype_array(values, dtype, copy=copy)\n    243 except (ValueError, TypeError):\n    244     # e.g. _astype_nansafe can fail on object-dtype of strings\n    245     #  trying to convert to float\n    246     if errors == ""ignore"":\n', ""File /opt/conda/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:187, in astype_array(values, dtype, copy)\n    184     values = values.astype(dtype, copy=copy)\n    186 else:\n--> 187     values = _astype_nansafe(values, dtype, copy=copy)\n    189 # in pandas we don't store numpy str dtypes, so convert to object\n    190 if isinstance(dtype, np.dtype) and issubclass(values.dtype.type, str):\n"", ""File /opt/conda/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:138, in _astype_nansafe(arr, dtype, copy, skipna)\n    134     raise ValueError(msg)\n    136 if copy or is_object_dtype(arr.dtype) or is_object_dtype(dtype):\n    137     # Explicit copy, or required since NumPy can't view from / to object.\n--> 138     return arr.astype(dtype, copy=True)\n    140 return arr.astype(dtype, copy=copy)\n"", ""ValueError: invalid literal for int() with base 10: '46.0'""]",/junobench_env/pandas_5/pandas_5_extension.ipynb,2026-01-14T17:53:59.382128,CellExecutionError,"An error occurred while executing the following cell:
------------------
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

from subprocess import check_output
print(check_output([""ls"", ""../input""]).decode(""utf8""))

# Any results you write to the current directory are saved as output.
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mCalledProcessError[0m                        Traceback (most recent call last)
[0;32m<ipython-input-1-695361520314>[0m in [0;36m<cell line: 12>[0;34m()[0m
[1;32m     10[0m [0;34m[0m[0m
[1;32m     11[0m [0;32mfrom[0m [0msubprocess[0m [0;32mimport[0m [0mcheck_output[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 12[0;31m [0mprint[0m[0;34m([0m[0mcheck_output[0m[0;34m([0m[0;34m[[0m[0;34m""ls""[0m[0;34m,[0m [0;34m""../input""[0m[0;34m][0m[0;34m)[0m[0;34m.[0m[0mdecode[0m[0;34m([0m[0;34m""utf8""[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     13[0m [0;34m[0m[0m
[1;32m     14[0m [0;31m# Any results you write to the current directory are saved as output.[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/lib/python3.10/subprocess.py[0m in [0;36mcheck_output[0;34m(timeout, *popenargs, **kwargs)[0m
[1;32m    419[0m         [0mkwargs[0m[0;34m[[0m[0;34m'input'[0m[0;34m][0m [0;34m=[0m [0mempty[0m[0;34m[0m[0;34m[0m[0m
[1;32m    420[0m [0;34m[0m[0m
[0;32m--> 421[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
[0m[1;32m    422[0m                **kwargs).stdout
[1;32m    423[0m [0;34m[0m[0m

[0;32m/usr/lib/python3.10/subprocess.py[0m in [0;36mrun[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)[0m
[1;32m    524[0m         [0mretcode[0m [0;34m=[0m [0mprocess[0m[0;34m.[0m[0mpoll[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    525[0m         [0;32mif[0m [0mcheck[0m [0;32mand[0m [0mretcode[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 526[0;31m             raise CalledProcessError(retcode, process.args,
[0m[1;32m    527[0m                                      output=stdout, stderr=stderr)
[1;32m    528[0m     [0;32mreturn[0m [0mCompletedProcess[0m[0;34m([0m[0mprocess[0m[0;34m.[0m[0margs[0m[0;34m,[0m [0mretcode[0m[0;34m,[0m [0mstdout[0m[0;34m,[0m [0mstderr[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mCalledProcessError[0m: Command '['ls', '../input']' returned non-zero exit status 2.
CalledProcessError: Command '['ls', '../input']' returned non-zero exit status 2.
",CalledProcessError,"Command '['ls', '../input']' returned non-zero exit status 2.",FALSE,No fix
pandas_6,063245cc-f98f-3c7e-a4ff-c99205c50f96,valueerror,invalid literal for int() with base 10: 'Order ID',"['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', ""Cell In[10], line 1\n----> 1 df=df['Order ID'].astype('int')\n      2 df\n      3 #This Error is occuring due to some string values in Order ID Column\n"", 'File /opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:6240, in NDFrame.astype(self, dtype, copy, errors)\n   6233     results = [\n   6234         self.iloc[:, i].astype(dtype, copy=copy)\n   6235         for i in range(len(self.columns))\n   6236     ]\n   6238 else:\n   6239     # else, only a single dtype is given\n-> 6240     new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n   6241     return self._constructor(new_data).__finalize__(self, method=""astype"")\n   6243 # GH 33113: handle empty frame or series\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/internals/managers.py:448, in BaseBlockManager.astype(self, dtype, copy, errors)\n    447 def astype(self: T, dtype, copy: bool = False, errors: str = ""raise"") -> T:\n--> 448     return self.apply(""astype"", dtype=dtype, copy=copy, errors=errors)\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/internals/managers.py:352, in BaseBlockManager.apply(self, f, align_keys, ignore_failures, **kwargs)\n    350         applied = b.apply(f, **kwargs)\n    351     else:\n--> 352         applied = getattr(b, f)(**kwargs)\n    353 except (TypeError, NotImplementedError):\n    354     if not ignore_failures:\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:526, in Block.astype(self, dtype, copy, errors)\n    508 """"""\n    509 Coerce to the new dtype.\n    510 \n   (...)\n    522 Block\n    523 """"""\n    524 values = self.values\n--> 526 new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n    528 new_values = maybe_coerce_values(new_values)\n    529 newb = self.make_block(new_values)\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:299, in astype_array_safe(values, dtype, copy, errors)\n    296     return values.copy()\n    298 try:\n--> 299     new_values = astype_array(values, dtype, copy=copy)\n    300 except (ValueError, TypeError):\n    301     # e.g. astype_nansafe can fail on object-dtype of strings\n    302     #  trying to convert to float\n    303     if errors == ""ignore"":\n', ""File /opt/conda/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:230, in astype_array(values, dtype, copy)\n    227     values = values.astype(dtype, copy=copy)\n    229 else:\n--> 230     values = astype_nansafe(values, dtype, copy=copy)\n    232 # in pandas we don't store numpy str dtypes, so convert to object\n    233 if isinstance(dtype, np.dtype) and issubclass(values.dtype.type, str):\n"", ""File /opt/conda/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:170, in astype_nansafe(arr, dtype, copy, skipna)\n    166     raise ValueError(msg)\n    168 if copy or is_object_dtype(arr.dtype) or is_object_dtype(dtype):\n    169     # Explicit copy, or required since NumPy can't view from / to object.\n--> 170     return arr.astype(dtype, copy=True)\n    172 return arr.astype(dtype, copy=copy)\n"", ""ValueError: invalid literal for int() with base 10: 'Order ID'""]",/junobench_env/pandas_6/pandas_6_extension.ipynb,2026-01-14T15:43:56.449464,CellExecutionError,"An error occurred while executing the following cell:
------------------
df[['Order ID','Quantity Ordered']]=df[['Order ID','Quantity Ordered']].astype(int)
df['Price Each']=df['Price Each'].astype(float)
df['Order Date']=pd.to_datetime(df['Order Date'])
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mKeyError[0m                                  Traceback (most recent call last)
[0;32m<ipython-input-10-32da7d19cfe0>[0m in [0;36m<cell line: 1>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m[[0m[0;34m[[0m[0;34m'Order ID'[0m[0;34m,[0m[0;34m'Quantity Ordered'[0m[0;34m][0m[0;34m][0m[0;34m=[0m[0mdf[0m[0;34m[[0m[0;34m[[0m[0;34m'Order ID'[0m[0;34m,[0m[0;34m'Quantity Ordered'[0m[0;34m][0m[0;34m][0m[0;34m.[0m[0mastype[0m[0;34m([0m[0mint[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mdf[0m[0;34m[[0m[0;34m'Price Each'[0m[0;34m][0m[0;34m=[0m[0mdf[0m[0;34m[[0m[0;34m'Price Each'[0m[0;34m][0m[0;34m.[0m[0mastype[0m[0;34m([0m[0mfloat[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m [0mdf[0m[0;34m[[0m[0;34m'Order Date'[0m[0;34m][0m[0;34m=[0m[0mpd[0m[0;34m.[0m[0mto_datetime[0m[0;34m([0m[0mdf[0m[0;34m[[0m[0;34m'Order Date'[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py[0m in [0;36m__getitem__[0;34m(self, key)[0m
[1;32m   1070[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_get_rows_with_mask[0m[0;34m([0m[0mkey[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1071[0m [0;34m[0m[0m
[0;32m-> 1072[0;31m         [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_get_with[0m[0;34m([0m[0mkey[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1073[0m [0;34m[0m[0m
[1;32m   1074[0m     [0;32mdef[0m [0m_get_with[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mkey[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py[0m in [0;36m_get_with[0;34m(self, key)[0m
[1;32m   1111[0m [0;34m[0m[0m
[1;32m   1112[0m         [0;31m# handle the dup indexing case GH#4246[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1113[0;31m         [0;32mreturn[0m [0mself[0m[0;34m.[0m[0mloc[0m[0;34m[[0m[0mkey[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1114[0m [0;34m[0m[0m
[1;32m   1115[0m     [0;32mdef[0m [0m_get_values_tuple[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mkey[0m[0;34m:[0m [0mtuple[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py[0m in [0;36m__getitem__[0;34m(self, key)[0m
[1;32m   1151[0m [0;34m[0m[0m
[1;32m   1152[0m             [0mmaybe_callable[0m [0;34m=[0m [0mcom[0m[0;34m.[0m[0mapply_if_callable[0m[0;34m([0m[0mkey[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mobj[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1153[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_getitem_axis[0m[0;34m([0m[0mmaybe_callable[0m[0;34m,[0m [0maxis[0m[0;34m=[0m[0maxis[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1154[0m [0;34m[0m[0m
[1;32m   1155[0m     [0;32mdef[0m [0m_is_scalar_access[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mkey[0m[0;34m:[0m [0mtuple[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py[0m in [0;36m_getitem_axis[0;34m(self, key, axis)[0m
[1;32m   1380[0m                     [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m""Cannot index with multidimensional key""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1381[0m [0;34m[0m[0m
[0;32m-> 1382[0;31m                 [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_getitem_iterable[0m[0;34m([0m[0mkey[0m[0;34m,[0m [0maxis[0m[0;34m=[0m[0maxis[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1383[0m [0;34m[0m[0m
[1;32m   1384[0m             [0;31m# nested tuple slicing[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py[0m in [0;36m_getitem_iterable[0;34m(self, key, axis)[0m
[1;32m   1320[0m [0;34m[0m[0m
[1;32m   1321[0m         [0;31m# A collection of keys[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1322[0;31m         [0mkeyarr[0m[0;34m,[0m [0mindexer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_get_listlike_indexer[0m[0;34m([0m[0mkey[0m[0;34m,[0m [0maxis[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1323[0m         return self.obj._reindex_with_indexers(
[1;32m   1324[0m             [0;34m{[0m[0maxis[0m[0;34m:[0m [0;34m[[0m[0mkeyarr[0m[0;34m,[0m [0mindexer[0m[0;34m][0m[0;34m}[0m[0;34m,[0m [0mcopy[0m[0;34m=[0m[0;32mTrue[0m[0;34m,[0m [0mallow_dups[0m[0;34m=[0m[0;32mTrue[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py[0m in [0;36m_get_listlike_indexer[0;34m(self, key, axis)[0m
[1;32m   1518[0m         [0maxis_name[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mobj[0m[0;34m.[0m[0m_get_axis_name[0m[0;34m([0m[0maxis[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1519[0m [0;34m[0m[0m
[0;32m-> 1520[0;31m         [0mkeyarr[0m[0;34m,[0m [0mindexer[0m [0;34m=[0m [0max[0m[0;34m.[0m[0m_get_indexer_strict[0m[0;34m([0m[0mkey[0m[0;34m,[0m [0maxis_name[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1521[0m [0;34m[0m[0m
[1;32m   1522[0m         [0;32mreturn[0m [0mkeyarr[0m[0;34m,[0m [0mindexer[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py[0m in [0;36m_get_indexer_strict[0;34m(self, key, axis_name)[0m
[1;32m   6113[0m             [0mkeyarr[0m[0;34m,[0m [0mindexer[0m[0;34m,[0m [0mnew_indexer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_reindex_non_unique[0m[0;34m([0m[0mkeyarr[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m   6114[0m [0;34m[0m[0m
[0;32m-> 6115[0;31m         [0mself[0m[0;34m.[0m[0m_raise_if_missing[0m[0;34m([0m[0mkeyarr[0m[0;34m,[0m [0mindexer[0m[0;34m,[0m [0maxis_name[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   6116[0m [0;34m[0m[0m
[1;32m   6117[0m         [0mkeyarr[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mtake[0m[0;34m([0m[0mindexer[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py[0m in [0;36m_raise_if_missing[0;34m(self, key, indexer, axis_name)[0m
[1;32m   6174[0m                 [0;32mif[0m [0muse_interval_msg[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   6175[0m                     [0mkey[0m [0;34m=[0m [0mlist[0m[0;34m([0m[0mkey[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 6176[0;31m                 [0;32mraise[0m [0mKeyError[0m[0;34m([0m[0;34mf""None of [{key}] are in the [{axis_name}]""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   6177[0m [0;34m[0m[0m
[1;32m   6178[0m             [0mnot_found[0m [0;34m=[0m [0mlist[0m[0;34m([0m[0mensure_index[0m[0;34m([0m[0mkey[0m[0;34m)[0m[0;34m[[0m[0mmissing_mask[0m[0;34m.[0m[0mnonzero[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m][0m[0;34m.[0m[0munique[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mKeyError[0m: ""None of [Index(['Order ID', 'Quantity Ordered'], dtype='object')] are in the [index]""
KeyError: ""None of [Index(['Order ID', 'Quantity Ordered'], dtype='object')] are in the [index]""
",KeyError,"None of [Index(['Order ID', 'Quantity Ordered'], dtype='object')] are in the [index]",FALSE,fixed the error
sklearn_10,7a0b6a76-224c-3df5-b99f-2b252b90117f,keyerror,"""None of [Int64Index([      0,       1,       2,       3,       4,       5,       6,\n                  8,       9,      11,\n            ...\n            1064640, 1064642, 1064643, 1064644, 1064645, 1064646, 1064647,\n            1064648, 1064650, 1064651],\n           dtype='int64', length=958186)] are in the [columns]""","['---------------------------------------------------------------------------', 'KeyError                                  Traceback (most recent call last)', '<ipython-input-105-43075d4b01ce> in <module>\n     11 for fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=n_splits, shuffle=True).split(X_train,y_train)):\n     12     # Fetch the train-validation indices.\n---> 13     X_train, y_train = X_train[train_idx], y_train[train_idx]\n     14     X_valid, y_valid = X_train[valid_idx], y_train[valid_idx]\n     15 \n', '/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key)\n   2984             if is_iterator(key):\n   2985                 key = list(key)\n-> 2986             indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)\n   2987 \n   2988         # take() does not accept boolean indexers\n', '/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py in _convert_to_indexer(self, obj, axis, is_setter, raise_missing)\n   1283                 # When setting, missing keys are not allowed, even with .loc:\n   1284                 kwargs = {""raise_missing"": True if is_setter else raise_missing}\n-> 1285                 return self._get_listlike_indexer(obj, axis, **kwargs)[1]\n   1286         else:\n   1287             try:\n', '/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py in _get_listlike_indexer(self, key, axis, raise_missing)\n   1090 \n   1091         self._validate_read_indexer(\n-> 1092             keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing\n   1093         )\n   1094         return keyarr, indexer\n', '/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing)\n   1175                 raise KeyError(\n   1176                     ""None of [{key}] are in the [{axis}]"".format(\n-> 1177                         key=key, axis=self.obj._get_axis_name(axis)\n   1178                     )\n   1179                 )\n', 'KeyError: ""None of [Int64Index([      0,       1,       2,       3,       4,       5,       6,\\n                  8,       9,      11,\\n            ...\\n            1064640, 1064642, 1064643, 1064644, 1064645, 1064646, 1064647,\\n            1064648, 1064650, 1064651],\\n           dtype=\'int64\', length=958186)] are in the [columns]""']",/junobench_env/sklearn_10/sklearn_10_extension.ipynb,2026-01-14T15:56:35.444530,CellExecutionError,"An error occurred while executing the following cell:
------------------
#correlation map
f,ax=plt.subplots(figsize=(12,12))
corr=df.corr()

sns.heatmap(corr, annot=True, linewidths=.5, fmt='.2f', 
            mask= np.zeros_like(corr,dtype=np.bool), 
            cmap=sns.diverging_palette(100,200,as_cmap=True), 
            square=True, ax=ax)

plt.show()
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mAttributeError[0m                            Traceback (most recent call last)
[0;32m<ipython-input-10-1ad1a7ea7f49>[0m in [0;36m<cell line: 5>[0;34m()[0m
[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m sns.heatmap(corr, annot=True, linewidths=.5, fmt='.2f', 
[0;32m----> 6[0;31m             [0mmask[0m[0;34m=[0m [0mnp[0m[0;34m.[0m[0mzeros_like[0m[0;34m([0m[0mcorr[0m[0;34m,[0m[0mdtype[0m[0;34m=[0m[0mnp[0m[0;34m.[0m[0mbool[0m[0;34m)[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      7[0m             [0mcmap[0m[0;34m=[0m[0msns[0m[0;34m.[0m[0mdiverging_palette[0m[0;34m([0m[0;36m100[0m[0;34m,[0m[0;36m200[0m[0;34m,[0m[0mas_cmap[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m      8[0m             square=True, ax=ax)

[0;32m/usr/local/lib/python3.10/dist-packages/numpy/__init__.py[0m in [0;36m__getattr__[0;34m(attr)[0m
[1;32m    322[0m [0;34m[0m[0m
[1;32m    323[0m         [0;32mif[0m [0mattr[0m [0;32min[0m [0m__former_attrs__[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 324[0;31m             [0;32mraise[0m [0mAttributeError[0m[0;34m([0m[0m__former_attrs__[0m[0;34m[[0m[0mattr[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    325[0m [0;34m[0m[0m
[1;32m    326[0m         [0;32mif[0m [0mattr[0m [0;34m==[0m [0;34m'testing'[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;31mAttributeError[0m: module 'numpy' has no attribute 'bool'.
`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
AttributeError: module 'numpy' has no attribute 'bool'.
`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
",AttributeError,module 'numpy' has no attribute 'bool'.,FALSE,no fix
pandas_7,1a123373-bbcd-36b9-ae19-8c46d0fcd622,valueerror,invalid literal for int() with base 10: 'nan',"['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', ""<ipython-input-7-4958b6271f98> in <module>\n----> 1 game_df['released'] = game_df['released'].apply(lambda x: str(x).split('-')[0]).astype('int')\n"", '/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py in astype(self, dtype, copy, errors, **kwargs)\n   5880             # else, only a single dtype is given\n   5881             new_data = self._data.astype(\n-> 5882                 dtype=dtype, copy=copy, errors=errors, **kwargs\n   5883             )\n   5884             return self._constructor(new_data).__finalize__(self)\n', '/opt/conda/lib/python3.6/site-packages/pandas/core/internals/managers.py in astype(self, dtype, **kwargs)\n    579 \n    580     def astype(self, dtype, **kwargs):\n--> 581         return self.apply(""astype"", dtype=dtype, **kwargs)\n    582 \n    583     def convert(self, **kwargs):\n', '/opt/conda/lib/python3.6/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\n    436                     kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy)\n    437 \n--> 438             applied = getattr(b, f)(**kwargs)\n    439             result_blocks = _extend_blocks(applied, result_blocks)\n    440 \n', '/opt/conda/lib/python3.6/site-packages/pandas/core/internals/blocks.py in astype(self, dtype, copy, errors, values, **kwargs)\n    557 \n    558     def astype(self, dtype, copy=False, errors=""raise"", values=None, **kwargs):\n--> 559         return self._astype(dtype, copy=copy, errors=errors, values=values, **kwargs)\n    560 \n    561     def _astype(self, dtype, copy=False, errors=""raise"", values=None, **kwargs):\n', '/opt/conda/lib/python3.6/site-packages/pandas/core/internals/blocks.py in _astype(self, dtype, copy, errors, values, **kwargs)\n    641                     # _astype_nansafe works fine with 1-d only\n    642                     vals1d = values.ravel()\n--> 643                     values = astype_nansafe(vals1d, dtype, copy=True, **kwargs)\n    644 \n    645                 # TODO(extension)\n', '/opt/conda/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna)\n    705         # work around NumPy brokenness, #1987\n    706         if np.issubdtype(dtype.type, np.integer):\n--> 707             return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape)\n    708 \n    709         # if we have a datetime/timedelta array of objects\n', 'pandas/_libs/lib.pyx in pandas._libs.lib.astype_intsafe()\n', ""ValueError: invalid literal for int() with base 10: 'nan'""]",/junobench_env/pandas_7/pandas_7_extension.ipynb,2026-01-14T15:59:21.456867,CellExecutionError,"An error occurred while executing the following cell:
------------------
from plotly.subplots import make_subplots
import plotly.graph_objects as go

# Create subplots with shared x-axis and subplot titles
fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=('Pearson Correlation', 'Spearman Correlation'))

# Compute Pearson correlation
pearson_corr = game_df.corr('pearson')
pearson_rows = pearson_corr.index
pearson_cols = pearson_corr.columns
pearson_vals = pearson_corr.values

# Add Pearson correlation heatmap with information cards
fig.add_trace(go.Heatmap(x=pearson_cols, y=pearson_rows, z=pearson_vals, name='Pearson', showscale=False, xgap=1, ygap=1, colorscale='Viridis'),
              row=1, col=1)

# Compute Spearman correlation
spearman_corr = game_df.corr('spearman')
spearman_rows = spearman_corr.index
spearman_cols = spearman_corr.columns
spearman_vals = spearman_corr.values

# Add Spearman correlation heatmap with information cards
fig.add_trace(go.Heatmap(x=spearman_cols, y=spearman_rows, z=spearman_vals, xgap=1, ygap=1, colorscale='Viridis'),
              row=2, col=1)

fig.update_layout(hoverlabel=dict(bgcolor=""white"", font_size=16, font_family=""Rockwell""))
fig.update_layout(height=700, width=900, title_text=""Correlations"")
fig.show()

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-6-9a8a2b7d8bec>[0m in [0;36m<cell line: 8>[0;34m()[0m
[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m [0;31m# Compute Pearson correlation[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 8[0;31m [0mpearson_corr[0m [0;34m=[0m [0mgame_df[0m[0;34m.[0m[0mcorr[0m[0;34m([0m[0;34m'pearson'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      9[0m [0mpearson_rows[0m [0;34m=[0m [0mpearson_corr[0m[0;34m.[0m[0mindex[0m[0;34m[0m[0;34m[0m[0m
[1;32m     10[0m [0mpearson_cols[0m [0;34m=[0m [0mpearson_corr[0m[0;34m.[0m[0mcolumns[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py[0m in [0;36mcorr[0;34m(self, method, min_periods, numeric_only)[0m
[1;32m  10702[0m         [0mcols[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0mcolumns[0m[0;34m[0m[0;34m[0m[0m
[1;32m  10703[0m         [0midx[0m [0;34m=[0m [0mcols[0m[0;34m.[0m[0mcopy[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m> 10704[0;31m         [0mmat[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0mto_numpy[0m[0;34m([0m[0mdtype[0m[0;34m=[0m[0mfloat[0m[0;34m,[0m [0mna_value[0m[0;34m=[0m[0mnp[0m[0;34m.[0m[0mnan[0m[0;34m,[0m [0mcopy[0m[0;34m=[0m[0;32mFalse[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m  10705[0m [0;34m[0m[0m
[1;32m  10706[0m         [0;32mif[0m [0mmethod[0m [0;34m==[0m [0;34m""pearson""[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py[0m in [0;36mto_numpy[0;34m(self, dtype, copy, na_value)[0m
[1;32m   1887[0m         [0;32mif[0m [0mdtype[0m [0;32mis[0m [0;32mnot[0m [0;32mNone[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1888[0m             [0mdtype[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0mdtype[0m[0;34m([0m[0mdtype[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1889[0;31m         [0mresult[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_mgr[0m[0;34m.[0m[0mas_array[0m[0;34m([0m[0mdtype[0m[0;34m=[0m[0mdtype[0m[0;34m,[0m [0mcopy[0m[0;34m=[0m[0mcopy[0m[0;34m,[0m [0mna_value[0m[0;34m=[0m[0mna_value[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1890[0m         [0;32mif[0m [0mresult[0m[0;34m.[0m[0mdtype[0m [0;32mis[0m [0;32mnot[0m [0mdtype[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1891[0m             [0mresult[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0marray[0m[0;34m([0m[0mresult[0m[0;34m,[0m [0mdtype[0m[0;34m=[0m[0mdtype[0m[0;34m,[0m [0mcopy[0m[0;34m=[0m[0;32mFalse[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py[0m in [0;36mas_array[0;34m(self, dtype, copy, na_value)[0m
[1;32m   1654[0m                 [0marr[0m[0;34m.[0m[0mflags[0m[0;34m.[0m[0mwriteable[0m [0;34m=[0m [0;32mFalse[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1655[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1656[0;31m             [0marr[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_interleave[0m[0;34m([0m[0mdtype[0m[0;34m=[0m[0mdtype[0m[0;34m,[0m [0mna_value[0m[0;34m=[0m[0mna_value[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1657[0m             [0;31m# The underlying data was copied within _interleave, so no need[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1658[0m             [0;31m# to further copy if copy=True or setting na_value[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py[0m in [0;36m_interleave[0;34m(self, dtype, na_value)[0m
[1;32m   1713[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1714[0m                 [0marr[0m [0;34m=[0m [0mblk[0m[0;34m.[0m[0mget_values[0m[0;34m([0m[0mdtype[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1715[0;31m             [0mresult[0m[0;34m[[0m[0mrl[0m[0;34m.[0m[0mindexer[0m[0;34m][0m [0;34m=[0m [0marr[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1716[0m             [0mitemmask[0m[0;34m[[0m[0mrl[0m[0;34m.[0m[0mindexer[0m[0;34m][0m [0;34m=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1717[0m [0;34m[0m[0m

[0;31mValueError[0m: could not convert string to float: 'D/Generation HD'
ValueError: could not convert string to float: 'D/Generation HD'
",ValueError,could not convert string to float: 'D/Generation HD',FALSE,no fix
sklearn_11,2bcbc897-9ee4-368a-a498-7bd42a69b05b,valueerror,"With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.","['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', '/tmp/ipykernel_27/1111092727.py in <module>\n----> 1 x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n      2 \n      3 lr = LinearRegression()\n      4 lr.fit(x_train, y_train)\n      5 y_pred = lr.predict(x_test)\n', '/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py in train_test_split(test_size, train_size, random_state, shuffle, stratify, *arrays)\n   2419     n_samples = _num_samples(arrays[0])\n   2420     n_train, n_test = _validate_shuffle_split(\n-> 2421         n_samples, test_size, train_size, default_test_size=0.25\n   2422     )\n   2423 \n', '/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py in _validate_shuffle_split(n_samples, test_size, train_size, default_test_size)\n   2099             ""With n_samples={}, test_size={} and train_size={}, the ""\n   2100             ""resulting train set will be empty. Adjust any of the ""\n-> 2101             ""aforementioned parameters."".format(n_samples, test_size, train_size)\n   2102         )\n   2103 \n', 'ValueError: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.']",/junobench_env/sklearn_11/sklearn_11_extension.ipynb,2026-01-14T15:59:04.362724,CellExecutionError,"An error occurred while executing the following cell:
------------------

plt.scatter(x,y)
plt.xlabel(""number_people"")
plt.ylabel('temperature')
plt.show()

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m<ipython-input-6-320c3223192f>[0m in [0;36m<cell line: 1>[0;34m()[0m
[0;32m----> 1[0;31m [0mplt[0m[0;34m.[0m[0mscatter[0m[0;34m([0m[0mx[0m[0;34m,[0m[0my[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mplt[0m[0;34m.[0m[0mxlabel[0m[0;34m([0m[0;34m""number_people""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m [0mplt[0m[0;34m.[0m[0mylabel[0m[0;34m([0m[0;34m'temperature'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0mplt[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mNameError[0m: name 'x' is not defined
NameError: name 'x' is not defined
",NameError,name 'x' is not defined,FALSE,no fix
torch_14,4e16c8a8-3972-3dc5-a091-d9b1bfddbeef,runtimeerror,Caught RuntimeError in DataLoader worker process 0.,"['---------------------------------------------------------------------------', 'RuntimeError                              Traceback (most recent call last)', 'Cell In[2], line 36\n     34 num_epochs = 10\n     35 for epoch in range(num_epochs):\n---> 36     for i, data in enumerate(trainloader, 0):\n     37         inputs, labels = data\n     38         optimizer.zero_grad()\n', 'File /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634, in _BaseDataLoaderIter.__next__(self)\n    631 if self._sampler_iter is None:\n    632     # TODO(https://github.com/pytorch/pytorch/issues/76750)\n    633     self._reset()  # type: ignore[call-arg]\n--> 634 data = self._next_data()\n    635 self._num_yielded += 1\n    636 if self._dataset_kind == _DatasetKind.Iterable and \\\n    637         self._IterableDataset_len_called is not None and \\\n    638         self._num_yielded > self._IterableDataset_len_called:\n', 'File /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346, in _MultiProcessingDataLoaderIter._next_data(self)\n   1344 else:\n   1345     del self._task_info[idx]\n-> 1346     return self._process_data(data)\n', 'File /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372, in _MultiProcessingDataLoaderIter._process_data(self, data)\n   1370 self._try_put_index()\n   1371 if isinstance(data, ExceptionWrapper):\n-> 1372     data.reraise()\n   1373 return data\n', ""File /opt/conda/lib/python3.10/site-packages/torch/_utils.py:644, in ExceptionWrapper.reraise(self)\n    640 except TypeError:\n    641     # If the exception takes multiple arguments, don't try to\n    642     # instantiate since we don't know how to\n    643     raise RuntimeError(msg) from None\n--> 644 raise exception\n"", 'RuntimeError: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py"", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py"", line 54, in fetch\n    return self.collate_fn(data)\n  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py"", line 264, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py"", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py"", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py"", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py"", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [3, 112, 500] at entry 0 and [3, 375, 500] at entry 1\n']",/junobench_env/torch_14/torch_14_extension.ipynb,2026-01-14T17:54:17.665652,CellExecutionError,"An error occurred while executing the following cell:
------------------
# import torch
# import torchvision
# from torchvision.models.detection import fasterrcnn_resnet50_fpn
# from torchvision.models.detection.rpn import AnchorGenerator
# from torchvision.transforms import transforms
# from torch.utils.data import DataLoader

# # ÂÆö‰πâËΩ¨Êç¢
# transform = transforms.Compose([
#     transforms.ToTensor(), 
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
# ])

# # Âä†ËΩΩÊï∞ÊçÆÈõÜ
# trainset = torchvision.datasets.VOCDetection(root=""data_small"", year='2012', image_set='train', download=False, transform=transform)
# testset = torchvision.datasets.VOCDetection(root=""data_small"", year='2012', image_set='val', download=False, transform=transform)

# # ÂàõÂª∫Êï∞ÊçÆÂä†ËΩΩÂô®
# trainloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)
# testloader = DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)

import torch
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights

from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.transforms import transforms
from torch.utils.data import DataLoader

# Define a custom collate function for object detection
def collate_fn(batch):
    images = [item[0] for item in batch]
    targets = [item[1] for item in batch]
    return images, targets

# Define transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# Load datasets
trainset = torchvision.datasets.VOCDetection(
    root=""data_small"",
    year='2012',
    image_set='train',
    download=False,
    transform=transform
)

testset = torchvision.datasets.VOCDetection(
    root=""data_small"",
    year='2012',
    image_set='val',
    download=False,
    transform=transform
)

# Create data loaders with the custom collate_fn
trainloader = DataLoader(
    trainset,
    batch_size=4,
    shuffle=True,
    num_workers=2,
    collate_fn=collate_fn
)

testloader = DataLoader(
    testset,
    batch_size=4,
    shuffle=False,
    num_workers=2,
    collate_fn=collate_fn
)

# Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÁöÑÊ®°Âûã
# model = fasterrcnn_resnet50_fpn(pretrained=True)
model =fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)
# ÊõøÊç¢ÂàÜÁ±ªÂô®
num_classes = 21  # 20 Á±ª + ËÉåÊôØ
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)

# ÂÆö‰πâ‰ºòÂåñÂô®
optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)

# ËÆ≠ÁªÉÊ®°Âûã
num_epochs = 10
for epoch in range(num_epochs):
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = sum(loss for loss in outputs.values())
        loss.backward()
        optimizer.step()

        if i % 2000 == 1999:    # ÊØè 2000 mini-batches ÊâìÂç∞‰∏ÄÊ¨°
            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, loss.item()))

print('Finished Training')

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mAssertionError[0m                            Traceback (most recent call last)
[0;32m<ipython-input-2-058b7c1bd20c>[0m in [0;36m<cell line: 91>[0;34m()[0m
[1;32m     93[0m         [0minputs[0m[0;34m,[0m [0mlabels[0m [0;34m=[0m [0mdata[0m[0;34m[0m[0;34m[0m[0m
[1;32m     94[0m         [0moptimizer[0m[0;34m.[0m[0mzero_grad[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 95[0;31m         [0moutputs[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     96[0m         [0mloss[0m [0;34m=[0m [0msum[0m[0;34m([0m[0mloss[0m [0;32mfor[0m [0mloss[0m [0;32min[0m [0moutputs[0m[0;34m.[0m[0mvalues[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     97[0m         [0mloss[0m[0;34m.[0m[0mbackward[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py[0m in [0;36m_wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1551[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_compiled_call_impl[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m  [0;31m# type: ignore[misc][0m[0;34m[0m[0;34m[0m[0m
[1;32m   1552[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1553[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_call_impl[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1554[0m [0;34m[0m[0m
[1;32m   1555[0m     [0;32mdef[0m [0m_call_impl[0m[0;34m([0m[0mself[0m[0;34m,[0m [0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1560[0m                 [0;32mor[0m [0m_global_backward_pre_hooks[0m [0;32mor[0m [0m_global_backward_hooks[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1561[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
[0;32m-> 1562[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1563[0m [0;34m[0m[0m
[1;32m   1564[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/generalized_rcnn.py[0m in [0;36mforward[0;34m(self, images, targets)[0m
[1;32m     60[0m         [0;32mif[0m [0mself[0m[0;34m.[0m[0mtraining[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m     61[0m             [0;32mif[0m [0mtargets[0m [0;32mis[0m [0;32mNone[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 62[0;31m                 [0mtorch[0m[0;34m.[0m[0m_assert[0m[0;34m([0m[0;32mFalse[0m[0;34m,[0m [0;34m""targets should not be none when in training mode""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     63[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m     64[0m                 [0;32mfor[0m [0mtarget[0m [0;32min[0m [0mtargets[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py[0m in [0;36m_assert[0;34m(condition, message)[0m
[1;32m   1775[0m     [0;32mif[0m [0mtype[0m[0;34m([0m[0mcondition[0m[0;34m)[0m [0;32mis[0m [0;32mnot[0m [0mtorch[0m[0;34m.[0m[0mTensor[0m [0;32mand[0m [0mhas_torch_function[0m[0;34m([0m[0;34m([0m[0mcondition[0m[0;34m,[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1776[0m         [0;32mreturn[0m [0mhandle_torch_function[0m[0;34m([0m[0m_assert[0m[0;34m,[0m [0;34m([0m[0mcondition[0m[0;34m,[0m[0;34m)[0m[0;34m,[0m [0mcondition[0m[0;34m,[0m [0mmessage[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1777[0;31m     [0;32massert[0m [0mcondition[0m[0;34m,[0m [0mmessage[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1778[0m [0;34m[0m[0m
[1;32m   1779[0m [0;31m################################################################################[0m[0;34m[0m[0;34m[0m[0m

[0;31mAssertionError[0m: targets should not be none when in training mode
AssertionError: targets should not be none when in training mode
",AssertionError,targets should not be none when in training mode,FALSE,no fix
sklearn_12,96cbdadd-6fa0-32cb-b20b-96d9604ef6bd,attributeerror,SVC' object has no attribute 'feature_importances_',"['---------------------------------------------------------------------------', 'AttributeError                            Traceback (most recent call last)', 'Cell In[73], line 3\n      1 from pandas import Series\n----> 3 feature_importance = model.feature_importances_\n      4 Series_feat_imp = Series(feature_importance, index=data.columns)\n', ""AttributeError: 'SVC' object has no attribute 'feature_importances_'""]",/junobench_env/sklearn_12/sklearn_12_extension.ipynb,2026-01-14T17:54:24.033555,CellExecutionError,"An error occurred while executing the following cell:
------------------
plt.figure(figsize=(8,8))
Series_feat_imp.sort_values(ascending=True).plot.barh()
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.show()
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m<ipython-input-13-6eabcdf360b5>[0m in [0;36m<cell line: 1>[0;34m()[0m
[0;32m----> 1[0;31m [0mplt[0m[0;34m.[0m[0mfigure[0m[0;34m([0m[0mfigsize[0m[0;34m=[0m[0;34m([0m[0;36m8[0m[0;34m,[0m[0;36m8[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mSeries_feat_imp[0m[0;34m.[0m[0msort_values[0m[0;34m([0m[0mascending[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m[0;34m.[0m[0mplot[0m[0;34m.[0m[0mbarh[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m [0mplt[0m[0;34m.[0m[0mxlabel[0m[0;34m([0m[0;34m'Feature Importance'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0mplt[0m[0;34m.[0m[0mylabel[0m[0;34m([0m[0;34m'Feature'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0mplt[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mNameError[0m: name 'plt' is not defined
NameError: name 'plt' is not defined
",NameError,name 'plt' is not defined,FALSE,fixed the eror
pandas_12,0f745de3-80f1-31b2-b520-9bd25b490a1e,keyerror,"'Column(s) [1, 2, 3, 4] do not exist'","['---------------------------------------------------------------------------', 'KeyError                                  Traceback (most recent call last)', ""Cell In[35], line 1\n----> 1 df.groupby('kmeans_cluster').agg({1: ['count','mean', 'median', 'sum'],\n      2                                     2: ['count','mean', 'median', 'sum'],\n      3                                     3: ['count','mean', 'median', 'sum'],\n      4                                     4: ['count','mean','median', 'sum']})\n"", 'File /opt/conda/lib/python3.10/site-packages/pandas/core/groupby/generic.py:1269, in DataFrameGroupBy.aggregate(self, func, engine, engine_kwargs, *args, **kwargs)\n   1266 func = maybe_mangle_lambdas(func)\n   1268 op = GroupByApply(self, func, args, kwargs)\n-> 1269 result = op.agg()\n   1270 if not is_dict_like(func) and result is not None:\n   1271     return result\n', ""File /opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:163, in Apply.agg(self)\n    160     return self.apply_str()\n    162 if is_dict_like(arg):\n--> 163     return self.agg_dict_like()\n    164 elif is_list_like(arg):\n    165     # we require a list, but not a 'str'\n    166     return self.agg_list_like()\n"", 'File /opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:403, in Apply.agg_dict_like(self)\n    400     selected_obj = obj._selected_obj\n    401     selection = obj._selection\n--> 403 arg = self.normalize_dictlike_arg(""agg"", selected_obj, arg)\n    405 is_groupby = isinstance(obj, (DataFrameGroupBy, SeriesGroupBy))\n    406 context_manager: ContextManager\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:535, in Apply.normalize_dictlike_arg(self, how, obj, func)\n    533     if len(cols) > 0:\n    534         cols_sorted = list(safe_sort(list(cols)))\n--> 535         raise KeyError(f""Column(s) {cols_sorted} do not exist"")\n    537 aggregator_types = (list, tuple, dict)\n    539 # if we have a dict of any non-scalars\n    540 # eg. {\'A\' : [\'mean\']}, normalize all to\n    541 # be list-likes\n    542 # Cannot use func.values() because arg may be a Series\n', ""KeyError: 'Column(s) [1, 2, 3, 4] do not exist'""]",/junobench_env/pandas_12/pandas_12_extension.ipynb,2026-01-14T17:51:31.117010,CellExecutionError,"An error occurred while executing the following cell:
------------------
plt.figure(figsize = (16,8))
for i,x in enumerate(num_cols):
  plt.title(""Boxplot for {}"".format(x))
  plt.subplot(2,4,i+1)
  sns.boxplot(df[x])
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-10-ddd84ef43344>[0m in [0;36m<cell line: 2>[0;34m()[0m
[1;32m      2[0m [0;32mfor[0m [0mi[0m[0;34m,[0m[0mx[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mnum_cols[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m   [0mplt[0m[0;34m.[0m[0mtitle[0m[0;34m([0m[0;34m""Boxplot for {}""[0m[0;34m.[0m[0mformat[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 4[0;31m   [0mplt[0m[0;34m.[0m[0msubplot[0m[0;34m([0m[0;36m2[0m[0;34m,[0m[0;36m4[0m[0;34m,[0m[0mi[0m[0;34m+[0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      5[0m   [0msns[0m[0;34m.[0m[0mboxplot[0m[0;34m([0m[0mdf[0m[0;34m[[0m[0mx[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py[0m in [0;36msubplot[0;34m(*args, **kwargs)[0m
[1;32m   1321[0m [0;34m[0m[0m
[1;32m   1322[0m     [0;31m# First, search for an existing subplot with a matching spec.[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1323[0;31m     [0mkey[0m [0;34m=[0m [0mSubplotSpec[0m[0;34m.[0m[0m_from_subplot_args[0m[0;34m([0m[0mfig[0m[0;34m,[0m [0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1324[0m [0;34m[0m[0m
[1;32m   1325[0m     [0;32mfor[0m [0max[0m [0;32min[0m [0mfig[0m[0;34m.[0m[0maxes[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/gridspec.py[0m in [0;36m_from_subplot_args[0;34m(figure, args)[0m
[1;32m    598[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    599[0m             [0;32mif[0m [0;32mnot[0m [0misinstance[0m[0;34m([0m[0mnum[0m[0;34m,[0m [0mIntegral[0m[0;34m)[0m [0;32mor[0m [0mnum[0m [0;34m<[0m [0;36m1[0m [0;32mor[0m [0mnum[0m [0;34m>[0m [0mrows[0m[0;34m*[0m[0mcols[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 600[0;31m                 raise ValueError(
[0m[1;32m    601[0m                     [0;34mf""num must be an integer with 1 <= num <= {rows*cols}, ""[0m[0;34m[0m[0;34m[0m[0m
[1;32m    602[0m                     [0;34mf""not {num!r}""[0m[0;34m[0m[0;34m[0m[0m

[0;31mValueError[0m: num must be an integer with 1 <= num <= 8, not 9
ValueError: num must be an integer with 1 <= num <= 8, not 9
",ValueError,"num must be an integer with 1 <= num <= 8, not 9",FALSE,no fix
sklearn_13,155e67a6-4b0b-3429-91e6-f64cafdcc665,valueerror,could not convert string to float: ' Private',"['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', '/tmp/ipykernel_27/3371469880.py in <module>\n      2 \n      3 linear_reg = LinearRegression()\n----> 4 linear_reg.fit(x,y)\n', '/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py in fit(self, X, y, sample_weight)\n    661 \n    662         X, y = self._validate_data(\n--> 663             X, y, accept_sparse=accept_sparse, y_numeric=True, multi_output=True\n    664         )\n    665 \n', '/opt/conda/lib/python3.7/site-packages/sklearn/base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)\n    579                 y = check_array(y, **check_y_params)\n    580             else:\n--> 581                 X, y = check_X_y(X, y, **check_params)\n    582             out = X, y\n    583 \n', '/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\n    974         ensure_min_samples=ensure_min_samples,\n    975         ensure_min_features=ensure_min_features,\n--> 976         estimator=estimator,\n    977     )\n    978 \n', '/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\n    744                     array = array.astype(dtype, casting=""unsafe"", copy=False)\n    745                 else:\n--> 746                     array = np.asarray(array, order=order, dtype=dtype)\n    747             except ComplexWarning as complex_warning:\n    748                 raise ValueError(\n', ""ValueError: could not convert string to float: ' Private'""]",/junobench_env/sklearn_13/sklearn_13_extension.ipynb,2026-01-14T17:54:04.288638,CellExecutionError,"An error occurred while executing the following cell:
------------------
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# linear_reg = LinearRegression() 
# linear_reg.fit(x,y)

df_encoded = pd.get_dummies(df, columns=['workclass'], drop_first=True)

# Define features and target
X = df_encoded.drop('Salary', axis=1).values
y = df_encoded['Salary'].values.reshape(-1, 1)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Fit Linear Regression model
linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)

print(""Model fitted successfully!"")
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-9-375f657ccb3d>[0m in [0;36m<cell line: 21>[0;34m()[0m
[1;32m     19[0m [0;31m# Fit Linear Regression model[0m[0;34m[0m[0;34m[0m[0m
[1;32m     20[0m [0mlinear_reg[0m [0;34m=[0m [0mLinearRegression[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 21[0;31m [0mlinear_reg[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0mX_train[0m[0;34m,[0m [0my_train[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     22[0m [0;34m[0m[0m
[1;32m     23[0m [0mprint[0m[0;34m([0m[0;34m""Model fitted successfully!""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_base.py[0m in [0;36mfit[0;34m(self, X, y, sample_weight)[0m
[1;32m    646[0m         [0maccept_sparse[0m [0;34m=[0m [0;32mFalse[0m [0;32mif[0m [0mself[0m[0;34m.[0m[0mpositive[0m [0;32melse[0m [0;34m[[0m[0;34m""csr""[0m[0;34m,[0m [0;34m""csc""[0m[0;34m,[0m [0;34m""coo""[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[1;32m    647[0m [0;34m[0m[0m
[0;32m--> 648[0;31m         X, y = self._validate_data(
[0m[1;32m    649[0m             [0mX[0m[0;34m,[0m [0my[0m[0;34m,[0m [0maccept_sparse[0m[0;34m=[0m[0maccept_sparse[0m[0;34m,[0m [0my_numeric[0m[0;34m=[0m[0;32mTrue[0m[0;34m,[0m [0mmulti_output[0m[0;34m=[0m[0;32mTrue[0m[0;34m[0m[0;34m[0m[0m
[1;32m    650[0m         )

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py[0m in [0;36m_validate_data[0;34m(self, X, y, reset, validate_separately, **check_params)[0m
[1;32m    582[0m                 [0my[0m [0;34m=[0m [0mcheck_array[0m[0;34m([0m[0my[0m[0;34m,[0m [0minput_name[0m[0;34m=[0m[0;34m""y""[0m[0;34m,[0m [0;34m**[0m[0mcheck_y_params[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    583[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 584[0;31m                 [0mX[0m[0;34m,[0m [0my[0m [0;34m=[0m [0mcheck_X_y[0m[0;34m([0m[0mX[0m[0;34m,[0m [0my[0m[0;34m,[0m [0;34m**[0m[0mcheck_params[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    585[0m             [0mout[0m [0;34m=[0m [0mX[0m[0;34m,[0m [0my[0m[0;34m[0m[0;34m[0m[0m
[1;32m    586[0m [0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py[0m in [0;36mcheck_X_y[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)[0m
[1;32m   1104[0m         )
[1;32m   1105[0m [0;34m[0m[0m
[0;32m-> 1106[0;31m     X = check_array(
[0m[1;32m   1107[0m         [0mX[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1108[0m         [0maccept_sparse[0m[0;34m=[0m[0maccept_sparse[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py[0m in [0;36mcheck_array[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)[0m
[1;32m    877[0m                     [0marray[0m [0;34m=[0m [0mxp[0m[0;34m.[0m[0mastype[0m[0;34m([0m[0marray[0m[0;34m,[0m [0mdtype[0m[0;34m,[0m [0mcopy[0m[0;34m=[0m[0;32mFalse[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    878[0m                 [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 879[0;31m                     [0marray[0m [0;34m=[0m [0m_asarray_with_order[0m[0;34m([0m[0marray[0m[0;34m,[0m [0morder[0m[0;34m=[0m[0morder[0m[0;34m,[0m [0mdtype[0m[0;34m=[0m[0mdtype[0m[0;34m,[0m [0mxp[0m[0;34m=[0m[0mxp[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    880[0m             [0;32mexcept[0m [0mComplexWarning[0m [0;32mas[0m [0mcomplex_warning[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    881[0m                 raise ValueError(

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py[0m in [0;36m_asarray_with_order[0;34m(array, dtype, order, copy, xp)[0m
[1;32m    183[0m     [0;32mif[0m [0mxp[0m[0;34m.[0m[0m__name__[0m [0;32min[0m [0;34m{[0m[0;34m""numpy""[0m[0;34m,[0m [0;34m""numpy.array_api""[0m[0;34m}[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    184[0m         [0;31m# Use NumPy API to support order[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 185[0;31m         [0marray[0m [0;34m=[0m [0mnumpy[0m[0;34m.[0m[0masarray[0m[0;34m([0m[0marray[0m[0;34m,[0m [0morder[0m[0;34m=[0m[0morder[0m[0;34m,[0m [0mdtype[0m[0;34m=[0m[0mdtype[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    186[0m         [0;32mreturn[0m [0mxp[0m[0;34m.[0m[0masarray[0m[0;34m([0m[0marray[0m[0;34m,[0m [0mcopy[0m[0;34m=[0m[0mcopy[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    187[0m     [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;31mValueError[0m: could not convert string to float: ' HS-grad'
ValueError: could not convert string to float: ' HS-grad'
",ValueError,could not convert string to float: ' HS-grad',FALSE,no fix
numpy_12,918f02b2-4746-3c97-a709-6229558495c8,valueerror,"could not broadcast input array from shape (84,4) into shape (84,1)","['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', 'Cell In[23], line 33\n     31 trainPredictPlot = np.empty_like(dataset)\n     32 trainPredictPlot[:, :] = np.nan\n---> 33 trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n     34 # shift test predictions for plotting\n     35 testPredictPlot = np.empty_like(dataset)\n', 'ValueError: could not broadcast input array from shape (84,4) into shape (84,1)']",/junobench_env/numpy_12/numpy_12_extension.ipynb,2026-01-14T16:37:41.330320,CellExecutionError,"An error occurred while executing the following cell:
------------------
# make predictions
trainPredict = model.predict(trainX)
testPredict = model.predict(testX)
# invert predictions
trainPredict = scaler.inverse_transform(trainPredict)
trainY = scaler.inverse_transform(trainY.reshape(-1, 1))
testPredict = scaler.inverse_transform(testPredict)
testY = scaler.inverse_transform(testY.reshape(-1, 1))
# calculate root mean squared error
trainScore = np.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = np.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-9-9be0cf7ad6ef>[0m in [0;36m<cell line: 10>[0;34m()[0m
[1;32m      8[0m [0mtestY[0m [0;34m=[0m [0mscaler[0m[0;34m.[0m[0minverse_transform[0m[0;34m([0m[0mtestY[0m[0;34m.[0m[0mreshape[0m[0;34m([0m[0;34m-[0m[0;36m1[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      9[0m [0;31m# calculate root mean squared error[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 10[0;31m [0mtrainScore[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0msqrt[0m[0;34m([0m[0mmean_squared_error[0m[0;34m([0m[0mtrainY[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m,[0m [0mtrainPredict[0m[0;34m[[0m[0;34m:[0m[0;34m,[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     11[0m [0mprint[0m[0;34m([0m[0;34m'Train Score: %.2f RMSE'[0m [0;34m%[0m [0;34m([0m[0mtrainScore[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     12[0m [0mtestScore[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0msqrt[0m[0;34m([0m[0mmean_squared_error[0m[0;34m([0m[0mtestY[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m,[0m [0mtestPredict[0m[0;34m[[0m[0;34m:[0m[0;34m,[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py[0m in [0;36mmean_squared_error[0;34m(y_true, y_pred, sample_weight, multioutput, squared)[0m
[1;32m    440[0m     [0;36m0.825[0m[0;34m...[0m[0;34m[0m[0;34m[0m[0m
[1;32m    441[0m     """"""
[0;32m--> 442[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(
[0m[1;32m    443[0m         [0my_true[0m[0;34m,[0m [0my_pred[0m[0;34m,[0m [0mmultioutput[0m[0;34m[0m[0;34m[0m[0m
[1;32m    444[0m     )

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py[0m in [0;36m_check_reg_targets[0;34m(y_true, y_pred, multioutput, dtype)[0m
[1;32m     98[0m         [0mcorrect[0m [0mkeyword[0m[0;34m.[0m[0;34m[0m[0;34m[0m[0m
[1;32m     99[0m     """"""
[0;32m--> 100[0;31m     [0mcheck_consistent_length[0m[0;34m([0m[0my_true[0m[0;34m,[0m [0my_pred[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    101[0m     [0my_true[0m [0;34m=[0m [0mcheck_array[0m[0;34m([0m[0my_true[0m[0;34m,[0m [0mensure_2d[0m[0;34m=[0m[0;32mFalse[0m[0;34m,[0m [0mdtype[0m[0;34m=[0m[0mdtype[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    102[0m     [0my_pred[0m [0;34m=[0m [0mcheck_array[0m[0;34m([0m[0my_pred[0m[0;34m,[0m [0mensure_2d[0m[0;34m=[0m[0;32mFalse[0m[0;34m,[0m [0mdtype[0m[0;34m=[0m[0mdtype[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py[0m in [0;36mcheck_consistent_length[0;34m(*arrays)[0m
[1;32m    395[0m     [0muniques[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0munique[0m[0;34m([0m[0mlengths[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    396[0m     [0;32mif[0m [0mlen[0m[0;34m([0m[0muniques[0m[0;34m)[0m [0;34m>[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 397[0;31m         raise ValueError(
[0m[1;32m    398[0m             [0;34m""Found input variables with inconsistent numbers of samples: %r""[0m[0;34m[0m[0;34m[0m[0m
[1;32m    399[0m             [0;34m%[0m [0;34m[[0m[0mint[0m[0;34m([0m[0ml[0m[0;34m)[0m [0;32mfor[0m [0ml[0m [0;32min[0m [0mlengths[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m

[0;31mValueError[0m: Found input variables with inconsistent numbers of samples: [1, 95]
ValueError: Found input variables with inconsistent numbers of samples: [1, 95]
",ValueError,"Found input variables with inconsistent numbers of samples: [1, 95]",FALSE,no fix
numpy_13,6b636360-3254-35af-b147-d251c78b54b2,valueerror,"could not broadcast input array from shape (10,100) into shape (10,)","['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', '/tmp/ipykernel_17/4286230538.py in <module>\n----> 1 NX = FeatureExtractor(path, n_mels = 10)\n', '/tmp/ipykernel_17/2160383838.py in FeatureExtractor(path, n_mels, fmax, fmin)\n     16 \n     17 \n---> 18     data = np.array(data)\n     19     return data\n', 'ValueError: could not broadcast input array from shape (10,100) into shape (10,)']",/junobench_env/numpy_13/numpy_13_extension.ipynb,2026-01-14T17:36:50.981688,CellExecutionError,"An error occurred while executing the following cell:
------------------
df[""Chord Type""] = df[""Chord Type""].replace(""Major"", 1)
df[""Chord Type""] = df[""Chord Type""].replace(""Minor"", 0)

#columns = [3::]
#columns.extend([""Interval 4_1"", ""Interval 5_1"", ""Interval 6_1""])
train_X, val_X, train_y, val_y = train_test_split(dfarray, df[""Chord Type""], test_size=0.2, random_state=0)

train_X
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-31-7528e0b1edf5>[0m in [0;36m<cell line: 6>[0;34m()[0m
[1;32m      4[0m [0;31m#columns = [3::][0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0;31m#columns.extend([""Interval 4_1"", ""Interval 5_1"", ""Interval 6_1""])[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 6[0;31m [0mtrain_X[0m[0;34m,[0m [0mval_X[0m[0;34m,[0m [0mtrain_y[0m[0;34m,[0m [0mval_y[0m [0;34m=[0m [0mtrain_test_split[0m[0;34m([0m[0mdfarray[0m[0;34m,[0m [0mdf[0m[0;34m[[0m[0;34m""Chord Type""[0m[0;34m][0m[0;34m,[0m [0mtest_size[0m[0;34m=[0m[0;36m0.2[0m[0;34m,[0m [0mrandom_state[0m[0;34m=[0m[0;36m0[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0mtrain_X[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py[0m in [0;36mtrain_test_split[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)[0m
[1;32m   2560[0m [0;34m[0m[0m
[1;32m   2561[0m     [0mn_samples[0m [0;34m=[0m [0m_num_samples[0m[0;34m([0m[0marrays[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 2562[0;31m     n_train, n_test = _validate_shuffle_split(
[0m[1;32m   2563[0m         [0mn_samples[0m[0;34m,[0m [0mtest_size[0m[0;34m,[0m [0mtrain_size[0m[0;34m,[0m [0mdefault_test_size[0m[0;34m=[0m[0;36m0.25[0m[0;34m[0m[0;34m[0m[0m
[1;32m   2564[0m     )

[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py[0m in [0;36m_validate_shuffle_split[0;34m(n_samples, test_size, train_size, default_test_size)[0m
[1;32m   2234[0m [0;34m[0m[0m
[1;32m   2235[0m     [0;32mif[0m [0mn_train[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 2236[0;31m         raise ValueError(
[0m[1;32m   2237[0m             [0;34m""With n_samples={}, test_size={} and train_size={}, the ""[0m[0;34m[0m[0;34m[0m[0m
[1;32m   2238[0m             [0;34m""resulting train set will be empty. Adjust any of the ""[0m[0;34m[0m[0;34m[0m[0m

[0;31mValueError[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.
ValueError: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.
",ValueError,"With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",FALSE,no fix
pandas_14,72ddb204-b59c-38dc-b378-a9c67a15dc4f,indexerror,index 6 is out of bounds for axis 0 with size 6,"['---------------------------------------------------------------------------', 'IndexError                                Traceback (most recent call last)', '/tmp/ipykernel_28/509017699.py in <module>\n     14         n = i * col + j\n     15         if n < len(nombres_col):\n---> 16             axs[i, j].set_title(nombres_col.columns[n])\n     17             axs[i, j].scatter(df[nombres_col.columns[n]][:N], y[:N])\n     18 \n', '/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py in __getitem__(self, key)\n   4602         if is_scalar(key):\n   4603             key = com.cast_scalar_indexer(key, warn_float=True)\n-> 4604             return getitem(key)\n   4605 \n   4606         if isinstance(key, slice):\n', 'IndexError: index 6 is out of bounds for axis 0 with size 6']",/junobench_env/pandas_14/pandas_14_extension.ipynb,2026-01-14T17:51:02.879453,CellExecutionError,"An error occurred while executing the following cell:
------------------
# Correlaci√≥n de las variables
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15,15))
p=sns.heatmap(df.corr(), annot=True,cmap='RdYlGn',square=True)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-17-9b9aa4217059>[0m in [0;36m<cell line: 5>[0;34m()[0m
[1;32m      3[0m [0;32mimport[0m [0mseaborn[0m [0;32mas[0m [0msns[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0mplt[0m[0;34m.[0m[0mfigure[0m[0;34m([0m[0mfigsize[0m[0;34m=[0m[0;34m([0m[0;36m15[0m[0;34m,[0m[0;36m15[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 5[0;31m [0mp[0m[0;34m=[0m[0msns[0m[0;34m.[0m[0mheatmap[0m[0;34m([0m[0mdf[0m[0;34m.[0m[0mcorr[0m[0;34m([0m[0;34m)[0m[0;34m,[0m [0mannot[0m[0;34m=[0m[0;32mTrue[0m[0;34m,[0m[0mcmap[0m[0;34m=[0m[0;34m'RdYlGn'[0m[0;34m,[0m[0msquare[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py[0m in [0;36mcorr[0;34m(self, method, min_periods, numeric_only)[0m
[1;32m  10052[0m         [0mcols[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0mcolumns[0m[0;34m[0m[0;34m[0m[0m
[1;32m  10053[0m         [0midx[0m [0;34m=[0m [0mcols[0m[0;34m.[0m[0mcopy[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m> 10054[0;31m         [0mmat[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0mto_numpy[0m[0;34m([0m[0mdtype[0m[0;34m=[0m[0mfloat[0m[0;34m,[0m [0mna_value[0m[0;34m=[0m[0mnp[0m[0;34m.[0m[0mnan[0m[0;34m,[0m [0mcopy[0m[0;34m=[0m[0;32mFalse[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m  10055[0m [0;34m[0m[0m
[1;32m  10056[0m         [0;32mif[0m [0mmethod[0m [0;34m==[0m [0;34m""pearson""[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py[0m in [0;36mto_numpy[0;34m(self, dtype, copy, na_value)[0m
[1;32m   1836[0m         [0;32mif[0m [0mdtype[0m [0;32mis[0m [0;32mnot[0m [0;32mNone[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1837[0m             [0mdtype[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0mdtype[0m[0;34m([0m[0mdtype[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1838[0;31m         [0mresult[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_mgr[0m[0;34m.[0m[0mas_array[0m[0;34m([0m[0mdtype[0m[0;34m=[0m[0mdtype[0m[0;34m,[0m [0mcopy[0m[0;34m=[0m[0mcopy[0m[0;34m,[0m [0mna_value[0m[0;34m=[0m[0mna_value[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1839[0m         [0;32mif[0m [0mresult[0m[0;34m.[0m[0mdtype[0m [0;32mis[0m [0;32mnot[0m [0mdtype[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1840[0m             [0mresult[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0marray[0m[0;34m([0m[0mresult[0m[0;34m,[0m [0mdtype[0m[0;34m=[0m[0mdtype[0m[0;34m,[0m [0mcopy[0m[0;34m=[0m[0;32mFalse[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py[0m in [0;36mas_array[0;34m(self, dtype, copy, na_value)[0m
[1;32m   1730[0m                 [0marr[0m[0;34m.[0m[0mflags[0m[0;34m.[0m[0mwriteable[0m [0;34m=[0m [0;32mFalse[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1731[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1732[0;31m             [0marr[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_interleave[0m[0;34m([0m[0mdtype[0m[0;34m=[0m[0mdtype[0m[0;34m,[0m [0mna_value[0m[0;34m=[0m[0mna_value[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1733[0m             [0;31m# The underlying data was copied within _interleave, so no need[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1734[0m             [0;31m# to further copy if copy=True or setting na_value[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py[0m in [0;36m_interleave[0;34m(self, dtype, na_value)[0m
[1;32m   1792[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1793[0m                 [0marr[0m [0;34m=[0m [0mblk[0m[0;34m.[0m[0mget_values[0m[0;34m([0m[0mdtype[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1794[0;31m             [0mresult[0m[0;34m[[0m[0mrl[0m[0;34m.[0m[0mindexer[0m[0;34m][0m [0;34m=[0m [0marr[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1795[0m             [0mitemmask[0m[0;34m[[0m[0mrl[0m[0;34m.[0m[0mindexer[0m[0;34m][0m [0;34m=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1796[0m [0;34m[0m[0m

[0;31mValueError[0m: could not convert string to float: 'Europa'
ValueError: could not convert string to float: 'Europa'
",ValueError,could not convert string to float: 'Europa',FALSE,no fix
pandas_15,c7221a95-ffc7-3173-beee-76efdc0892c5,keyerror,"""['Alley', 'PoolQC', 'Fence', 'MiscFeature'] not found in axis""","['---------------------------------------------------------------------------', 'KeyError                                  Traceback (most recent call last)', ""Cell In[37], line 1\n----> 1 test=test.drop(['Alley','PoolQC','Fence','MiscFeature','Id'],axis=1)\n"", 'File /opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331, in deprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper(*args, **kwargs)\n    325 if len(args) > num_allow_args:\n    326     warnings.warn(\n    327         msg.format(arguments=_format_argument_list(allow_args)),\n    328         FutureWarning,\n    329         stacklevel=find_stack_level(),\n    330     )\n--> 331 return func(*args, **kwargs)\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:5399, in DataFrame.drop(self, labels, axis, index, columns, level, inplace, errors)\n   5251 @deprecate_nonkeyword_arguments(version=None, allowed_args=[""self"", ""labels""])\n   5252 def drop(  # type: ignore[override]\n   5253     self,\n   (...)\n   5260     errors: IgnoreRaise = ""raise"",\n   5261 ) -> DataFrame | None:\n   5262     """"""\n   5263     Drop specified labels from rows or columns.\n   5264 \n   (...)\n   5397             weight  1.0     0.8\n   5398     """"""\n-> 5399     return super().drop(\n   5400         labels=labels,\n   5401         axis=axis,\n   5402         index=index,\n   5403         columns=columns,\n   5404         level=level,\n   5405         inplace=inplace,\n   5406         errors=errors,\n   5407     )\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331, in deprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper(*args, **kwargs)\n    325 if len(args) > num_allow_args:\n    326     warnings.warn(\n    327         msg.format(arguments=_format_argument_list(allow_args)),\n    328         FutureWarning,\n    329         stacklevel=find_stack_level(),\n    330     )\n--> 331 return func(*args, **kwargs)\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:4505, in NDFrame.drop(self, labels, axis, index, columns, level, inplace, errors)\n   4503 for axis, labels in axes.items():\n   4504     if labels is not None:\n-> 4505         obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n   4507 if inplace:\n   4508     self._update_inplace(obj)\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:4546, in NDFrame._drop_axis(self, labels, axis, level, errors, only_slice)\n   4544         new_axis = axis.drop(labels, level=level, errors=errors)\n   4545     else:\n-> 4546         new_axis = axis.drop(labels, errors=errors)\n   4547     indexer = axis.get_indexer(new_axis)\n   4549 # Case for non-unique axis\n   4550 else:\n', 'File /opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:6934, in Index.drop(self, labels, errors)\n   6932 if mask.any():\n   6933     if errors != ""ignore"":\n-> 6934         raise KeyError(f""{list(labels[mask])} not found in axis"")\n   6935     indexer = indexer[~mask]\n   6936 return self.delete(indexer)\n', 'KeyError: ""[\'Alley\', \'PoolQC\', \'Fence\', \'MiscFeature\'] not found in axis""']",/junobench_env/pandas_15/pandas_15_extension.ipynb,2026-01-14T17:51:13.948907,CellExecutionError,"An error occurred while executing the following cell:
------------------
# Identify categorical columns
categorical_cols_final = train.select_dtypes(include=['object']).columns.tolist()

# One-hot encode categorical features
train_encoded = pd.get_dummies(train, columns=categorical_cols_final, drop_first=True)
test_encoded = pd.get_dummies(test, columns=categorical_cols_final, drop_first=True)

# Align columns between train and test
common_cols = list(set(train_encoded.columns) & set(test_encoded.columns))
train_encoded = train_encoded[common_cols]
test_encoded = test_encoded[common_cols]

# Remove target from test features if present
if 'SalePrice' in test_encoded.columns:
    test_encoded = test_encoded.drop('SalePrice', axis=1)

# Define features and target
X = train_encoded.drop('SalePrice', axis=1)
y = train['SalePrice']  # Use original column

# Align test features
x_val = test_encoded[X.columns]

# Train-test split and model training
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)
pred_rf = model.predict(x_val)

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mKeyError[0m                                  Traceback (most recent call last)
[0;32m<ipython-input-26-111a3050521c>[0m in [0;36m<cell line: 18>[0;34m()[0m
[1;32m     16[0m [0;34m[0m[0m
[1;32m     17[0m [0;31m# Define features and target[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 18[0;31m [0mX[0m [0;34m=[0m [0mtrain_encoded[0m[0;34m.[0m[0mdrop[0m[0;34m([0m[0;34m'SalePrice'[0m[0;34m,[0m [0maxis[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     19[0m [0my[0m [0;34m=[0m [0mtrain[0m[0;34m[[0m[0;34m'SalePrice'[0m[0;34m][0m  [0;31m# Use original column[0m[0;34m[0m[0;34m[0m[0m
[1;32m     20[0m [0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py[0m in [0;36mdrop[0;34m(self, labels, axis, index, columns, level, inplace, errors)[0m
[1;32m   5256[0m                 [0mweight[0m  [0;36m1.0[0m     [0;36m0.8[0m[0;34m[0m[0;34m[0m[0m
[1;32m   5257[0m         """"""
[0;32m-> 5258[0;31m         return super().drop(
[0m[1;32m   5259[0m             [0mlabels[0m[0;34m=[0m[0mlabels[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m   5260[0m             [0maxis[0m[0;34m=[0m[0maxis[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py[0m in [0;36mdrop[0;34m(self, labels, axis, index, columns, level, inplace, errors)[0m
[1;32m   4547[0m         [0;32mfor[0m [0maxis[0m[0;34m,[0m [0mlabels[0m [0;32min[0m [0maxes[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   4548[0m             [0;32mif[0m [0mlabels[0m [0;32mis[0m [0;32mnot[0m [0;32mNone[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 4549[0;31m                 [0mobj[0m [0;34m=[0m [0mobj[0m[0;34m.[0m[0m_drop_axis[0m[0;34m([0m[0mlabels[0m[0;34m,[0m [0maxis[0m[0;34m,[0m [0mlevel[0m[0;34m=[0m[0mlevel[0m[0;34m,[0m [0merrors[0m[0;34m=[0m[0merrors[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   4550[0m [0;34m[0m[0m
[1;32m   4551[0m         [0;32mif[0m [0minplace[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py[0m in [0;36m_drop_axis[0;34m(self, labels, axis, level, errors, only_slice)[0m
[1;32m   4589[0m                 [0mnew_axis[0m [0;34m=[0m [0maxis[0m[0;34m.[0m[0mdrop[0m[0;34m([0m[0mlabels[0m[0;34m,[0m [0mlevel[0m[0;34m=[0m[0mlevel[0m[0;34m,[0m [0merrors[0m[0;34m=[0m[0merrors[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m   4590[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 4591[0;31m                 [0mnew_axis[0m [0;34m=[0m [0maxis[0m[0;34m.[0m[0mdrop[0m[0;34m([0m[0mlabels[0m[0;34m,[0m [0merrors[0m[0;34m=[0m[0merrors[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   4592[0m             [0mindexer[0m [0;34m=[0m [0maxis[0m[0;34m.[0m[0mget_indexer[0m[0;34m([0m[0mnew_axis[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m   4593[0m [0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py[0m in [0;36mdrop[0;34m(self, labels, errors)[0m
[1;32m   6697[0m         [0;32mif[0m [0mmask[0m[0;34m.[0m[0many[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   6698[0m             [0;32mif[0m [0merrors[0m [0;34m!=[0m [0;34m""ignore""[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 6699[0;31m                 [0;32mraise[0m [0mKeyError[0m[0;34m([0m[0;34mf""{list(labels[mask])} not found in axis""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   6700[0m             [0mindexer[0m [0;34m=[0m [0mindexer[0m[0;34m[[0m[0;34m~[0m[0mmask[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[1;32m   6701[0m         [0;32mreturn[0m [0mself[0m[0;34m.[0m[0mdelete[0m[0;34m([0m[0mindexer[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mKeyError[0m: ""['SalePrice'] not found in axis""
KeyError: ""['SalePrice'] not found in axis""
",KeyError,['SalePrice'] not found in axis,FALSE,fixed the eror
matplotlib_1,c95bb4af-7523-3ed1-8586-c53a1a20d807,typeerror,only integer scalar arrays can be converted to a scalar index,"['---------------------------------------------------------------------------', 'TypeError                                 Traceback (most recent call last)', '/tmp/ipykernel_24/2178739758.py in <module>\n      7     plt.subplot(5, 5, i+1)\n      8     plt.imshow(train_images[i].astype(""uint8""))\n----> 9     plt.title(class_names[train_labels[i]])\n     10     plt.axis(""off"")\n', 'TypeError: only integer scalar arrays can be converted to a scalar index']",/junobench_env/matplotlib_1/matplotlib_1_extension.ipynb,2026-01-14T18:42:48.157654,CellExecutionError,"An error occurred while executing the following cell:
------------------
from tensorflow import keras 
train_folder = ""../input/intel-image-classification/seg_train/seg_train"" 
test_folder = ""../input/intel-image-classification/seg_test/seg_test"" 
train_ds = keras.utils.image_dataset_from_directory(directory=train_folder, labels=""inferred"", label_mode=""int"", color_mode=""rgb"", batch_size=32, image_size=(150, 150), validation_split=0.25, subset=""training"", seed=123) 
val_ds = keras.utils.image_dataset_from_directory(directory=train_folder, labels=""inferred"", label_mode=""int"", color_mode=""rgb"", batch_size=32, image_size=(150, 150), validation_split=0.25, subset=""validation"", seed=123) 
test_ds = keras.utils.image_dataset_from_directory(directory=test_folder, labels=""inferred"", 
                                                   # Added for clarity and robustness 
                                                   label_mode=""int"", 
                                                   # Added for clarity and robustness 
                                                   color_mode=""rgb"", batch_size=32, image_size=(150, 150))
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mNotFoundError[0m                             Traceback (most recent call last)
[0;32m<ipython-input-11-204f2a872b0a>[0m in [0;36m<cell line: 4>[0;34m()[0m
[1;32m      2[0m [0mtrain_folder[0m [0;34m=[0m [0;34m""../input/intel-image-classification/seg_train/seg_train""[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m [0mtest_folder[0m [0;34m=[0m [0;34m""../input/intel-image-classification/seg_test/seg_test""[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 4[0;31m [0mtrain_ds[0m [0;34m=[0m [0mkeras[0m[0;34m.[0m[0mutils[0m[0;34m.[0m[0mimage_dataset_from_directory[0m[0;34m([0m[0mdirectory[0m[0;34m=[0m[0mtrain_folder[0m[0;34m,[0m [0mlabels[0m[0;34m=[0m[0;34m""inferred""[0m[0;34m,[0m [0mlabel_mode[0m[0;34m=[0m[0;34m""int""[0m[0;34m,[0m [0mcolor_mode[0m[0;34m=[0m[0;34m""rgb""[0m[0;34m,[0m [0mbatch_size[0m[0;34m=[0m[0;36m32[0m[0;34m,[0m [0mimage_size[0m[0;34m=[0m[0;34m([0m[0;36m150[0m[0;34m,[0m [0;36m150[0m[0;34m)[0m[0;34m,[0m [0mvalidation_split[0m[0;34m=[0m[0;36m0.25[0m[0;34m,[0m [0msubset[0m[0;34m=[0m[0;34m""training""[0m[0;34m,[0m [0mseed[0m[0;34m=[0m[0;36m123[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      5[0m [0mval_ds[0m [0;34m=[0m [0mkeras[0m[0;34m.[0m[0mutils[0m[0;34m.[0m[0mimage_dataset_from_directory[0m[0;34m([0m[0mdirectory[0m[0;34m=[0m[0mtrain_folder[0m[0;34m,[0m [0mlabels[0m[0;34m=[0m[0;34m""inferred""[0m[0;34m,[0m [0mlabel_mode[0m[0;34m=[0m[0;34m""int""[0m[0;34m,[0m [0mcolor_mode[0m[0;34m=[0m[0;34m""rgb""[0m[0;34m,[0m [0mbatch_size[0m[0;34m=[0m[0;36m32[0m[0;34m,[0m [0mimage_size[0m[0;34m=[0m[0;34m([0m[0;36m150[0m[0;34m,[0m [0;36m150[0m[0;34m)[0m[0;34m,[0m [0mvalidation_split[0m[0;34m=[0m[0;36m0.25[0m[0;34m,[0m [0msubset[0m[0;34m=[0m[0;34m""validation""[0m[0;34m,[0m [0mseed[0m[0;34m=[0m[0;36m123[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      6[0m test_ds = keras.utils.image_dataset_from_directory(directory=test_folder, labels=""inferred"", 

[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_dataset_utils.py[0m in [0;36mimage_dataset_from_directory[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)[0m
[1;32m    221[0m     [0;32mif[0m [0mseed[0m [0;32mis[0m [0;32mNone[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    222[0m         [0mseed[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0mrandom[0m[0;34m.[0m[0mrandint[0m[0;34m([0m[0;36m1e6[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 223[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(
[0m[1;32m    224[0m         [0mdirectory[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m    225[0m         [0mlabels[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/dataset_utils.py[0m in [0;36mindex_directory[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)[0m
[1;32m    528[0m     [0;32mif[0m [0mlabels[0m [0;34m==[0m [0;34m""inferred""[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    529[0m         [0msubdirs[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 530[0;31m         [0;32mfor[0m [0msubdir[0m [0;32min[0m [0msorted[0m[0;34m([0m[0mtf[0m[0;34m.[0m[0mio[0m[0;34m.[0m[0mgfile[0m[0;34m.[0m[0mlistdir[0m[0;34m([0m[0mdirectory[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    531[0m             [0;32mif[0m [0mtf[0m[0;34m.[0m[0mio[0m[0;34m.[0m[0mgfile[0m[0;34m.[0m[0misdir[0m[0;34m([0m[0mtf[0m[0;34m.[0m[0mio[0m[0;34m.[0m[0mgfile[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0mdirectory[0m[0;34m,[0m [0msubdir[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    532[0m                 [0;32mif[0m [0;32mnot[0m [0msubdir[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m"".""[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py[0m in [0;36mlist_directory_v2[0;34m(path)[0m
[1;32m    766[0m   """"""
[1;32m    767[0m   [0;32mif[0m [0;32mnot[0m [0mis_directory[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 768[0;31m     raise errors.NotFoundError(
[0m[1;32m    769[0m         [0mnode_def[0m[0;34m=[0m[0;32mNone[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m    770[0m         [0mop[0m[0;34m=[0m[0;32mNone[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m

[0;31mNotFoundError[0m: Could not find directory ../input/intel-image-classification/seg_train/seg_train
NotFoundError: Could not find directory ../input/intel-image-classification/seg_train/seg_train
",NotFoundError,Could not find directory ../input/intel-image-classification/seg_train/seg_train,FALSE,fixed the eror
matplotlib_3,0b400994-9ace-3c61-949c-7153cadfaf86,typeerror,"float() argument must be a string or a real number, not 'MetricObservation'","['---------------------------------------------------------------------------', 'TypeError                                 Traceback (most recent call last)', ""Cell In[83], line 12\n     10 for trial in best_trials:\n     11     val_accuracy_history = trial.metrics.get_history(name='val_accuracy')\n---> 12     plt.plot(val_accuracy_history, label=f'Trial {trial.trial_id}')\n     14 plt.title('Validation Accuracy of Best Trials')\n     15 plt.xlabel('Epochs')\n"", 'File /opt/conda/lib/python3.10/site-packages/matplotlib/pyplot.py:2812, in plot(scalex, scaley, data, *args, **kwargs)\n   2810 @_copy_docstring_and_deprecators(Axes.plot)\n   2811 def plot(*args, scalex=True, scaley=True, data=None, **kwargs):\n-> 2812     return gca().plot(\n   2813         *args, scalex=scalex, scaley=scaley,\n   2814         **({""data"": data} if data is not None else {}), **kwargs)\n', 'File /opt/conda/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1690, in Axes.plot(self, scalex, scaley, data, *args, **kwargs)\n   1688 lines = [*self._get_lines(*args, data=data, **kwargs)]\n   1689 for line in lines:\n-> 1690     self.add_line(line)\n   1691 if scalex:\n   1692     self._request_autoscale_view(""x"")\n', ""File /opt/conda/lib/python3.10/site-packages/matplotlib/axes/_base.py:2304, in _AxesBase.add_line(self, line)\n   2301 if line.get_clip_path() is None:\n   2302     line.set_clip_path(self.patch)\n-> 2304 self._update_line_limits(line)\n   2305 if not line.get_label():\n   2306     line.set_label(f'_child{len(self._children)}')\n"", 'File /opt/conda/lib/python3.10/site-packages/matplotlib/axes/_base.py:2327, in _AxesBase._update_line_limits(self, line)\n   2323 def _update_line_limits(self, line):\n   2324     """"""\n   2325     Figures out the data limit of the given line, updating self.dataLim.\n   2326     """"""\n-> 2327     path = line.get_path()\n   2328     if path.vertices.size == 0:\n   2329         return\n', 'File /opt/conda/lib/python3.10/site-packages/matplotlib/lines.py:1028, in Line2D.get_path(self)\n   1026 """"""Return the `~matplotlib.path.Path` associated with this line.""""""\n   1027 if self._invalidy or self._invalidx:\n-> 1028     self.recache()\n   1029 return self._path\n', 'File /opt/conda/lib/python3.10/site-packages/matplotlib/lines.py:664, in Line2D.recache(self, always)\n    662 if always or self._invalidy:\n    663     yconv = self.convert_yunits(self._yorig)\n--> 664     y = _to_unmasked_float_array(yconv).ravel()\n    665 else:\n    666     y = self._y\n', 'File /opt/conda/lib/python3.10/site-packages/matplotlib/cbook/__init__.py:1345, in _to_unmasked_float_array(x)\n   1343     return np.ma.asarray(x, float).filled(np.nan)\n   1344 else:\n-> 1345     return np.asarray(x, float)\n', ""TypeError: float() argument must be a string or a real number, not 'MetricObservation'""]",/junobench_env/matplotlib_3/matplotlib_3_extension.ipynb,2026-01-14T17:50:48.882606,CellExecutionError,"An error occurred while executing the following cell:
------------------
# Plot the architecture of the best model
best_model = tuner.oracle.get_best_trials(1)[0].model
best_model.summary()

# Plot the summary of the hyperparameter search
plot_summary(tuner.oracle.get_best_trials(5), metric='val_accuracy')

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mAttributeError[0m                            Traceback (most recent call last)
[0;32m<ipython-input-20-59374f519abd>[0m in [0;36m<cell line: 2>[0;34m()[0m
[1;32m      1[0m [0;31m# Plot the architecture of the best model[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 2[0;31m [0mbest_model[0m [0;34m=[0m [0mtuner[0m[0;34m.[0m[0moracle[0m[0;34m.[0m[0mget_best_trials[0m[0;34m([0m[0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m.[0m[0mmodel[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      3[0m [0mbest_model[0m[0;34m.[0m[0msummary[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m [0;31m# Plot the summary of the hyperparameter search[0m[0;34m[0m[0;34m[0m[0m

[0;31mAttributeError[0m: 'Trial' object has no attribute 'model'
AttributeError: 'Trial' object has no attribute 'model'
",AttributeError,'Trial' object has no attribute 'model',FALSE,No fix
seaborn_2,cfe1996a-785a-385e-87d9-62039731a35e,typeerror,countplot() got multiple values for argument 'data',"['---------------------------------------------------------------------------', 'TypeError                                 Traceback (most recent call last)', 'Cell In[13], line 7\n      5 dev_train[\'SeriousDlqin2yrs\'].value_counts().plot.pie(explode=[0,0.1],autopct=""%1.1f%%"",ax=axes[0])\n      6 axes[0].set_title(""SeriousDlqin2yrs"")\n----> 7 sns.countplot(""SeriousDlqin2yrs"",data=dev_train,ax=axes[1])\n      8 axes[1].set_title(""SeriousDlqin2yrs"")\n      9 plt.show()\n', ""TypeError: countplot() got multiple values for argument 'data'""]",/junobench_env/seaborn_2/seaborn_2_extension.ipynb,2026-01-14T16:35:55.169206,CellExecutionError,"An error occurred while executing the following cell:
------------------
fig=plt.figure(figsize=[25,25])
for col,i in zip(dev_train.columns,range(1,13)):
    axes=fig.add_subplot(7,2,i)
    sns.regplot(dev_train[col],dev_train.SeriousDlqin2yrs,ax=axes)
plt.show()
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mTypeError[0m                                 Traceback (most recent call last)
[0;32m<ipython-input-8-314ed5a9dfd5>[0m in [0;36m<cell line: 2>[0;34m()[0m
[1;32m      2[0m [0;32mfor[0m [0mcol[0m[0;34m,[0m[0mi[0m [0;32min[0m [0mzip[0m[0;34m([0m[0mdev_train[0m[0;34m.[0m[0mcolumns[0m[0;34m,[0m[0mrange[0m[0;34m([0m[0;36m1[0m[0;34m,[0m[0;36m13[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m     [0maxes[0m[0;34m=[0m[0mfig[0m[0;34m.[0m[0madd_subplot[0m[0;34m([0m[0;36m7[0m[0;34m,[0m[0;36m2[0m[0;34m,[0m[0mi[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 4[0;31m     [0msns[0m[0;34m.[0m[0mregplot[0m[0;34m([0m[0mdev_train[0m[0;34m[[0m[0mcol[0m[0;34m][0m[0;34m,[0m[0mdev_train[0m[0;34m.[0m[0mSeriousDlqin2yrs[0m[0;34m,[0m[0max[0m[0;34m=[0m[0maxes[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      5[0m [0mplt[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mTypeError[0m: regplot() takes from 0 to 1 positional arguments but 2 positional arguments (and 1 keyword-only argument) were given
TypeError: regplot() takes from 0 to 1 positional arguments but 2 positional arguments (and 1 keyword-only argument) were given
",TypeError,regplot() takes from 0 to 1 positional arguments but 2 positional arguments (and 1 keyword-only argument) were given,FALSE,fixed the error
seaborn_3,12880de0-a70b-3adf-a0a9-90284cdd75b6,typeerror,barplot() takes from 0 to 1 positional arguments but 2 were given,"['---------------------------------------------------------------------------', 'TypeError                                 Traceback (most recent call last)', 'Cell In[22], line 18\n     15     num_summary(df, col, True)\n     17 for col in num_cols:\n---> 18     sns.barplot(col,df)\n     20 for col in num_cols:\n     21     sns.boxplot(data=df, x=col)\n', 'TypeError: barplot() takes from 0 to 1 positional arguments but 2 were given']",/junobench_env/seaborn_3/seaborn_3_extension.ipynb,2026-01-14T17:36:59.950891,CellExecutionError,"An error occurred while executing the following cell:
------------------
corr = df[num_cols].corr()
corr

# Korelasyonlarƒ±n g√∂sterilmesi
sns.set(rc={'figure.figsize': (12, 12)})
sns.heatmap(corr, cmap=""RdBu"")
plt.show(block=True)



def high_correlated_cols(dataframe, plot=False, corr_th=0.70):
    corr = dataframe.corr()
    cor_matrix = corr.abs()
    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))
    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]
    if plot:
        import seaborn as sns
        import matplotlib.pyplot as plt
        sns.set(rc={'figure.figsize': (15, 15)})
        sns.heatmap(corr, cmap=""RdBu"")
        plt.show(block=True)
    return drop_list

high_correlated_cols(df, plot=True)

df.shape
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mAttributeError[0m                            Traceback (most recent call last)
[0;32m<ipython-input-9-4971d8791244>[0m in [0;36m<cell line: 24>[0;34m()[0m
[1;32m     22[0m     [0;32mreturn[0m [0mdrop_list[0m[0;34m[0m[0;34m[0m[0m
[1;32m     23[0m [0;34m[0m[0m
[0;32m---> 24[0;31m [0mhigh_correlated_cols[0m[0;34m([0m[0mdf[0m[0;34m,[0m [0mplot[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     25[0m [0;34m[0m[0m
[1;32m     26[0m [0mdf[0m[0;34m.[0m[0mshape[0m[0;34m[0m[0;34m[0m[0m

[0;32m<ipython-input-9-4971d8791244>[0m in [0;36mhigh_correlated_cols[0;34m(dataframe, plot, corr_th)[0m
[1;32m     12[0m     [0mcorr[0m [0;34m=[0m [0mdataframe[0m[0;34m.[0m[0mcorr[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     13[0m     [0mcor_matrix[0m [0;34m=[0m [0mcorr[0m[0;34m.[0m[0mabs[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 14[0;31m     [0mupper_triangle_matrix[0m [0;34m=[0m [0mcor_matrix[0m[0;34m.[0m[0mwhere[0m[0;34m([0m[0mnp[0m[0;34m.[0m[0mtriu[0m[0;34m([0m[0mnp[0m[0;34m.[0m[0mones[0m[0;34m([0m[0mcor_matrix[0m[0;34m.[0m[0mshape[0m[0;34m)[0m[0;34m,[0m [0mk[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m.[0m[0mastype[0m[0;34m([0m[0mnp[0m[0;34m.[0m[0mbool[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     15[0m     [0mdrop_list[0m [0;34m=[0m [0;34m[[0m[0mcol[0m [0;32mfor[0m [0mcol[0m [0;32min[0m [0mupper_triangle_matrix[0m[0;34m.[0m[0mcolumns[0m [0;32mif[0m [0many[0m[0;34m([0m[0mupper_triangle_matrix[0m[0;34m[[0m[0mcol[0m[0;34m][0m [0;34m>[0m [0mcorr_th[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[1;32m     16[0m     [0;32mif[0m [0mplot[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/numpy/__init__.py[0m in [0;36m__getattr__[0;34m(attr)[0m
[1;32m    322[0m [0;34m[0m[0m
[1;32m    323[0m         [0;32mif[0m [0mattr[0m [0;32min[0m [0m__former_attrs__[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 324[0;31m             [0;32mraise[0m [0mAttributeError[0m[0;34m([0m[0m__former_attrs__[0m[0;34m[[0m[0mattr[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    325[0m [0;34m[0m[0m
[1;32m    326[0m         [0;32mif[0m [0mattr[0m [0;34m==[0m [0;34m'testing'[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;31mAttributeError[0m: module 'numpy' has no attribute 'bool'.
`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
AttributeError: module 'numpy' has no attribute 'bool'.
`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
",AttributeError,module 'numpy' has no attribute 'bool'.,FALSE,fixed the eror
seaborn_4,9c322458-628d-3376-91e0-b1ce34d408d6,indexerror,index 14 is out of bounds for axis 0 with size 14,"['---------------------------------------------------------------------------', 'IndexError                                Traceback (most recent call last)', '/tmp/ipykernel_28/1417668694.py in <module>\n      5 \n      6 for col, value in df.items():\n----> 7     sns.boxplot(y=col, data=df, ax=ax[index])\n      8     index += 1\n      9 plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)\n', 'IndexError: index 14 is out of bounds for axis 0 with size 14']",/junobench_env/seaborn_4/seaborn_4_extension.ipynb,2026-01-14T17:22:02.205478,CellExecutionError,"An error occurred while executing the following cell:
------------------
from sklearn.linear_model import LinearRegression
model = LinearRegression(normalize=True)
train(model, X, y)
coef = pd.Series(model.coef_, X.columns).sort_values()
coef.plot(kind='bar', title='Model Coefficients')
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mTypeError[0m                                 Traceback (most recent call last)
[0;32m<ipython-input-13-35f65edbe23f>[0m in [0;36m<cell line: 2>[0;34m()[0m
[1;32m      1[0m [0;32mfrom[0m [0msklearn[0m[0;34m.[0m[0mlinear_model[0m [0;32mimport[0m [0mLinearRegression[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 2[0;31m [0mmodel[0m [0;34m=[0m [0mLinearRegression[0m[0;34m([0m[0mnormalize[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      3[0m [0mtrain[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mX[0m[0;34m,[0m [0my[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0mcoef[0m [0;34m=[0m [0mpd[0m[0;34m.[0m[0mSeries[0m[0;34m([0m[0mmodel[0m[0;34m.[0m[0mcoef_[0m[0;34m,[0m [0mX[0m[0;34m.[0m[0mcolumns[0m[0;34m)[0m[0;34m.[0m[0msort_values[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0mcoef[0m[0;34m.[0m[0mplot[0m[0;34m([0m[0mkind[0m[0;34m=[0m[0;34m'bar'[0m[0;34m,[0m [0mtitle[0m[0;34m=[0m[0;34m'Model Coefficients'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mTypeError[0m: LinearRegression.__init__() got an unexpected keyword argument 'normalize'
TypeError: LinearRegression.__init__() got an unexpected keyword argument 'normalize'
",TypeError,LinearRegression.__init__() got an unexpected keyword argument 'normalize',FALSE,fixed the eror
matplotlib_4,742aa349-be15-3da6-be0e-026b70dc146f,typeerror,only integer scalar arrays can be converted to a scalar index,"['---------------------------------------------------------------------------', 'TypeError                                 Traceback (most recent call last)', '/tmp/ipykernel_23/922550585.py in <module>\n      6     ax = plt.subplot(3, 3, i + 1)\n      7     plt.imshow(images[i].numpy().astype(""uint8""))\n----> 8     plt.title(class_names[labels[i]])\n      9     plt.axis(""off"")\n', '/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in __index__(self)\n   1061 \n   1062   def __index__(self):\n-> 1063     return self._numpy().__index__()\n   1064 \n   1065   def __bool__(self):\n', 'TypeError: only integer scalar arrays can be converted to a scalar index']",/junobench_env/matplotlib_4/matplotlib_4_extension.ipynb,2026-01-14T17:50:05.781048,CellExecutionError,"An error occurred while executing the following cell:
------------------
resnet_model = Sequential() 
pretrained_model= tf.keras.applications.ResNet50(include_top=False, input_shape=(200,200,3), pooling='avg',classes=58, weights='imagenet') 
for layer in pretrained_model.layers: layer.trainable=False 
resnet_model.add(pretrained_model) 
# Removed the redundant Flatten() layer 
resnet_model.add(Dense(512, activation='relu')) 
resnet_model.add(Dense(58, activation='softmax'))
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m<ipython-input-10-10fa72d7d6db>[0m in [0;36m<cell line: 6>[0;34m()[0m
[1;32m      4[0m [0mresnet_model[0m[0;34m.[0m[0madd[0m[0;34m([0m[0mpretrained_model[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0;31m# Removed the redundant Flatten() layer[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 6[0;31m [0mresnet_model[0m[0;34m.[0m[0madd[0m[0;34m([0m[0mDense[0m[0;34m([0m[0;36m512[0m[0;34m,[0m [0mactivation[0m[0;34m=[0m[0;34m'relu'[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      7[0m [0mresnet_model[0m[0;34m.[0m[0madd[0m[0;34m([0m[0mDense[0m[0;34m([0m[0;36m58[0m[0;34m,[0m [0mactivation[0m[0;34m=[0m[0;34m'softmax'[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py[0m in [0;36madd[0;34m(self, layer, rebuild)[0m
[1;32m     93[0m                 [0mlayer[0m [0;34m=[0m [0morigin_layer[0m[0;34m[0m[0;34m[0m[0m
[1;32m     94[0m         [0;32mif[0m [0;32mnot[0m [0misinstance[0m[0;34m([0m[0mlayer[0m[0;34m,[0m [0mLayer[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 95[0;31m             raise ValueError(
[0m[1;32m     96[0m                 [0;34m""Only instances of `keras.Layer` can be ""[0m[0;34m[0m[0;34m[0m[0m
[1;32m     97[0m                 [0;34mf""added to a Sequential model. Received: {layer} ""[0m[0;34m[0m[0;34m[0m[0m

[0;31mValueError[0m: Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow.python.keras.layers.core.Dense object at 0x7fd2342b6c20> (of type <class 'tensorflow.python.keras.layers.core.Dense'>)
ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow.python.keras.layers.core.Dense object at 0x7fd2342b6c20> (of type <class 'tensorflow.python.keras.layers.core.Dense'>)
",ValueError,Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow.python.keras.layers.core.Dense object at 0x7fd2342b6c20> (of type <class 'tensorflow.python.keras.layers.core.Dense'>),FALSE,fixed the eror
lightgbm_1,3e0645e7-6fed-3afe-ac64-83f9ec38f16b,valueerror,"Input contains NaN, infinity or a value too large for dtype('float64').","['---------------------------------------------------------------------------', 'ValueError                                Traceback (most recent call last)', '/tmp/ipykernel_27/2908357616.py in <module>\n      1 study = optuna.create_study(direction=""maximize"")\n----> 2 study.optimize(find_out_params_model, n_trials=2)\n', '/opt/conda/lib/python3.7/site-packages/optuna/study/study.py in optimize(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\n    432             callbacks=callbacks,\n    433             gc_after_trial=gc_after_trial,\n--> 434             show_progress_bar=show_progress_bar,\n    435         )\n    436 \n', '/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py in _optimize(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\n     74                 reseed_sampler_rng=False,\n     75                 time_start=None,\n---> 76                 progress_bar=progress_bar,\n     77             )\n     78         else:\n', '/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py in _optimize_sequential(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\n    161 \n    162         try:\n--> 163             frozen_trial = _run_trial(study, func, catch)\n    164         finally:\n    165             # The following line mitigates memory problems that can be occurred in some\n', '/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py in _run_trial(study, func, catch)\n    249         and not isinstance(func_err, catch)\n    250     ):\n--> 251         raise func_err\n    252     return frozen_trial\n    253 \n', '/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py in _run_trial(study, func, catch)\n    198     with get_heartbeat_thread(trial._trial_id, study._storage):\n    199         try:\n--> 200             value_or_values = func(trial)\n    201         except exceptions.TrialPruned as e:\n    202             # TODO(mamu): Handle multi-objective cases.\n', ""/tmp/ipykernel_27/2150223056.py in find_out_params_model(trial)\n      8         random_state = random_state\n      9     )\n---> 10     for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\n     11         #print(fold, end=' ')\n     12         X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n"", '/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py in split(self, X, y, groups)\n    745         to an integer.\n    746         """"""\n--> 747         y = check_array(y, ensure_2d=False, dtype=None)\n    748         return super().split(X, y, groups)\n    749 \n', '/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\n    798 \n    799         if force_all_finite:\n--> 800             _assert_all_finite(array, allow_nan=force_all_finite == ""allow-nan"")\n    801 \n    802     if ensure_min_samples > 0:\n', '/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py in _assert_all_finite(X, allow_nan, msg_dtype)\n    114             raise ValueError(\n    115                 msg_err.format(\n--> 116                     type_err, msg_dtype if msg_dtype is not None else X.dtype\n    117                 )\n    118             )\n', ""ValueError: Input contains NaN, infinity or a value too large for dtype('float64').""]",/junobench_env/lightgbm_1/lightgbm_1_extension.ipynb,2026-01-14T18:04:18.709326,CellExecutionError,"An error occurred while executing the following cell:
------------------
# Check for NaNs in the target variable
print(
    f""NaNs in target 'quality' before handling: ""
    f""{train3[conf.target].isnull().sum()}""
)

initial_rows = train3.shape[0]

# Drop rows where the target variable is NaN
train3.dropna(subset=[conf.target], inplace=True)

print(
    f""Dropped {initial_rows - train3.shape[0]} rows with NaN in target.""
)

# Re-define X and y after dropping NaNs to ensure they are clean
y = train3[conf.target]
X = train3.drop([conf.target], axis=1)

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mKeyError[0m                                  Traceback (most recent call last)
[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py[0m in [0;36mget_loc[0;34m(self, key)[0m
[1;32m   3652[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 3653[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_engine[0m[0;34m.[0m[0mget_loc[0m[0;34m([0m[0mcasted_key[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   3654[0m         [0;32mexcept[0m [0mKeyError[0m [0;32mas[0m [0merr[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx[0m in [0;36mpandas._libs.index.IndexEngine.get_loc[0;34m()[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx[0m in [0;36mpandas._libs.index.IndexEngine.get_loc[0;34m()[0m

[0;32mpandas/_libs/hashtable_class_helper.pxi[0m in [0;36mpandas._libs.hashtable.PyObjectHashTable.get_item[0;34m()[0m

[0;32mpandas/_libs/hashtable_class_helper.pxi[0m in [0;36mpandas._libs.hashtable.PyObjectHashTable.get_item[0;34m()[0m

[0;31mKeyError[0m: 'quality'

The above exception was the direct cause of the following exception:

[0;31mKeyError[0m                                  Traceback (most recent call last)
[0;32m<ipython-input-23-3f4b0c6d56c4>[0m in [0;36m<cell line: 2>[0;34m()[0m
[1;32m      2[0m print(
[1;32m      3[0m     [0;34mf""NaNs in target 'quality' before handling: ""[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 4[0;31m     [0;34mf""{train3[conf.target].isnull().sum()}""[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      5[0m )
[1;32m      6[0m [0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py[0m in [0;36m__getitem__[0;34m(self, key)[0m
[1;32m   3759[0m             [0;32mif[0m [0mself[0m[0;34m.[0m[0mcolumns[0m[0;34m.[0m[0mnlevels[0m [0;34m>[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   3760[0m                 [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_getitem_multilevel[0m[0;34m([0m[0mkey[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 3761[0;31m             [0mindexer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mcolumns[0m[0;34m.[0m[0mget_loc[0m[0;34m([0m[0mkey[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   3762[0m             [0;32mif[0m [0mis_integer[0m[0;34m([0m[0mindexer[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   3763[0m                 [0mindexer[0m [0;34m=[0m [0;34m[[0m[0mindexer[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py[0m in [0;36mget_loc[0;34m(self, key)[0m
[1;32m   3653[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_engine[0m[0;34m.[0m[0mget_loc[0m[0;34m([0m[0mcasted_key[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m   3654[0m         [0;32mexcept[0m [0mKeyError[0m [0;32mas[0m [0merr[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 3655[0;31m             [0;32mraise[0m [0mKeyError[0m[0;34m([0m[0mkey[0m[0;34m)[0m [0;32mfrom[0m [0merr[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   3656[0m         [0;32mexcept[0m [0mTypeError[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   3657[0m             [0;31m# If we have a listlike key, _check_indexing_error will raise[0m[0;34m[0m[0;34m[0m[0m

[0;31mKeyError[0m: 'quality'
KeyError: 'quality'
",KeyError,'quality',FALSE,No fix